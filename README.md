# Doc2vec

I thought that word embeddings would perform better than tf-idf as they would generate the vectors for words based on the context of the text 
but I realised that it would be difficult to compare documents as they would be a matrix of vectors of different sizes

> ## word2vec
 - gives one vector per word, so gives set of vectors for a document, so hard to take cosine similarty			
available methods 
- take mean and proceed.			
- [using the paragraph vector](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)
- [some say it is extrmely difficult(2 ans in the given link)](https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings/15173821#15173821)		
- using cosine formula implementation 		
> ## Doc2vec
- [Sentence / Document Similarity](https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt)		
- [Refecence](https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630)
				
> ## Universal sentence Encoder
- Why use universal sentence encoder(https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents)
- [Research Paper](https://arxiv.org/pdf/1803.11175.pdf)
				

> ## Other methods
- [StackOverflow reference](https://stackoverflow.com/questions/22129943/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-pyt)
> ## All the similarity methods
- [medium refernce](https://medium.com/analytics-vidhya/best-nlp-algorithms-to-get-document-similarity-a5559244b23b)
