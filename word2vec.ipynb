{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyMuPDF\n",
      "  Using cached PyMuPDF-1.21.1-cp310-cp310-win_amd64.whl (11.7 MB)\n",
      "Installing collected packages: pyMuPDF\n",
      "Successfully installed pyMuPDF-1.21.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8-py3-none-any.whl (1.5 MB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2022.10.31-cp310-cp310-win_amd64.whl (267 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: tqdm in d:\\program files\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: colorama in d:\\program files\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8 regex-2022.10.31\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1  added to doclist\n",
      "file 2  added to doclist\n",
      "file 3  added to doclist\n",
      "file 4  added to doclist\n",
      "file 5  added to doclist\n",
      "file 6  added to doclist\n",
      "file 7  added to doclist\n",
      "file 8  added to doclist\n",
      "file 9  added to doclist\n",
      "file 10  added to doclist\n",
      "file 11  added to doclist\n",
      "file 12  added to doclist\n",
      "file 13  added to doclist\n",
      "file 14  added to doclist\n",
      "file 15  added to doclist\n",
      "file 16  added to doclist\n",
      "file 17  added to doclist\n",
      "file 18  added to doclist\n",
      "file 19  added to doclist\n",
      "file 20  added to doclist\n"
     ]
    }
   ],
   "source": [
    "doclist = []\n",
    "i = 1\n",
    "doc=\"\"\n",
    "folder = os.listdir('pdffiles')\n",
    "for file in folder:\n",
    "    with fitz.open(rf\"pdffiles/{file}\") as pdf:\n",
    "        for page in pdf:\n",
    "            text = page.get_text()\n",
    "            doc = doc+text\n",
    "    doclist.append(doc)\n",
    "    doc=\"\"\n",
    "    print(\"file\",i,\" added to doclist\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub('[^a-zA-Z ]',' ',text)\n",
    "    text = re.sub('[^\\w\\s\\']',' ',text)\n",
    "    text = re.sub('[ \\n]+',' ',text)\n",
    "    text = text.lower().split()     \n",
    "    text = [word for word in text if word not in stopwords.words('english')] \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for doc in doclist:\n",
    "    corpus.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.index_to_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using n_similarity method of w2v\n",
    "---\n",
    "\n",
    "n_similarity is an in built function that calculates the similarity between 2 list of words.\n",
    "_it seems to be yielding ***incorrect*** results_ but okay , we will try different methods as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,20):\n",
    "    for j in range(i+1,20):\n",
    "        if(i!=j):\n",
    "            print(\"The Similarity between document\",i,\"and document\",j,\"is\" ,model.wv.n_similarity(corpus[i],corpus[j]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average of all the word vectors to create the document vectors\n",
    "---\n",
    "One way to like create a vector for a dociment would be average out all the word vectors in the document but that would mean loss of a lot of information as well extreme dilution resulting to inaccurate similarity results.\n",
    "\n",
    "SO instead I used doc2vec."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "080da6c26f0a175bdb916ba142774bba8599a404fc80d6d1f14d481a78eeafad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
