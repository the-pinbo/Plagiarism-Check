{
    "abstract": "The Intrusion Detection and Prevention System (IDPS) services of a North American cloud service provider were ineffective against a simulated network timing channel attack. During the tests, three conspiring white hat agents exchanged a total of 33,024 network packets. As the proxy based attack executed, the vendor\u2019s intrusion detection service did not generate a warning, nor did its intrusion pre...",
    "articleNumber": "9973314",
    "articleTitle": "A Zero-Day Cloud Timing Channel Attack",
    "authors": [
        {
            "preferredName": "Robert Flowers",
            "normalizedName": "R. Flowers",
            "firstName": "Robert",
            "lastName": "Flowers",
            "searchablePreferredName": "Robert Flowers"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227420",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9973314/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>More than a decade ago, Yale University researchers Ford and Aviram questioned the unmitigated trust corporations have of Cloud Service Providers (CSPs) <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. As recently as 2018, Yale University researchers Deng et al. <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a> warned the cybersecurity community about the threat posed by timing channels within the cloud. Those warnings echoed many of the concerns first expressed by their Yale University colleagues eight years earlier. A timing channel is a form of covert data transfer that uses time itself as a carrier and as a result, it is extremely difficult to develop, deploy, and detect <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>.</p><p>The Yale-identified threats distill into four inherent risks for cloud computing related to a timing channel: implicit clocks, shared resources, insider breach, and difficulty of detection. The last two of those inherent risks are the focus of the proxy based attack executed by this study. The National Institute of Standards and Technology (NIST) defines a zero-day weakness as a vulnerability within hardware/software that is discovered after its release <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>, <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>. In the current context, the vulnerability in question is in the cloud. This study demonstrates timing channels in the cloud are, in fact, a zero-day security flaw.</p><p>As noted by MIT <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>, organizations place insufficient importance on insider threats. Lack of emphasis on trusted bad actors compounds the issues arising from the implicit trust of CSPs. A system can perform well on any number of penetration tests, but its degree of porosity when insiders attempt to unlawfully move data from the inside out can be a cause for concern. When combined, the two threats create scenarios where an insider with high-level privileges, such as contractors and consultants, can steal confidential data and use the rapid setup and tear down of virtualization in the cloud to cover their tracks.</p><p>This research focused on the Infrastructure as a Service (IaaS) capabilities of a cloud service provider whose <i>infiltration</i> technologies were found to perform as advertised using assessment methods prescribed by the CSP.<a ref-type=\"fn\" anchor=\"fn1\" class=\"footnote-link\">1</a> Once those infiltration countermeasures were confirmed, the study then interrogated those same defenses to determine how well they resisted <i>exfiltration</i> and alerted on attempts to hijack their IaaS layer by having them serve as a proxy during a sophisticated network steganographic assault. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> illustrates the vulnerability explored by this article.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe1-3227420-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe1-3227420-small.gif\" alt=\"FIGURE 1. - The stegacloud attack defeats client exfiltration defenses, thwarts the IDPS and threat intelligence controls in the cloud, and hides the true destination of the stolen Personally Identifiable Information (PII).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>The <i>stegacloud</i> attack defeats client exfiltration defenses, thwarts the IDPS and threat intelligence controls in the cloud, and hides the true destination of the stolen Personally Identifiable Information (PII).</p></fig></div><p class=\"links\"><a href=\"/document/9973314/all-figures\" class=\"all\">Show All</a></p></div></p><p>Alerting is a foundational component of a timely security incident response <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>. Cloud deployments feature numerous security related advantages over typical on-premise solutions: especially those spanning multiple locations. One advantage is integration of event and incident notifications. Many enterprise-class CSPs offer Security Information and Event Management (SIEM) as an integral component to complement the inherent scalability and resilience benefits of cloud platforms. Whether part of the default package or a marketplace add-on, CSPs present homogeneous options for their clients to apply centralized management and monitoring functions to cover their entire end-to-end technical infrastructure.</p><p>Microsoft Azure Threat Intelligence <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, Amazon Web Services (AWS) Intelligent Threat Detection <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, Google Threat Intelligence <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>, and similar CSP solutions are examples of smart countermeasures that dynamically adjust their operation based on external threat events. For example, assume Client A and Client B both utilize the same CSP. If an aggressor launches a Distributed Denial of Service (DDoS) strike against Client A, the CSP will not only execute countermeasures to defend against the originator of the Client A attack, it will also replicate the defensive configuration for Client B. Thanks to a multifaceted response, a single incursion could have the positive effect of hundreds or thousands of other customers becoming immune to the same offensive in a matter of seconds.</p><p>In order to determine the degree to which the intelligent responses described above could handle a real world data breach, the experiment that follows measured the CSP\u2019s response to suspicious Transmission Control Protocol/Internet Protocol (TCP/IP) segments when the selected CSP was not the direct subject of hostile action. The ubiquitous availability and rapid deployment of CSP virtual machines makes them an ideal target for indirect and ephemeral attacks via network steganography.<a ref-type=\"fn\" anchor=\"fn2\" class=\"footnote-link\">2</a></p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Background</h2></div><div class=\"section_2\" id=\"sec2a\"><h3>A. Steganography</h3><p>Petitcolas <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2a\">[12]</a> defined steganography as the practice of hiding data in plain sight. The word steganography (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sigma \\tau \\varepsilon \\gamma \\alpha v$\n</tex-math></inline-formula> \u00f3 <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\zeta, \\gamma \\rho$\n</tex-math></inline-formula> \u03ac <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho-\\varepsilon \\iota \\nu$\n</tex-math></inline-formula>) has Greek origins. The root of the word is <i>steganos</i> which loosely translates to <i>cover</i>. The suffix <i>graphy</i> is based on the Greek word <i>graphein</i> which means <i>to write</i> <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2a\">[13]</a>. Steganography, or hidden writing, is today a method of hiding communications so those looking at the output are unable to discern the cover (overt message) hides a second meaning (covert message) of which only the sender and receiver are aware. To an observer unaware of the covert content, the communication is interpreted based solely on the overt cover.</p><p>In the modern computing era, steganography is a tool with alleged links to global terrorism and documented evidence of international espionage. According to Schmurr and Crawley <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2a\">[13]</a>, Osama bin Ladin and his conspirators used steganography to plan the September 11, 2001 attacks as well as the bombing of embassies in Tanzania and Kenya. In sharp contrast to <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2a\">[13]</a>, Kellen <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2a\">[14]</a> published a SANS Institute article indicating the direct links between bin Laden, the 911 attacks, and the application of steganography were tenuous. One crucial insight into the difference of opinion within the cybersecurity community on the terrorist-to-steganography connection was identified by the Dinca <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2a\">[15]</a> survey. Dinca concluded the tools used by scientists to confirm the use of steganography by bin Laden were not sophisticated enough to detect the commercial-grade steganographic tools used by terrorists circa 2001.</p><p>Despite the varied perspectives on terrorist misuse of steganography, the nefarious application of steganography by Russian spies is undisputed. U.S. Department of Justice (DOJ) charging documents, such as United States v. Metsos, make it clear steganography has been a threat to U.S. national security for more than a decade <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2a\">[16]</a>. For several years, the Federal Bureau of Investigation (FBI) conducted an investigation into spying by Russian Federation agents. The investigation concluded with the arrests of 10 spies who had been planted in various locations around the country including Arlington, Virginia and Seattle, Washington <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2a\">[17]</a>.</p><p>The FBI reported the goal of the Russian agents was \u201c\\ldots to become sufficiently \u2019Americanized\u2019 such that they can gather information about the United States for Russia, and can successfully recruit sources who are in, or are able to infiltrate, United States policy-making circles\u201d <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2a\">[19]</a>. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section 3</a>, subsection A.1.21 of the DOJ charging document describes how the Russian agents utilized Russian-created steganography tools to encode secret communications. The United States Government secured recordings of two of the defendants discussing how they used steganography to exchange classified data with their Moscow Center handlers.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Network Steganography</h3><p>The deployment of steganographic tools by the Russian government makes it clear steganography is a vehicle for Advanced Persistent Threats (APTs). As a consequence, the level of sophistication of APT actors (e.g., hacktivists, federal governments, military forces, etc.) is far greater than the capability of the average hacker. Hosmer <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a> stated highly skilled rogue agents have increased the complexity and detection resistance of the latest generation of data hiding methods. One of those methods is called <i>network steganography</i>. The SANS Institute <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2b\">[21]</a> classified network steganography as one of the most complex forms of modern steganography <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2b\">[22]</a>. Unlike the image-based steganography leveraged by the Russian Federation, network steganography uses network packets as cover data as opposed to the pixel values within image covers.</p><p>Basic network steganography techniques store covert data within unused locations inside network packet headers <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2b\">[23]</a>. Such methods are called storage channels. As a more advanced form of network steganography, a timing channel does not physically store covert data, so investigators are unable to later perform forensic examinations to understand the nature of the breach <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2b\">[24]</a>. A timing channel is considered the most complex network steganographic method because it adjusts packet transmission delays to hide data which increases the difficulty of both detection and prevention <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2b\">[25]</a>. This state-of-the-art form of network steganography was the method applied by the experiment herein.</p></div><div class=\"section_2\" id=\"sec2c\"><h3>C. Conventional Data Representation</h3><p>A computer represents data with binary code. Computer main memory, for example, can set an electrical charge above a predefined threshold voltage to represent the equivalent of a binary one bit. Conversely, if that charge is set below a specified threshold voltage or there is zero voltage, it represents a zero bit <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2c\">[26]</a>. Four bits equal a nibble and eight bits is a byte. Those bytes can have a decimal value of zero through 255. The American Standard Code for Information Interchange (ASCII) uses the first 128 values of the 256 possible outcomes to represent the numeric values 0-9, the upper case letters A-Z, the lower case letters a-z, as well as punctuation and control characters <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2c\">[27]</a>. Applying the ASCII 7-bit encoding scheme, the upper case letter \u2019A\u2019 is represented by the decimal value 65, the upper case letter \u2019B\u2019 has a decimal value of 66, and so on.<a ref-type=\"fn\" anchor=\"fn3\" class=\"footnote-link\">3</a> By employing recognized standards like ASCII and Unicode, each computer participating in a data exchange affords its user(s) a universally recognized mapping of computer values to human language elements. Without uniform standards, each computer manufacturer could have its own method for representing text that could be incompatible with others.</p></div><div class=\"section_2\" id=\"sec2d\"><h3>D. Timing Channel Data Representation</h3><p>The mapping of data onto diverse mediums is not limited to computing <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2d\">[28]</a>. There are many other ways to represent data. A common example is the oft-cited line from Longfellow\u2019s poem about Paul Revere: \u201cOne if by land, two if by sea\u201d <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2d\">[29]</a>. In the poem\u2019s encoding scheme, one lantern represented the enemy marching on land and two lanterns meant the British would invade via the Charles River. Per the Longfellow poem, light represented a storage medium for data. Time can also be utilized to transmit data <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2d\">[30]</a>. As described by Keller <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2d\">[31]</a> and Ganivev <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2d\">[32]</a>, a timing channel does just that by establishing temporal relationships between network packets so the receiver of those packets can determine whether it should decode a zero bit or a one bit. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a> contains a line graph illustrating how time delays can be used to represent bits of data.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe2-3227420-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe2-3227420-small.gif\" alt=\"FIGURE 2. - A timing channel places delays between network packet transmissions to encode data. Above, the sender transmits the letter \u2019F\u2019 by converting it to a decimal value of 70. The sender then transmits the binary representation of the number 70 one bit at a time in the form of delays between overt TCP segments. The receiver reverses the process. If the elapsed time since its previous reception of an overt TCP segment is less than 200 MS, the receiver stores a zero bit. If the delta is greater than or equal to 200 MS, the receiver stores a one bit. This process continues until the sender has transmitted all of the covert data.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>A timing channel places delays between network packet transmissions to encode data. Above, the sender transmits the letter \u2019F\u2019 by converting it to a decimal value of 70. The sender then transmits the binary representation of the number 70 one bit at a time in the form of delays between overt TCP segments. The receiver reverses the process. If the elapsed time since its previous reception of an overt TCP segment is less than 200 MS, the receiver stores a zero bit. If the delta is greater than or equal to 200 MS, the receiver stores a one bit. This process continues until the sender has transmitted all of the covert data.</p></fig></div><p class=\"links\"><a href=\"/document/9973314/all-figures\" class=\"all\">Show All</a></p></div></p><p>From the perspective of data thieves, the primary disadvantage of a timing channel is the level of effort required to create the algorithms needed to conduct a real-world exfiltration. A packet-level timing channel is the most difficult network steganographic method to code <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2d\">[33]</a> because it requires a deep knowledge of the TCP/IP finite state machine during development as well as a thorough understanding of latency during execution <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2d\">[34]</a>, <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2d\">[35]</a>. The programming languages used to create network steganographic programs also require considerable skill in order to generate IP datagrams and TCP segments <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2d\">[36]</a>, <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2d\">[37]</a>.</p><p>The payoff for a data thief willing to ignore the implementation difficulty is the degree of detection resistance demonstrated by a timing channel. Much like a sequence channel (i.e., a covert transfer mechanism that rearranges the transmission order of IP datagrams to encode covert data) the assailant need only alter the transmission pattern to create a timing channel. The result is a covert communication channel that is extremely hard to detect unless there are advanced controls in place to analyze statistical anomalies in the arrival times of network packets <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2d\">[38]</a>, <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2d\">[39]</a>. Another advantage for the data thief, identified by Schmidbauer and Wendzel <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_2d\">[40]</a>, is variations in packet timing introduced by multiple network hops can make detection even more difficult.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Experimental Approach</h2></div><div class=\"section_2\" id=\"sec3a\"><h3>A. Design</h3><p>The current study posed the question: Are the most sophisticated cloud-based intrusion detection and prevention technologies capable of thwarting the most complicated data exfiltration attack? The null hypothesis (H textsubscript 0) of the study stated there would be no difference in the successful transmission and reception of covert network packets regardless of the enabled or disabled state of cloud countermeasures. The research hypothesis (H textsubscript 1) was the complement of the null hypothesis. The study applied a quantitative method with a before-and-after experimental design in order to determine the relationship between bits-per-second as the dependent variable and the state of defenses as the independent variable <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_3a\">[41]</a>.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Dependent Variable</h3><p>The bits-per-second (bps) dependent variable for each of the experiments was continuous. Pilot testing in preparation for the experiment showed experiments using throughput measurements, like bits-per-second, can generate negative or positive skew. Unlike prior tests, which were conducted on an isolated Local Area Network (LAN) by Flowers <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_3b\">[42]</a>, the current study used multiple Internet hosts where latency, routing decisions, and the varying speeds of networks between client and server caused slight variations in data measurements <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_3b\">[43]</a>. The timing variations produced by routing decisions alone, as noted by Crepsi <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_3b\">[44]</a> and as previously mentioned by Schmidbauer and Wendzel <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_3b\">[40]</a>, can have a normalizing impact on a time-based frequency distribution. Due to those variations, the skew in the current study was minimized but not entirely eliminated. To accommodate the residual non-normal distribution, an alternate statistical test was chosen to determine statistical significance.</p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Independent Variable</h3><p>The independent variable selected was categorical. If the targeted CSP defense was disabled, the independent variable was equivalent to zero. If the targeted CSP countermeasure was enabled, the independent variable was recorded as one. The selected CSP\u2019s threat intelligence and IDPS services enabled the experimenter to drop suspicious packets, so the drop feature was enabled anytime the alert function was turned on. Enabling the dropping of suspicious network segments was necessary in order to ensure the dependent variable (bps) was impacted by the intervention of the CSP\u2019s countermeasures.</p></div><div class=\"section_2\" id=\"sec3d\"><h3>D. Variable Considerations</h3><p>A simple read-only alert may not have an affect on covert data throughput; however, it should be noted Rashid <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_3d\">[45]</a> found Deep Packet Inspection (DPI) deployed to detect network steganography can slow transfer speeds, so it is plausible the CSP\u2019s detective processing effort could have had an effect on the dependent variable. In either case, the selection of bits-per-second as a dependent variable was ideal because it can catch detective interventions as a side effect of slowed throughput due to DPI. Bits-per-second can also measure preventative intervention when dropped packets cause extended retransmission-based delays.</p></div><div class=\"section_2\" id=\"sec3e\"><h3>E. High Level Architecture</h3><p>Prior research on timing channel cloud vulnerabilities has focused on co-resident targets on the same cloud. As illustrated in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>, the current research deviates from the requirement for co-residency and instead investigates the threat posed by a single cloud agent bookended by two non-resident client and server co-conspirators. Inserting indirection between client and server is not new. Indeed, using one or more intermediaries is the core identity obscuring benefit of The Onion Router (TOR) and therefore the dark web itself <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_3e\">[46]</a>. The value of indirection is such that the sponsors of TOR include the U.S. National Science Foundation and the U.S. Department of Defense (DOD) Defense Advanced Research Projects Agency (DARPA) <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_3e\">[47]</a>, <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_3e\">[48]</a>. DARPA has funded the Advanced Research Projects Agency Network (ARPANET) which was the predecessor to the Internet, Messenger RNA research by Moderna which led to its COVID 19 vaccine, the Global Positioning System (GPS), and numerous other noteworthy technological innovations <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_3e\">[49]</a>.</p><p>The indirection used within this study is more akin to the use of proxies to centralize all outbound web traffic by performing Source Network Address Translation (SNAT) on outbound IP datagrams <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_3e\">[50]</a>. Indirection, however, can have a darker side. The key difference between the application of a web proxy and stegacloud is the former is intended to preserve privacy whereas the latter is designed to simulate subverting it. The primary objective of the data thief is to obtain possession of confidential data. The secondary objective is to avoid detection. Unfortunately, stegacloud enables a rogue but trusted insider to accomplish both.</p></div><div class=\"section_2\" id=\"sec3f\"><h3>F. Detailed Architecture</h3><p>As illustrated <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a>, three distinct Internet locations were required to replicate a real-world environment. The first site hosted the client as well as test instrumentation. The second site was the CSP that housed the stegacloud proxy executable. The third and final Internet site consisted solely of the server component. Each site was protected by a firewall preventing extraneous computers from participating in the experiment. The active TCP ports were 8080 on the stegacloud proxy and port 80 on the host running the server.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe3-3227420-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe3-3227420-small.gif\" alt=\"FIGURE 3. - The experimental environment consisted of three core components: A client (Step 1) generating synthetic steganograms, an instance of the stegacloud proxy at the CSP\u2019s site (Step 2), and a server (Step 3) receiving the steganograms generated by the client.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>The experimental environment consisted of three core components: A client (Step 1) generating synthetic steganograms, an instance of the stegacloud proxy at the CSP\u2019s site (Step 2), and a server (Step 3) receiving the steganograms generated by the client.</p></fig></div><p class=\"links\"><a href=\"/document/9973314/all-figures\" class=\"all\">Show All</a></p></div></p><p>During the experiments, the primary actors included the three conspiring agents described in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>, but the number of participating TCP/IP hosts was expanded to properly measure the covert exchanges. A data server supplied storage services for the test executable operating on the client as well as the Comma Separated Values (CSV) files that held the outcomes from each test. Once all tests were complete, the R Studio statistical analysis application referenced those files via the Common Internet File System (CIFS). The CIFS server enabled the client and R Studio to access the test data via a network drive.</p><p>The server operated on a separate network connected to a second Internet Service Provider (ISP). The IP address of the server was in the Internet Assigned Numbers Authority (IANA) private range so the internal computers were not directly addressable from the Internet without traffic first going through a firewall <a ref-type=\"bibr\" anchor=\"ref51\" id=\"context_ref_51_3f\">[51]</a>. The previously mentioned port 80 was opened on FW2. Destination Network Address Translation (DNAT) on FW2 enabled the stegacloud proxy to connect to FW2\u2019s public address, and port forwarding completed the configuration to facilitate proxy-to-server TCP segment exchanges. Similarly, the stegacloud proxy leveraged the CSP\u2019s DNAT functionality to translate client traffic received on its public IP address to a private IP address on port 8080.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Results</h2></div><div class=\"section_2\" id=\"sec4a\"><h3>A. Statistical Significance</h3><p>As summarized in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>, a Wilcoxon Ranked Sum test of the CSP\u2019s threat intelligence service indicated no statistically significant difference <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(p=0.703)$\n</tex-math></inline-formula> between the inactive and active state of its defenses. An identical test demonstrated no statistically significant difference <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(p=0.998)$\n</tex-math></inline-formula> between the active or inactive state of the CSP\u2019s IDPS service. Similarly, a Wilcoxon test with both threat intelligence and IDPS enabled concurrently resulted in no statistically significant difference <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(p=0.118)$\n</tex-math></inline-formula> between the enabled and disabled state of the defenses. All tests were conducted using an alpha of 0.05 vs. 0.01 to minimize the risk of a false-negative; however, all quantitative output values were greater than the alpha, so the null hypothesis was accepted for each of the experiments.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nStatistical analysis indicates countermeasures were ineffective</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe.t1-3227420-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe.t1-3227420-small.gif\" alt=\"Table 1- &#10;Statistical analysis indicates countermeasures were ineffective\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p><a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> also summarizes the correlation and effect size tests. A Pearson Point Bi-Serial Correlation Test returned absolute values less than the range defined as \u2019no relationship\u2019 between the before-and-after state of all defenses <a ref-type=\"bibr\" anchor=\"ref52\" id=\"context_ref_52_4a\">[52]</a>. The outcome suggests there was no evidence supporting a correlation between the categorical independent variable and the continuous dependent variable. The output of Cohen\u2019s D Effect Size tests corroborated the tests of statistical significance and the correlation tests. The effect sizes, for the individual before-and-after threat intelligence test as well as the individual IDPS service test, returned values in the \u2019no relationship\u2019 range. The concurrent threat intelligence and IDPS test produced a \u2019weak relationship\u2019 value. The mean of each group of tests was also measured based on the state identified in the <i>Defense</i> column in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. The mean throughput values were measured with a high of 4.469 bps and a low of 4.398 bps.</p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Test Execution</h3><p>During each test, the participating network hosts were monitored for anomalies that could cause overt TCP segment delivery failure. The synthesized TCP stack utilized for each test supported all features of the TCP protocol necessary to deliver exceptional reliability including retransmissions, fast retransmissions, Selective Acknowledgements (SACKs), congestion control, and a sliding window tuned for high reliability as opposed to maximum throughput. The result was overt transfer effectiveness of 100% (with 100% accuracy) from client to server. The covert data transfer effectiveness was 100% (with 99.22% accuracy). The 0.78% covert inaccuracy was attributed to variations in routing decisions which negatively impacted TCP segment synchronization used by timing channels to covertly transfer data. The dependent variable did not factor for covert accuracy; however, 99.22% is a reasonable target given the inherent volatility of a timing channel. The following sections describe observations made during pilot runs of the experiment. Those observations led to adjustments to the testing procedures; however, the reported measurements were taken subsequent to those modifications.</p><div class=\"section_2\" id=\"sec4b1\"><h4>1) Sub-Linear Observations</h4><p>During testing, there were instances when the per-packet covert-to-overt ratio conformed to the Square Root Law of Steganography (SRLS), but the total number of covert bytes transferred exceeded the square root of the overt bytes transferred. The Square Root Law of Steganography (SRLS) states the number of covert bytes must be less than the square root of the number of overt bytes in order to lower the risk of detection <a ref-type=\"bibr\" anchor=\"ref53\" id=\"context_ref_53_4b1\">[53]</a>. The SRLS has implications for the researcher seeking to replicate real-world environments during controlled testing. Increasing covert payload size beyond the SRLS increases the chance a detective control will be triggered <a ref-type=\"bibr\" anchor=\"ref54\" id=\"context_ref_54_4b1\">[54]</a>. Conversely, the larger the covert payload, the greater the rate of covert transfer. Accordingly, there is a tension between covert throughput and covert detection risk.</p></div><div class=\"section_2\" id=\"sec4b2\"><h4>2) Sub-Linear Root Cause</h4><p>As shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a>, graphing the linear growth of overt values and their square roots was instructive. The graph illustrates the relationship between holistic totals (i.e., the cumulative number of overt bytes transferred at any point in the transmission process) and the square roots of those incremental values. The reason some tests demonstrated per-packet SRLS compliance despite holistic non-compliance was due to sub-linear growth. The count of overt bytes scales in a linear manner whereas the square root of the same value scales in a sub-linear fashion.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe4-3227420-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe4-3227420-small.gif\" alt=\"FIGURE 4. - This line graph illustrates how packets can conform to the SRLS when sending each TCP segment, yet exceed the SRLS once a sample test is complete due to the sub-linear growth of square roots. Exceeding the cumulative SRLS can lead to false positives because the excess covert data increases detectability and gives an artificial advantage to the detective controls at the heart of the study.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>This line graph illustrates how packets can conform to the SRLS when sending <b>each</b> TCP segment, yet exceed the SRLS once a sample test is complete due to the sub-linear growth of square roots. Exceeding the cumulative SRLS can lead to false positives because the excess covert data increases detectability and gives an artificial advantage to the detective controls at the heart of the study.</p></fig></div><p class=\"links\"><a href=\"/document/9973314/all-figures\" class=\"all\">Show All</a></p></div></p><p>A review of steganography related scholarly literature revealed sub-linear growth of square roots was also analyzed in 2005 at the Cambridge University Computer Laboratory. Anderson <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_4b2\">[48]</a> described sub-linear growth during an investigation into the theoretical limits of steganographic covers. Additional work by Filler et al. <a ref-type=\"bibr\" anchor=\"ref55\" id=\"context_ref_55_4b2\">[55]</a> four years later further explored the SRLS as it applied to digital image steganography. The relationship between holistic transfer totals and packet-level covert byte counts observed herein demonstrates prior research on detection resistance in digital image steganography provides valuable insight in network steganographic contexts.</p></div><div class=\"section_2\" id=\"sec4b3\"><h4>3) Sub-Linear Adjustments</h4><p>To conform to the SRLS, the number of overt bytes was increased to ensure adequate cover data. Each TCP segment\u2019s overt data payload was increased to 1,000 bytes; thus, the covert-to-overt ratio for the timing channel in each test was one bit of covert data for every 8,000 bits of overt data: start-up packet notwithstanding. One thousand bytes of overt payload data was well within the TCP Maximum Segment Size (MSS) used for testing.<a ref-type=\"fn\" anchor=\"fn4\" class=\"footnote-link\">4</a> <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Equation 1</a> contains the SRLS constraint applied during testing.<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} c_{\\text {covert units }} \\ll \\sqrt{o}_{\\text {overt units }}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} c_{\\text {covert units }} \\ll \\sqrt{o}_{\\text {overt units }}\\tag{1}\\end{equation*}\n</span></span></disp-formula> where <i>c</i> is the count of covert bits/bytes transferred from the sender to the receiver and <i>sqrt(o)</i> is the square root of the number of overt bits/bytes transmitted. <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> applies <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Equation 1</a> to the measured transfer totals.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nCovert transfer tests of threat intelligence and IDPS services conformed to the SRLS</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe.t2-3227420-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe.t2-3227420-small.gif\" alt=\"Table 2- &#10;Covert transfer tests of threat intelligence and IDPS services conformed to the SRLS\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>An examination of <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> shows that at the packet level, the number of covert units (i.e., one covert bit per TCP segment) transferred was much less than the SRLS limit of 89.4 bits. At the sample level, there were 64 samples for each of the four defense states listed in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. For each of those samples, 16 bytes of covert data were transferred. Sixteen bytes of covert data is far less than the 359.1 byte maximum necessary to conform to the SRLS. Conformance to the SRLS constraints demonstrates no artificial advantage was given to the threat intelligence or IDPS countermeasures during the experiments.</p></div></div><div class=\"section_2\" id=\"sec4c\"><h3>C. Test Data</h3><p>As shown in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a>, the data for some of the tests was normally distributed, but in other tests, data showed a small degree of skew. Shapiro-Wilk Normality Tests quantitatively confirmed the skew observed in the visualization of the data. Rather than perform transformations on the data to impart normality, a T-Test of Independent Means was replaced by a Wilcoxon Ranked Sum Test. Unfortunately, the Wilcoxon Ranked Sum Test is not as strong as the T-Test of Independent Means. To account for the difference in statistical strength, the ranked sum test was bolstered by the aforementioned correlation and effect size tests to ensure statistical significance was supported by complementary measurements.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe5-3227420-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973314/flowe5-3227420-small.gif\" alt=\"FIGURE 5. - Despite apparent near-normal distribution in the visualizations above, the p-values reported by Shapiro-Wilk Normality Tests necessitated the use of non-parametric Wilcoxon Ranked Sum Tests.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Despite apparent near-normal distribution in the visualizations above, the p-values reported by Shapiro-Wilk Normality Tests necessitated the use of non-parametric Wilcoxon Ranked Sum Tests.</p></fig></div><p class=\"links\"><a href=\"/document/9973314/all-figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Discussion</h2></div><p>In a stegacloud data exfiltration scenario, the victim organization controls just one third of the infrastructure components required for the attack. The rightful data owner does not control the data thief\u2019s stolen data cache, nor does the lawful owner of the data have direct control over the CSP. A proactive security organization does, however, have one critical indirect tool at their disposal to reduce the risks associated with the CSP\u2019s role in the attack described above: Service Organization Control (SOC) 2 reports. The following sections will describe how SOC reports can provide assurances the CSP has controls in place to mitigate a stegacloud attack.</p><div class=\"section_2\" id=\"sec5a\"><h3>A. SOC 2: Overview</h3><p>As more on-premise data centers migrate to the cloud, instances of IaaS leakage will increase. Network steganography belongs to a large family of covert transfer techniques. The benefit of evaluating holistic data theft risk is no single technique will be overlooked. When enterprises evaluate prospective CSPs, they should ensure the service provider has an up-to-date SOC 2 report addressing a broad spectrum of covert threats. SOC 2 reports are issued by auditors who evaluate the design and operating effectiveness of controls within an organization promising safe and secure computing services <a ref-type=\"bibr\" anchor=\"ref56\" id=\"context_ref_56_5a\">[56]</a>. An organization may have multiple SOC 2 reports if it sells numerous services. The SOC 2 report clearly defines the CSP\u2019s control objectives as well as any weaknesses for which the auditor issued a finding.</p><p>Gartner Research publishes guidance for organizations with regard to SOC 2 reports which is specifically applicable to those considering cloud services <a ref-type=\"bibr\" anchor=\"ref57\" id=\"context_ref_57_5a\">[57]</a>. Based upon Gartner guidance, as well as the outcome of the network steganography tests above, consumers of cloud services should:\n<ul style=\"list-style-type:disc\"><li><p>Thoroughly review the control objectives to determine if data exfiltration controls (e.g., network steganography) are covered by the SOC 2 report.</p></li><li><p>Check the User Control Considerations (UCCs) section of the SOC 2 report to see if the CSP places data exfiltration responsibilities on the client.</p></li><li><p>Examine logging features and ensure virtualization deployment and destruction records are kept.</p></li></ul></p><p>Unless an organization is an existing customer or is willing to sign a Non-Disclosure Agreement (NDA), obtaining a SOC 2 report may be difficult. Understandably, CSPs do not want to publicly broadcast security weaknesses. As an alternate first step, the American Institute of Certified Public Accountants (AICPA) suggests the use of a SOC 3 report <a ref-type=\"bibr\" anchor=\"ref58\" id=\"context_ref_58_5a\">[58]</a>. It is a less private version of the SOC 2 report and is frequently publicly available. Ernst and Young <a ref-type=\"bibr\" anchor=\"ref59\" id=\"context_ref_59_5a\">[59]</a> contains an example of a SOC 3 report. The SOC 3 report is a positive indication management\u2019s assertions have been tested by an independent third party. The AICPA <a ref-type=\"bibr\" anchor=\"ref60\" id=\"context_ref_60_5a\">[60]</a> also issues comprehensive guidance on the UCCs described above. The key to these attestations and audits is that the CSP is evaluated against the full suite of AICPA Trust Services Criteria which consists of security, confidentiality, processing integrity, privacy, and availability <a ref-type=\"bibr\" anchor=\"ref61\" id=\"context_ref_61_5a\">[61]</a>. The threat of network steganography intersects with several of those areas, so SOC 3 report language specifically addressing network steganography is a good sign the CSP has thought through all potential risks to the client\u2019s data.</p></div><div class=\"section_2\" id=\"sec5b\"><h3>B. SOC 2: Access Controls</h3><p>All existing employees and contractors, CSP or otherwise, should have capabilities limited to the level of privilege necessary to perform their job function. Accordingly, the SOC 2 auditor will also inquire about the levels of privileged access held by current employees and whether the level of access is appropriate given the employee\u2019s job responsibilities. The 2022 Verizon Data Breach Investigations Report (DBIR) found 13% of breaches were caused by misconfigurations largely associated with improper cloud data access controls, so cloud access reviews (privileged or normal) are critical <a ref-type=\"bibr\" anchor=\"ref62\" id=\"context_ref_62_5b\">[62]</a>. Certified evaluations by independent third parties ensures the risk-mitigating controls a CSP has in place are not simply one-time patches. SOC 2 reports give customers the confidence the CSP has had stringent processes in place for a period of time (i.e., the SOC 2 reports are retrospective, not prospective).</p></div><div class=\"section_2\" id=\"sec5c\"><h3>C. SOC 2: Records Retention</h3><p>The attack tested within this study focused on the misdeeds of a privileged user whose goal was to leverage the cloud as a stepping stone in a larger data exfiltration attack. Those criminal actions required the thief to become a CSP customer for just a few hours. The SOC 2 auditor will review the CSP\u2019s records of due diligence such as verifying the identity of its customers regardless of how long those customers actually subscribed. Another element of that due diligence is retention of all inbound and outbound TCP/IP connections made by/to a virtual machine running on the CSP\u2019s platform. Given the attack simulated within this study, law enforcement could utilize that connection history even after the attacker deletes the virtual machine to cover their tracks <a ref-type=\"bibr\" anchor=\"ref63\" id=\"context_ref_63_5c\">[63]</a>.</p></div><div class=\"section_2\" id=\"sec5d\"><h3>D. SOC 2: Auditor Quality</h3><p>The prospective consumer of cloud services should also scrutinize the issuer (auditor) of the SOC 2 report. The Public Company Accounting Oversight Board (PCAOB) is the organization U.S. Congress empowers to ensure audit firms follow Auditing Standards (AS) when preparing their audit reports <a ref-type=\"bibr\" anchor=\"ref64\" id=\"context_ref_64_5d\">[64]</a>. In 2015, the PCAOB issued an enforcement action against an auditor for failure to follow AS 7 (to be superseded by PCAOB AS 1220 in 2024 <a ref-type=\"bibr\" anchor=\"ref65\" id=\"context_ref_65_5d\">[65]</a>) which requires an auditor to obtain an Engagement Quality Review (EQR) before delivering the outcome of an audit to a client <a ref-type=\"bibr\" anchor=\"ref66\" id=\"context_ref_66_5d\">[66]</a>, <a ref-type=\"bibr\" anchor=\"ref67\" id=\"context_ref_67_5d\">[67]</a>. In this case, the client was a cloud-based provider of data storage services. The auditor was a sole proprietor and therefore likely unable to internally fulfil the requirement of having the partner-level secondary review mandated by AS 7. Size, credibility, and experience are essential attributes of an audit firm as it relates to assessing CSP countermeasure effectiveness given the technical complexity of network-based hidden channels.</p></div></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VI.</div><h2>Future Research</h2></div><p>The architecture of the experiment conducted within the current study consisted of three primary TCP/IP hosts. Those hosts included a client, proxy, and server computers on separate networks. Future experimentation will investigate a two-tiered architecture with the client component, which represents the victim of the exfiltration, being located on the selected cloud platform. In such a configuration, the state of the exfiltration detection and prevention countermeasures on the client\u2019s cloud would serve as the independent variable. Another reasonable variant would be locating the server component on a cloud platform to examine the behavior of countermeasures when they are the destination of a timing channel exfiltration.</p><p>In either of the aforementioned test architectures, and unlike the current experiment, the CSP would serve as the direct object of the verification effort. Prospective consumers of cloud services could therefore leverage the analysis to improve their planned cloud deployments. Similarly, existing users of cloud platforms could leverage the experimental outcomes to bolster their current deployments via configuration changes. Such research could also inform their search for newly developed marketplace components specifically developed to mitigate the risk of timing channel exfiltration.</p></div>\n<div class=\"section\" id=\"sec7\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VII.</div><h2>Conclusion</h2></div><p>The experiment conducted herein demonstrated the detective and preventative capabilities of the selected CSP are lacking with regard to network steganographic countermeasures. Ultimately, from a customer perspective, the client who is consuming the services of a CSP must ensure their end-to-end controls are sufficient to mitigate the risk of data exfiltration. Further, there is an intersection between public company cloud-based platforms and the United States Cyber Command\u2019s vision of achieving superiority in cyberspace. The stated objective is to match America\u2019s \u201c\\ldots superiority in the air, land and space\\ldots \u201d to its capabilities in the cyber domain <a ref-type=\"bibr\" anchor=\"ref68\" id=\"context_ref_68_7\">[68]</a>.</p><p>The <i>Command Vision for U.S. Cyber Command</i> makes it clear adversaries of the United States of America plan to disrupt the economies of America and its allies as a means of warfare. Andress <a ref-type=\"bibr\" anchor=\"ref69\" id=\"context_ref_69_7\">[69]</a> noted the alarming fact that hackers benefit from the consolidation of disparate targets onto the cloud. As large enterprises continue to migrate currently dispersed operations onto the cloud-based resources of a relatively small number of large-scale cloud providers, an offensive against a single CSP could conceivably harm numerous unaffiliated corporations sharing the same cloud space. In addition to the two inherent risks evaluated by the experiment above, such centralization realizes a third Yale University identified inherent risk: shared resource vulnerability <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_7\">[1]</a>.</p><p>The current study revealed a considerable weakness as it relates to cloud platforms being used as an unwitting agent in a data theft. As noted by Yale University researchers, resource sharing and pay-as-you-go features are inherent to the cloud\u2019s core value, but those attributes are also prime vulnerabilities. Given the previously mentioned intersection between national and corporate weaknesses, it is in the common best interest of the private and military sectors to respond strategically to the threat of network steganography for the benefit of all.</p></div>\n<h3>ACKNOWLEDGMENT</h3><p>The author would like to thank Dr. Mary Dobransky, Dean of the College of Science and Technology and Douglas Rausch, Director of the Center for Cybersecurity Education at Bellevue University for their continued support during the research, writing, and publication of this article. Additionally, the author expresses sincere appreciation for the support and guidance of Dr. Ian McAndrew, Dean of Doctoral Programs at Capitol Technology University.</p></div></div>\n"
}