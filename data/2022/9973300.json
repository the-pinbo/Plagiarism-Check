{
    "abstract": "Due to the COVID-19 pandemic and the development of educational technology, e-learning has become essential in the educational process. However, the adoption of e-learning in sectors such as engineering, science, and technology faces a particular challenge as it needs a special Laboratory Learning Management System (LLMS) capable of supporting online lab activities through virtual and controlled r...",
    "articleNumber": "9973300",
    "articleTitle": "A Generic AI-Based Technique for Assessing Student Performance in Conducting Online Virtual and Remote Controlled Laboratories",
    "authors": [
        {
            "preferredName": "Ahmed M. Abd El-Haleem",
            "normalizedName": "A. M. Abd El-Haleem",
            "firstName": "Ahmed M.",
            "lastName": "Abd El-Haleem",
            "searchablePreferredName": "Ahmed M. Abd El-Haleem"
        },
        {
            "preferredName": "Mohab Mohammed Eid",
            "normalizedName": "M. M. Eid",
            "firstName": "Mohab Mohammed",
            "lastName": "Eid",
            "searchablePreferredName": "Mohab Mohammed Eid"
        },
        {
            "preferredName": "Mahmoud M. Elmesalawy",
            "normalizedName": "M. M. Elmesalawy",
            "firstName": "Mahmoud M.",
            "lastName": "Elmesalawy",
            "searchablePreferredName": "Mahmoud M. Elmesalawy"
        },
        {
            "preferredName": "Hadeer A. Hassan Hosny",
            "normalizedName": "H. A. H. Hosny",
            "firstName": "Hadeer A. Hassan",
            "lastName": "Hosny",
            "searchablePreferredName": "Hadeer A. Hassan Hosny"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227505",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9973300/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>The ultimate goal of digital learning is to provide a platform for a broader range of people to achieve large learning outcomes <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. In today\u2019s world, the phrase e-learning, sometimes known as digital learning attracts many researchers. The distribution of learning content (e.g. text materials, audio, video, assessment, and interactive methods) through any electronic media is referred to as e-learning. In comparison to traditional face-to-face learning, this can aid to boost learning levels. Various variables have been studied during the last decade to increase the performance of e-learning systems. These variables are the characteristics that can influence students\u2019 happiness and enhances the prediction of digital learning systems <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>.</p><p>E-learning does not allow students and professors to communicate directly. As a consequence, teachers face many difficulties with e-learning systems. From these difficulties is how the teacher will evaluate the course\u2019s effectiveness on students. Moreover, from the difficulties that faces the teachers, is that the students in e-learning courses can dropout more than students in traditional learning modes. Furthermore, it is difficult to evaluate a student\u2019s performance. Also, it\u2019s difficult to forecast at-risk pupils in new classes. Finally, of the difficulties that face teachers is how to estimate their students\u2019 projected outcomes on forthcoming tests <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>.</p><p>Due to the previous mentioned difficulties that faced the e-learning systems, many methods appeared to face these difficulties. One of the most effective recent methods is using Educational data mining (EDM) and Learning Analytics (LA) <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a> to predict the student\u2019s performance. EDM and LA are closely connected fields that attempt to aid in the analysis of educational data, due to the ever-growing data quantities which become increasingly difficult to process. As a consequence, EDM and LA offer tools and approaches for gathering, processing, and analyzing educational data. Most of the time, data mining, machine learning, and statistical analysis approaches use historical data to detect students\u2019 performance.</p><p>As mentioned previously most of the EDM and LA depend on historical data. As a consequence, the researchers have focused on a variety of data elements, including past exam results, background, demographic data, and the students\u2019 behaviours in and outside the classroom <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a> to predict student success. The majority of performance prediction research focuses on prior exam results. However, relatively little research appears to focus on data analysis, such as logging and analyzing student interactions with online platforms <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>.</p><p>It is found that using past scores on data analysis has two drawbacks: The first one is predicting the long-term performance of students from old collected data, which sometimes be inaccurate. For example, predicting a student\u2019s performance in junior or sophomore year courses based on exam grades and course grades from previous years. The second one is if they take grades from major1 and major2 exams to predict the course performance, it is too late to perform any corrective actions to help the student succeed this semester. As a consequence, in this paper, a new generic technique is proposed to automatically evaluate lab asks based on the mouse dynamics of the students, not the historical data of students.</p><p>The newly proposed technique will detect and improve the student flaws besides providing a reliable system to evaluate and assist learning staff. This research is providing the students with an intelligent automatic online performance evaluation module while performing the experiment. Furthermore, applying the performance evaluation module in the new LLMS provides an assistant, students friendly system, and simulates the atmosphere of a realistic laboratory. The system is tested with a customized dataset collected by the course instructors and teaching assistants, which is a record of mouse dynamics done by the student. Then, the feature extraction phase takes place to process data and provide meaningful information provided by the mouse actions. The extracted features will be introduced and processed by various Machine Learning (ML) Models to have a comparative analysis aiming to provide the best AI architecture for the task. Based on this, the contribution of this paper is summarized as follows:</p><ul style=\"list-style-type:disc\"><li><p>Design an effective LLMS that supports and manages various types of online labs such as virtual simulated labs and remote-controlled labs.</p></li><li><p>A new intelligent and automatic Evaluation Module is proposed as the main part of the new LLMS, which helps in assisting the stumbled students while performing the experiments by detecting the stumbled parts of the experiment and providing them with the correct help. Furthermore, it provides the intelligence of evaluating the steps of the experiment while performing the experiments in the online practical exams.</p></li><li><p>The proposed Performance Evaluation Module is designed to be generic, so that it can works with any simulation or control software for virtual and remote controlled lab experimentations.</p></li><li><p>The newly designed generic Performance Evaluation Module is based on the idea of collecting the mouse dynamics and using the AI algorithms for evaluating student performance during experiment run-time.</p></li><li><p>To the best of our knowledge, the proposed students\u2019 performance evaluation technique, is a new generic technique that provides a general interface with any type of lab software and remote controlled experiments at the least cost.</p></li></ul><p>The paper is organized as follows: <a ref-type=\"sec\" anchor=\"sec1\" class=\"fulltext-link\">Section I</a> presents an overview introduction of the paper. <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a> includes the previous literature review. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>, explain the newly proposed LLMS and performance evaluation module in detail. <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a> presents the experiments carried out on a number of students to assist them while performing the experiment remotely. <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a> presents a survey study that is performed on a number of students to evaluate the new system. Finally, <a ref-type=\"sec\" anchor=\"sec6\" class=\"fulltext-link\">section VI</a> presents the conclusion.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Literature Review</h2></div><p>Most of the approaches that have been developed for student performance prediction during performing an assignment were based on the student\u2019s historical data. Many data are recorded for students such as the student\u2019s previous grades and their overall performance are recorded. Although these methods helped to improve the student\u2019s performance, they couldn\u2019t automate the process of evaluation of the assignment.</p><p>Buena\u00f1o-Fern\u00e1ndez, et al. <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2\">[8]</a>, proposed a new research paper that predicts the student\u2019s performance based on previous and current exam grades. The target is to identify the students\u2019 \u201cat-risk\u201d for each subject and to provide real-time feedback on their current status. Such information would help in the development of appropriate remedial strategies to help these students and improve their retention rates. They used the Decision Tree algorithm in the prediction model of their work to classify students into passing and failing categories.</p><p>S. Huang and N. Fang <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2\">[9]</a> concentrated on forecasting student success in engineering dynamics. The authors used four prediction models in their study: multiple linear regression (MLR), multilayer perceptron network (MLPN), radial basis function network (RBFN), and support vector machine-based model (SVM). The goal of their research was to find the best prediction model and the most influential variable within a group of six variables that could lead to a more accurate prediction model. Only historic performance data were included in this study\u2019s dataset. It covered nine different grade dynamics as well as pre-requisite dynamic courses. This model, like the one described in <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2\">[8]</a> only looked at one type of student data (past performance-related data) and didn\u2019t take into account other factors like engagement or demographics.</p><p>On the other hand, some researchers such as Huan Wei, et al. <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2\">[10]</a> proposed new features based on student mouse movement trajectories to define students\u2019 problem-solving characteristics (e.g., think time, first try, and first drag-and-drop). Furthermore, a heterogeneous information network is used to integrate students\u2019 previous problem-solving knowledge on comparable topics in order to improve the student performance predictions on a new question. The authors used four Machine learning models. The results demonstrate that approached methodology can predict student performance in interactive online question pools with higher accuracy than the usual method of merely employing statistical variables (e.g., students\u2019 history question scores) in various models.</p><p>S. Liu and M. d\u2019Aquin <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2\">[11]</a>, used unsupervised learning to see how students\u2019 demographic factors and participation in online learning activities impact their learning outcomes. Their approach used the K-Prototypes clustering approach to identify student groups based on demographic features and interactions with an online learning environment. They used the results gathered for each group of students regarding their positive or negative learning outcomes to create online courses that cater to the diverse student\u2019s requirements. Furthermore, it helps the students to choose online courses that are most compatible for them.</p><p>One of the interesting approaches has been adopted by Brahim <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2\">[12]</a>, which used a real dataset acquired using a Digital Electronics Education and Design Suite (DEEDS) <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_2\">[5]</a> to predict students\u2019 performance throughout a series of online interactive sessions. The dataset records student engagement during online lab work in terms of text editing, keystrokes, time spent on each activity, and the test score earned every session. The suggested prediction model is made up of 86 new statistical variables that were semantically classified into three major groups based on criteria including activity type, timing statistics, and peripheral activity count. During the feature selection step, this collection of characteristics was further reduced, and only the most influential elements were kept for training. The suggested machine learning approach is designed to predict whether a student\u2019s performance would be poor or excellent. Random Forest (RF), Support Vector Machine (SVM), Nave Bayes, logistic regression, and multilayer perceptron were the five common classifiers employed in their research.</p><p>As presented in the previous research their work is based on predicting student performance, most of the contribution has been made based on student personality, historic performance, and platform interactions to determine the certainty of the student about his answer. Unfortunately, this may not help in determining the validity of the task outcome itself as most of the data and features extracted are based on the student only. As a consequence, to perform the task evaluation automatically and validation of students\u2019 answers, the research started to concentrate on another form of data which is mouse dynamics during performing the task. Many researchers have contributed to the use of mouse dynamics as data for AI models but not to evaluate tasks. Mostly, they were discussing the user authentication based on his mouse dynamics during sessions.</p><p>From the recent research that used the mouse, dynamics is the one proposed by T. Hu, et. al <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2\">[13]</a>. He used mouse dynamics deep learning as an approach for performing continuous user identification effectively and quickly while addressing insider threats. The research was based on an open-sourced dataset which is the Balabit Mouse Dynamics dataset <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\">[14]</a>. Experiments were carried out with 10 users. The dataset was mapped in the shape of an image to represent users\u2019 behavior, then they were introduced to a CNN model to be trained with. The findings confirmed the usefulness of the strategy. Achieved a false acceptance rate of 2.94 percent, a false rejection rate of 2.28 percent, and an authentication time of 7.072 seconds. These findings indicate that this method was capable of detecting insider threats in a particular setting.</p><p>M. Antal and N. Fej\u00e9r <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2\">[15]</a>, presented a new convolutional neural network for extracting characteristics from user mouse movement time series. They used several training modes, such as transfer learning and training from scratch were used in their model. Both authentication and identification system results are evaluated using the performance metric Receiver Operating Characteristics (ROC). The ROC curve is a graph that shows how well something works. The area under the curve (ROCAUC) is frequently used to compare the performances of different biometric systems, and it compares the true positive ratio (TPR) against the false positive ratio (FPR). The Balabit public dataset <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2\">[13]</a> was utilized for performance evaluation, however, the DFL data set <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2\">[16]</a> was used for transfer learning. From the gathered results, it is found that the novel used 1D-CNN model outperforms the other CNN models presented for the same job, especially with using transfer learning.</p><p>It has been noticed also that the sequence of events and actions in time series affects the prediction process. Accordingly, the authors in <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2\">[17]</a> have implemented a new approach in user authentication using LSTM and CNN-LSTM hybrid model aiming to take advantage of the sequential form in data related to mouse dynamics. The research has experimented with those models using the Balabit Mouse Dynamics Challenge <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\">[14]</a> and the Wolf of SUTD (TWOS) <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2\">[18]</a> datasets. Convolutional networks, recurrent networks, and a hybrid model combining convolutional and recurrent layers are used in the study to classify mouse movement sequences. It is well known that training these networks on small datasets with random initialization of weights results in models that perform badly.</p><p>As a consequence, the authors explore a two-dimensional convolutional neural network using transfer learning, which is a domain adaptation strategy that works well with tiny datasets. Although using such a design may appear contradictory, it has outperformed all other deep architectures tested, as well as a traditional machine learning approach, because the temporal information is eliminated from the input data. To compute relevance scores for each component of the mouse curves, the suggested approach used the layer-wise relevance propagation (LRP) technique. The models are also evaluated in terms of their usefulness and efficacy in real-world circumstances.</p></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Proposed System</h2></div><p>The proposed system describes first the new intelligent generic Laboratory Learning Management System (LLMS) with a performance evaluation module and virtual tutor. Afterward, the operation of the virtual assistant and Lab evaluation component that exists in LLMS is described in detail.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. The Proposed Generic LLMS</h3><p>The new intelligent generic LLMS provides many features that help the students and teachers while assigning and performing experiments online. One of the main features of the new LLMS system is the performance evaluation module, which automatically triggers the virtual assistant service to detect if the students need help or not and show suitable help while performing online experiments. This new feature is provided in the new LLMS system with the intelligence of providing help to students in a suitable form such as text, audio, or video help according to the student\u2019s cognitive style <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_3a\">[19]</a>. Furthermore, if the students need extra help by performing chatting with the tutor or colleague, the session collaborator\u2019s help from audio, text, and video are used <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_3a\">[19]</a>. Also, the performance evaluation module helps in evaluating the conducted experimental steps of students while performing the practical exams by collecting the mouse dynamics behavior of the students and triggers the auto-grading module to evaluate the steps of the experiment performed by the students.</p><p>The architecture of the new generic LLMS system is presented in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1</a>. The LLMS system is proposed to allow the teacher and students to assign and perform virtual and remote controlled lab experiments and practical exams online.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel1-3227505-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel1-3227505-small.gif\" alt=\"FIGURE 1. - The Proposed Generic LLMS system.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>The Proposed Generic LLMS system.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>Furthermore, the new LLMS system provides teachers with the simplicity of designing the hardware and software experiments remotely. The new LLMS system added the performance evaluation module that triggers the virtual assistant service, which acts as a virtual tutor that helps the students while performing the experiments. As the performance evaluation module detects that the student needs help while performing the experiment, and then triggers the virtual Assistant service to open one of the available text, audio, or video help according to the student\u2019s cognitive style. If the students still need help so a Session Collaborator will be used to open a chatting session with the teacher assistant or colleague, which may be audio, video, or text help automatically through the Experimental Runtime Engine Module. Furthermore, the new LLMS system can be used to assign practical exams to students and after collecting students\u2019 answers, the performance evaluation module collects and studies the student\u2019s mouse behavior to evaluate the student\u2019s performing the experimental steps and then triggers the auto-grading service that exists to performs automatic grading for exams. Also, the new LLMS system is considered generic because it is used to add any type of software for virtual or remote controlled experiments using a single generic user interface.</p><p>As shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1</a>, the new generic LLMS system actors are the teacher and students. The teacher accesses the LLMS\u2019s system through an internet connection. Afterward, the teacher starts to design the online experiment or exam using the authoring tool. The new authoring tool consists of the following:</p><p>The Generate Experiment Content Package is used to design the experiments or exams.</p><p>The Content Component Assets are used to add static draggable components for experiments such as text, images, tables, etc.</p><p>The VRL-specified Component Assets are used to add the simulator component as part of experimental exams or experiments by automatically and remotely accessing the VM using the Myrtille technology. Moreover, the VRL-specified Component Assets are used to open sessions to control the hardware that exists physically in the laboratory.</p><p>The Lab Learning Object (LLO) represents the generated designed experiment or practical exam by a teacher, stored as a JSON object, which is stored in the LLMS database.</p><p>The new LLMS\u2019s performance evaluation Module provides the system with the new intelligent virtual assistant service and auto-grading components. The main idea of the performance evaluation Module is to collect the students\u2019 mouse dynamics while performing the experiment in the runtime using the Experiment Runtime Engine Module. Afterward, those mouse dynamics are stored in the Learning Record Store (LRS) in the LLMS database. The LRS stores all the learning information associated with students as defined by the xAPI spec documentation. Afterward, the performance evaluation Module runs the AI algorithm to detect if students need help. Then it triggers the virtual Assistant service to read the stored mouse dynamics from LRS through Experiment Runtime Engine Module to automatically show the correct help for the students while performing the experiment. Furthermore, the performance evaluation Module runs the AI algorithm to detect if students performed the experimental steps correctly while performing a practical exam and triggers the auto-grading module to automatically evaluate and grade the experimental steps performed by the students.</p><p>The performance evaluation module is used to provide the intelligence of the new LLMS system by learning from the behavior of students performing the parts of the experiment if they need help triggers the e-learning virtual assistant service to provide the students with suitable help or open chatting with other using the Session Collaborator while performing an experiment using the mouse dynamics behavior.</p><p>The Experimental View is used to preview the shape of the designed experiment or practical exam interface while designing them using the authoring tool.</p><p>After designing the experiment or the practical exam using the authoring tool, the generated LLO objects stored in the LLO store of the LLMS database are scheduled and assigned to one of the available resources in the laboratory server using the Lab Resource Manager by the Experiment Runtime Engine Module.</p><p>The students access the assigned experiment or practical exam through the internet remotely by using the Students Remote Access Interface, which is used to read the LLO assigned to the student from the LLMS database using the Experiment Runtime Engine module. Furthermore, the Experiment Runtime Engine module will run the Resources manager to assign the user one of the VM or physical machines that exists on the server. If the LLO includes the e-learning virtual Assistant component, the Experiment Runtime Engine module will run the performance evaluation module which collects the mouse dynamics and store it in the LRS store, then triggers the virtual assistant service to act as a virtual tutor. However, if the students are performing a practical exam, the performance evaluation module which collects the mouse dynamics will evaluate the experimental steps performed by students and triggers the auto-grading module to perform auto-grading for students\u2019 exams.</p><p>Finally, after the students performed the experiment and all the required added services are run, the output of the experiment will be stored in an LRO (Learning Report Outcome) and stored in the LRO store of the LLMS database student\u2019s profile. If the students finished performing a practical exam, the answers and the student\u2019s evaluation of performing the experimental steps will enter an auto-grading module, to mark the students\u2019 answers and store both the answers and the student\u2019s Lab evaluation in an LRO (Learning Report Outcome) stored in the student\u2019s profile in LLMS database through the Experimental Runtime Engine Module.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. The Proposed Performance Evaluation System</h3><p>The purposed performance evaluation system for the experiments can be represented in iterative processes as in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a>. A case study for a lab\u2019s experimental task scenario as presented in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a> is used as a running example to show how the performance evaluation system works. The main function of this experiment is to have a house intruder system along with a motion detector for the home light system.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel2-3227505-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel2-3227505-small.gif\" alt=\"FIGURE 2. - New performance evaluation System.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>New performance evaluation System.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel3-3227505-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel3-3227505-small.gif\" alt=\"FIGURE 3. - Lab Scenario for the study evaluation.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Lab Scenario for the study evaluation.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>The total lab is divided into a number of sequenced tasks. After each task, if the student did not solve it right, the system will provide a hint for him to help. So, there are a specific number of checkpoints for the lab, after each checkpoint, the system evaluates student steps and according to this, it provides a hint. In case he solved the lab incorrectly again, the system provides another hint and this process will be looped till there is no hint left. Then the system will advise the student to have a meeting with the teaching assistant associated with the course.</p><p>Based on <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a>, the dataset for mouse dynamics for a number of students performing an online lab task on LLMS is collected, then the process of action segmentation is performed to divide the collected mouse dynamics of the students while performing the lab experiments into a number of actions. Next, the collected mouse actions will enter the extraction stage to extract the main features as will be explained later. Afterward, the lab evaluation system is used to detect if the students performed lab experiment steps (i.e., tasks) correctly or not. Then, a number of AI algorithms are used in the prediction, to detect if students performed steps correctly or need help. Finally, if the students perform an exam so an evaluation is performed by the performance evaluation system, and the grade is approved, and add results through the auto-grading Module to LRO as explained in the previous section. However, if Manual Grade Validation is needed so use the teacher assistant or instructor to perform grading and store the result in the LRO. The reason behind choosing the mouse dynamics to evaluate the practical lab was due to the hardware dependency in using unstructured data such as images. According to Xin, et al. <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_3b\">[20]</a>, one of the main advantages of using traditional machine learning techniques is the low computational power needed for it in relation to complex deep learning models such as CNNs, LSTMs, Hybrid models and reinforcement learning. Therefore, this gives traditional machine learning an edge over Complex models with respect to hardware dependency. That is why the whole research and the proposed system is based on mouse dynamics instead of images or unstructured data, to reduce the hardware dependency. In the following subsection, each process can be discussed explicitly and in more detail.</p><div class=\"section_2\" id=\"sec3b1\"><h4>1) B.1 Dataset</h4><p>A lab experimental experiment built on the new LLMS as presented in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a> is divided into a number of pages (i.e., tasks), each task consists of a number of steps and acts as a separate experiment that depends on the previous task and with increasing hardness. Since there were no available datasets that simulate a general experimental task without depending on specific software. As a consequence, it was a necessity to build such a dataset <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_3b1\">[21]</a> to support the generic feature of the new LLMS. The lab experimental task that was suggested in this research was based on TinkerCad online circuit simulator <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_3b1\">[22]</a>. The TinkerCad simulator was used in the experiment to train the student to build a low-level circuit design and to evaluate the student\u2019s concepts regarding building the circuit. The purposed task scenario in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a> for the lab experimental task is as follows:</p><ul style=\"list-style-type:disc\"><li><p>Grab the Arduino Uno to the simulation area and attach a LED to it, then connect the Anode pin to Pin 13, and the Cathode side to GND.</p></li><li><p>Grab the PIR sensor to the simulation area and attach it to the Arduino, then connect the signal pin to Pin 8, the power pin to 5V, and the ground pin to GND.</p></li><li><p>Grab the ultrasonic sensor and attach it to Arduino, then connect the GND pin to GND, VCC pin to 5V, and SIG pin to Pin 6. Afterward, grab the Buzzer and attach it also to the Arduino by connecting the Positive pin to Pin 7, and the Negative pin to GND.</p></li></ul><p>Each page (i.e., stage) of the lab experiment has been evaluated separately as a task using the LLMS. The inputs that have been provided as data to the models are the mouse dynamics which are the Clicks, Moves, and Drags. When the student is assigned an experiment, so the Experiment Runtime Engine Module in the new LLMS will run a new python script <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_3b1\">[23]</a> on the student\u2019s machine to fetch activities made by the mouse and gives all details about it such as the time when the action was taken which are represented by record Time, and client Time. Also from the fetched activities, details are the coordination of x and y of the mouse during this action. It is important to mention that the X-coordinates and Y- Coordinates provided by the system are adjusted with the standard ratio for the common PC screens 16:9 to fit in the resolution of 1920:1080, to avoid the difference in screens resolutions among all PCs. Furthermore, the button used for the action (i.e., button), and the type of action itself (i.e., state) are from the fetched data. A sample of these collected raw data from the python script is presented in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel4-3227505-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel4-3227505-small.gif\" alt=\"FIGURE 4. - Sample of Data Rows fetched by the python script in task 1.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Sample of Data Rows fetched by the python script in task 1.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>The dataset was divided into 3 partitions, which are task 1, task 2, and task 3, each with more than 600 instances of actions made. All three tasks are stages to construct the practical lab as a whole project.</p><p>After collecting the Raw data as presented in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a>, the data need to be reconstructed to a more meaningful form for further processing using Action Segmentation. This was accomplished by converting all the collected raw data instances to action events. For example, the movement of the mouse between two screen coordinates is described by a mouse action, which is a sequence of sequential mouse events. Because each event includes a screen position, a mouse action with n events is represented as a series of n points. Each point is representing an instance. Each session\u2019s data was divided into three categories of mouse actions. Ahmed and Traore <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b1\">[24]</a> identified these action kinds as MM, PC, and DD. A general mouse movement action is denoted by the letter MM. The mouse pointer is simply moved between two spots on the screen in this operation. PC stands for point and click. This motion is similar to mouse movement, except it concludes with a mouse click. Drag and drop action is denoted by the letter DD, and it always begins with a left mouse button pushed event, then a series of mouse drag events, and finally a left mouse button released event.</p><p>For example, in a PC-type action, the segment\u2019s ending is marked when the last event of a mouse-released event is a mouse-pressed event. Otherwise, a DD-type action is used to signal the end of a segment. Before concluding in a PC or DD-type action, these segments may include one or more MM-type actions. The MM activities are segregated in the second phase using a time field threshold. When the time gap between two consecutive events exceeds a threshold, the segment is split into two actions. The time limit has been set at 10 seconds. Finally, actions with less than four occurrences were disregarded using the spline interpolation, which requires at least four points.</p><p>Finally, after collecting Raw data and performing Action segmentation now features can be extracted as will be presented in the next subsection.</p></div><div class=\"section_2\" id=\"sec3b2\"><h4>2) B.2. Feature Extraction</h4><p>The feature extraction is performed to extract the main features needed to train the AI algorithms. As mentioned in the previous subsection Action segmentation is used to extract the mouse actions from raw data. Afterward, the feature extraction is used to extract statistical features for each action segment, the research has adopted the approach that has been studied by Gamboa and Fred in <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b2\">[25]</a>, as they used the 3 data rows only from each action segment which are <b>Client time, x and y</b> coordinates. However, <b>buttons and states</b> were used in the segmentation process. Hence each action is represented as a sequence of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left ({\\boldsymbol {t}_{ \\boldsymbol {i}},\\mathrm { } \\boldsymbol {x}_{ \\boldsymbol {i}}, \\boldsymbol {y}_{ \\boldsymbol {i}} }\\right)$\n</tex-math></inline-formula> for each <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the number of mouse records in each segment.</p><p>The angle of the path tangent with the x-axis was calculated using the sequence of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left ({x,y }\\right)$\n</tex-math></inline-formula> by <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">(1)</a>:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\boldsymbol {\\alpha }_{ \\boldsymbol {i}}=\\text {atan} 2{\\left ({\\frac {\\delta \\boldsymbol {y}_{ \\boldsymbol {i}}}{\\delta \\boldsymbol {x}_{ \\boldsymbol {i}}} }\\right)\\mathrm { }}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\boldsymbol {\\alpha }_{ \\boldsymbol {i}}=\\text {atan} 2{\\left ({\\frac {\\delta \\boldsymbol {y}_{ \\boldsymbol {i}}}{\\delta \\boldsymbol {x}_{ \\boldsymbol {i}}} }\\right)\\mathrm { }}\\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {y}_{ \\boldsymbol {i}}=\\mathrm { } \\boldsymbol {y}_{ \\boldsymbol {i}}- \\boldsymbol {y}_{ \\boldsymbol {i-1}},\\delta \\boldsymbol {x}_{ \\boldsymbol {i}}=\\mathrm { } \\boldsymbol {x}_{ \\boldsymbol {i}}- \\boldsymbol {x}_{ \\boldsymbol {i-1}},\\mathrm { \\boldsymbol {\\alpha }}_{ \\boldsymbol {1}}= \\boldsymbol {0}$\n</tex-math></inline-formula>, and the atan2 trigonometric function is a common variation of the standard arctangent function. Then the features: <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {v}_{ \\boldsymbol {x}},\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {y}},v,\\mathrm { }a,j, and w$\n</tex-math></inline-formula>, which are horizontal velocity, vertical velocity, velocity, jerk, and angular velocity, respectively were calculated based on the statistical feature extraction method proposed by <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b2\">[24]</a> using <a ref-type=\"disp-formula\" anchor=\"deqn2\" href=\"#deqn2\" class=\"fulltext-link\">(2)</a> and <a ref-type=\"disp-formula\" anchor=\"deqn3\" href=\"#deqn3\" class=\"fulltext-link\">(3)</a>:<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\boldsymbol {v}_{ \\boldsymbol {xi}}=\\frac {\\delta \\mathrm { } \\boldsymbol {x}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}},\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {yi}}=\\frac {\\delta \\mathrm { } \\boldsymbol {y}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}} \\boldsymbol {,}\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {i}}=\\sqrt {v_{xi}^{2}+v_{yi}^{2}}\\tag{2}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\boldsymbol {v}_{ \\boldsymbol {xi}}=\\frac {\\delta \\mathrm { } \\boldsymbol {x}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}},\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {yi}}=\\frac {\\delta \\mathrm { } \\boldsymbol {y}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}} \\boldsymbol {,}\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {i}}=\\sqrt {v_{xi}^{2}+v_{yi}^{2}}\\tag{2}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta \\boldsymbol {t}_{ \\boldsymbol {i}}=\\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}- \\boldsymbol {t}_{ \\boldsymbol {i-1}}\\mathrm { }, \\boldsymbol {v}_{ \\boldsymbol {x1}}= \\boldsymbol {v}_{ \\boldsymbol {y1}}=\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {1}}=0$\n</tex-math></inline-formula><disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\boldsymbol {a}_{ \\boldsymbol {i}}=\\frac {\\delta \\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}},\\mathrm { } \\boldsymbol {j}_{ \\boldsymbol {i}}=\\frac {\\delta \\mathrm { } \\boldsymbol {a}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}},\\mathrm { } \\boldsymbol {w}_{ \\boldsymbol {i}}=\\frac {\\delta \\mathrm { \\boldsymbol {\\alpha }}_{ \\boldsymbol {i}}}{\\delta \\boldsymbol {t}_{ \\boldsymbol {i}}}\\tag{3}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\boldsymbol {a}_{ \\boldsymbol {i}}=\\frac {\\delta \\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}},\\mathrm { } \\boldsymbol {j}_{ \\boldsymbol {i}}=\\frac {\\delta \\mathrm { } \\boldsymbol {a}_{ \\boldsymbol {i}}}{\\delta \\mathrm { } \\boldsymbol {t}_{ \\boldsymbol {i}}},\\mathrm { } \\boldsymbol {w}_{ \\boldsymbol {i}}=\\frac {\\delta \\mathrm { \\boldsymbol {\\alpha }}_{ \\boldsymbol {i}}}{\\delta \\boldsymbol {t}_{ \\boldsymbol {i}}}\\tag{3}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta \\boldsymbol {a}_{ \\boldsymbol {i}}=\\mathrm { } \\boldsymbol {a}_{ \\boldsymbol {i}}- \\boldsymbol {a}_{ \\boldsymbol {i-1}},\\mathrm { }\\delta \\boldsymbol {v}_{ \\boldsymbol {i}}=\\mathrm { } \\boldsymbol {v}_{ \\boldsymbol {i}}- \\boldsymbol {v}_{ \\boldsymbol {i-1}},\\delta \\mathrm { } \\boldsymbol {\\alpha }_{ \\boldsymbol {i}}=\\mathrm { \\boldsymbol {\\alpha }}_{ \\boldsymbol {i}}- \\boldsymbol {\\alpha }_{ \\boldsymbol {i-1}},\\mathrm { } \\boldsymbol {a}_{ \\boldsymbol {1}}= \\boldsymbol {j}_{ \\boldsymbol {1}}=\\mathrm { } \\boldsymbol {w}_{ \\boldsymbol {1}}=0$\n</tex-math></inline-formula>.</p><p>One of the features that have been calculated is the curvature time series, which is the ratio between the angle change and the traveled distance. This feature was calculated based on the angle of a tangent with the x-axis and the length of the trajectory <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {s}_{ \\boldsymbol {i}}$\n</tex-math></inline-formula> which is reachable using <a ref-type=\"disp-formula\" anchor=\"deqn4-deqn5\" href=\"#deqn4-deqn5\" class=\"fulltext-link\">(4)</a>. The formula that will be used for having the curvature time series is <a ref-type=\"disp-formula\" anchor=\"deqn4-deqn5\" href=\"#deqn4-deqn5\" class=\"fulltext-link\">(5)</a>.<disp-formula id=\"deqn4-deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\boldsymbol {s}_{ \\boldsymbol {i}}=&amp;\\sum \\nolimits _{k=1}^{i} \\sqrt {x_{k}^{2}+y_{k}^{2}} \\tag{4}\\\\ \\boldsymbol {C}_{ \\boldsymbol {i}}=&amp;\\frac {\\delta \\mathrm { \\boldsymbol {\\alpha }}_{ \\boldsymbol {i}}}{\\delta \\boldsymbol {s}_{ \\boldsymbol {i}}}\\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\boldsymbol {s}_{ \\boldsymbol {i}}=&amp;\\sum \\nolimits _{k=1}^{i} \\sqrt {x_{k}^{2}+y_{k}^{2}} \\tag{4}\\\\ \\boldsymbol {C}_{ \\boldsymbol {i}}=&amp;\\frac {\\delta \\mathrm { \\boldsymbol {\\alpha }}_{ \\boldsymbol {i}}}{\\delta \\boldsymbol {s}_{ \\boldsymbol {i}}}\\tag{5}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {s}_{ \\boldsymbol {i}}=\\mathrm { } \\boldsymbol {s}_{ \\boldsymbol {i}}- \\boldsymbol {s}_{ \\boldsymbol {i-1}}\\mathrm { }, \\boldsymbol {s}_{ \\boldsymbol {1}}=0$\n</tex-math></inline-formula>, while the critical point of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {C}$\n</tex-math></inline-formula> is the low Curvature point with a threshold <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${ \\boldsymbol {TH}}_{ \\boldsymbol {c}}=0.0005$\n</tex-math></inline-formula>. Each statistical feature has been converted to four features, which are Mean Standard deviation, Minimal value, and Maximal value to be used as the main features of the action segment.</p><p>Besides those features, there were other features which are the type of action segment, and the length of the trajectory <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {s}_{ \\boldsymbol {n}}$\n</tex-math></inline-formula> length of the segment between two endpoints. Also, the time needed to perform this action is considered a feature. Aiming to reduce the direction of each action segment, the method of eight main directions, which was defined by <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b2\">[24]</a>, has been used.</p><p>There were other features which are the number of points, which represents the total number of mouse events in an action segment, the sum of projections, and the maximum deviation, which is the segment between the two endpoints, and the distance separating the trajectory\u2019s points that is the greatest. Also, the length of the trajectory divided by the segment between an action\u2019s two endpoints was used to calculate the straightness. Finally, the duration of the first segment of the action with positive acceleration was calculated to be used as a feature. A summary of the 39 features extracted is presented in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nModels hyper parameters.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t1-3227505-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t1-3227505-small.gif\" alt=\"Table 1- &#10;Models hyper parameters.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3b3\"><h4>3) B.3. Data PREPROCESSING</h4><p>After collecting the features, data preprocessing is performed to make sure that there is no mislabeling and to check the datatype of each feature. The number of actions that will be processed has been reviewed to consider the misbalancing of the data. The number of corrective actions to accomplish the lab task was 993 samples, while the number of the overall actions with wrong actions was 1027.</p><p>Moving from checking the imbalanced data, the distribution of feature values was concerned to clear any outliers in the data. Outliers are checked by making sure that there is a normal distribution for feature values. This can be checked using fast analysis as knowing the mean of the feature values, Min., Max., standard deviation, and percentiles. After that, the correlation between features was introduced to make sure that there is no high correlation between features. High correlation may cause overfitting in the model as it causes neural representations to become more redundant and can therefore reduce the expressive ability of neural networks. One may explicitly penalize the correlation between features as part of the objective function to eliminate this impact but doing so comes at a significant computing cost <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_3b3\">[26]</a>. The correlation between the weight vectors (or convolutional kernels) of various hidden units may also be penalized <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_3b3\">[27]</a>, but this method is less computationally expensive and only slightly more effective.</p><p>Furthermore, both strategies need the penalty strength to be manually adjusted. Instead of combating feature association with additional regularization methods, it would be preferable to just avoid it in the first place by dropping the highly correlated features. The correlation between two features is calculated using <a ref-type=\"disp-formula\" anchor=\"deqn6\" href=\"#deqn6\" class=\"fulltext-link\">(6)</a>.<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} r=\\frac {\\sum {\\left ({x_{i}-\\bar {x} }\\right)\\left ({y_{i}-\\bar {y} }\\right)} }{\\sqrt {\\Sigma \\left ({x_{i}-\\bar {x} }\\right)^{2}\\Sigma \\left ({y_{i}-\\bar {y} }\\right)^{2}}}\\tag{6}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} r=\\frac {\\sum {\\left ({x_{i}-\\bar {x} }\\right)\\left ({y_{i}-\\bar {y} }\\right)} }{\\sqrt {\\Sigma \\left ({x_{i}-\\bar {x} }\\right)^{2}\\Sigma \\left ({y_{i}-\\bar {y} }\\right)^{2}}}\\tag{6}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r$\n</tex-math></inline-formula> is the correlation coefficient, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x_{i}\\mathrm { },y_{i}$\n</tex-math></inline-formula> are the values of features x and y in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> sample, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {x},\\bar {y}$\n</tex-math></inline-formula> are the mean value of features x and y for the overall data.</p><p>The correlation table was visualized in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a>. As it appears, there is a high correlation between the standard deviation of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {v}_{ \\boldsymbol {x}}$\n</tex-math></inline-formula>, so the feature that represents the standard deviation of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {v}_{ \\boldsymbol {x}}$\n</tex-math></inline-formula> has been dropped. Also, there was a high correlation between the standard deviation of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v$\n</tex-math></inline-formula> and the maximal value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v$\n</tex-math></inline-formula>, so the maximal value for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v$\n</tex-math></inline-formula> was dropped. One of the most noticed high correlations was between features extracted from acceleration and jerk <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a,j$\n</tex-math></inline-formula> respectively. As there was a high correlation between the mean, standard deviation, and maximal value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula>, with standard deviation and maximal value for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. So mean of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula> was chosen to be in the features that are being used in the prediction and all other features were dropped, and it can be noticed the final form of features will be used in the prediction model as an input can be presented in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6</a>.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel5-3227505-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel5-3227505-small.gif\" alt=\"FIGURE 5. - Correlation map before dropping high correlated features.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Correlation map before dropping high correlated features.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel6-3227505-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel6-3227505-small.gif\" alt=\"FIGURE 6. - Correlation map for the finalized features.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Correlation map for the finalized features.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>After choosing the feature for the model, the data had to be divided between the training set and the testing set. As the whole samples are less than 2500, the data was divided with 70:30 ratios between training and testing, respectively. Finally, the whole data has been normalized using the standard scaler method as in <a ref-type=\"disp-formula\" anchor=\"deqn7\" href=\"#deqn7\" class=\"fulltext-link\">(7)</a>.<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} z=\\frac {x-\\mu }{\\sigma }\\tag{7}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} z=\\frac {x-\\mu }{\\sigma }\\tag{7}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x$\n</tex-math></inline-formula> is the current value, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu $\n</tex-math></inline-formula> is the mean value of all samples, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> is the standard deviation of all samples.</p><p>The reason behind using the standard scaler is to make it easier for the model to predict the pattern between data. The difficulty of the task being modeled might be made more difficult by differences in scales across input variables. As an illustration, a model may learn huge weight values when given big input values such as a range of hundreds or thousands of units. A model with high weight values is frequently unstable, which means that it may exhibit poor learning performance and sensitivity to input values, leading to a larger generalization error. So, normalizing data using a standard scaler reduce the scale of the data which leads to the stability of the model.</p></div><div class=\"section_2\" id=\"sec3b4\"><h4>4) B.4. Prediction Models</h4><p>In this section, the different prediction models used to train the collected raw data after the previous steps are explained in detail. Each model will be specified with its hyperparameters and architecture, also they will be illustrated in detail.</p></div><div class=\"section_2\" id=\"sec3b5\"><h4>5) Support Vector Machine</h4><p>Support Vector Machines (SVM) is a class of statistical machine learning techniques, that aims to lower the structural risk principle. SVM\u2019s feature gives it a strong foundation for use in a variety of pattern analysis issues <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_3b5\">[28]</a>. Vapnik et al <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3b5\">[29]</a> introduced this class for a two-class categorization issue. The SVM was further expanded to address challenges involving several classes of categorization. The foundation of SVMC relies on preprocessing training data points and attempts to create a hyperplane that can accurately segregate training data points. To locate a new data point\u2019s associated cluster, SVMC measures the distance between the data point and a chosen hyperplane. The labeled entries in the training data points denote the entry cluster. The kernel function is a distance function carried out in the measuring procedure Several distinct Kernel functions are depending on the type of item and how it is distributed, whereas the Kernel function would measure the distance between objects. Like other kinds of learning techniques, the performance of SVMC is unquestionably influenced by the Kernel function and the training data points, which must accurately reflect the clusters. There are two different kinds of data points in any problem: those that can be separated nonlinearly and those that can be separated linearly. It is a known fact that non-linearly separable data points can be separated using a linear function by increasing the dimension of the data points. This fact is utilized by SVMC, which then applies a map function to inject data points from the Hulbert space into a higher-dimensional subspace, where a kernel function is intended to linearly divide the data points. To generalize the model, a soft margin was used. A soft margin is used to allow some misclassification in the data, hoping to have a more generalized model, and the value of this soft margin has been selected to be 2. The radial basis function (RBF) was used as a kernel to increase the dimensionality of the data, which will help the model to find the hyperplane.</p></div><div class=\"section_2\" id=\"sec3b6\"><h4>6) Naive Bayes</h4><p>One of the most well-known Bayesian classification algorithms used in Naive Bayes is the NBC (Naive Bayes Classifier) feature, which is a straightforward algorithm structure with a high level of processing efficiency. The Naive Bayes classifier has the benefit of just requiring a modest quantity of training data to estimate the essential parameters which are the variable mean and variance. The entire covariance matrix is not required because independent variables are assumed, just the technique of estimating each variable is needed. Using the maximum likelihood estimation principle, naive Bayes classification will classify the data into the category with the highest probability <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3b6\">[29]</a> as appears in <a ref-type=\"disp-formula\" anchor=\"deqn8\" href=\"#deqn8\" class=\"fulltext-link\">(8)</a>.<disp-formula id=\"deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=Max\\left \\{{P\\left ({\\left.{ C_{1} }\\right |X }\\right),P\\left ({\\left.{ C_{1} }\\right |X }\\right),..P\\left ({\\left.{ C_{n} }\\right |X }\\right) }\\right \\}\\tag{8}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=Max\\left \\{{P\\left ({\\left.{ C_{1} }\\right |X }\\right),P\\left ({\\left.{ C_{1} }\\right |X }\\right),..P\\left ({\\left.{ C_{n} }\\right |X }\\right) }\\right \\}\\tag{8}\\end{equation*}\n</span></span></disp-formula> While sample <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\begin{aligned} X=\\left \\{{{\\begin{array}{l} A_{1},A_{2},\\ldots, \\\\ \\end{array}} A_{K}}\\right \\} \\end{aligned}$\n</tex-math></inline-formula> is an attribute <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$A_{j}$\n</tex-math></inline-formula>, Naive Bayes classification considers that the attributes are independent of each other, so to calculate conditional probability, <a ref-type=\"disp-formula\" anchor=\"deqn9\" href=\"#deqn9\" class=\"fulltext-link\">(9)</a> will be used.<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} P\\left ({\\left.{ X }\\right |C_{i} }\\right)=\\prod \\limits _{j=0}^{k} {P\\left ({\\left.{ A_{j}=x_{j} }\\right |C_{i} }\\right)}\\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} P\\left ({\\left.{ X }\\right |C_{i} }\\right)=\\prod \\limits _{j=0}^{k} {P\\left ({\\left.{ A_{j}=x_{j} }\\right |C_{i} }\\right)}\\tag{9}\\end{equation*}\n</span></span></disp-formula> Since the conditional probability of having a specific class given feature can be retrieved using <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">(10)</a>, so <a ref-type=\"disp-formula\" anchor=\"deqn9\" href=\"#deqn9\" class=\"fulltext-link\">(9)</a> can be substituted in <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">(10)</a>.<disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\frac {P\\left ({\\left.{ X }\\right |C_{i} }\\right)P\\left ({C_{i} }\\right)}{\\sum \\nolimits _{i=1}^{n} {P\\left ({\\left.{ X }\\right |C_{i} }\\right)P\\left ({C_{i} }\\right)}}\\tag{10}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\frac {P\\left ({\\left.{ X }\\right |C_{i} }\\right)P\\left ({C_{i} }\\right)}{\\sum \\nolimits _{i=1}^{n} {P\\left ({\\left.{ X }\\right |C_{i} }\\right)P\\left ({C_{i} }\\right)}}\\tag{10}\\end{equation*}\n</span></span></disp-formula> So, now the new formula for calculating <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$P\\left ({\\left.{ C_{i} }\\right |X }\\right)$\n</tex-math></inline-formula> can be represented in <a ref-type=\"disp-formula\" anchor=\"deqn11\" href=\"#deqn11\" class=\"fulltext-link\">(11)</a>.<disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\frac {\\prod \\nolimits _{j=0}^{k} {P\\left ({\\left.{ A_{j}=x_{j} }\\right |C_{i} }\\right)} P\\left ({C_{i} }\\right)}{P\\left ({X }\\right)}\\tag{11}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\frac {\\prod \\nolimits _{j=0}^{k} {P\\left ({\\left.{ A_{j}=x_{j} }\\right |C_{i} }\\right)} P\\left ({C_{i} }\\right)}{P\\left ({X }\\right)}\\tag{11}\\end{equation*}\n</span></span></disp-formula> Then, let\u2019s have a notation for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\frac {1}{P\\left ({X }\\right)}=\\mathrm { \\alpha }\\left ({\\mathrm {&gt;0} }\\right)$\n</tex-math></inline-formula>, so the final form for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$P\\left ({\\left.{ C_{i} }\\right |X }\\right)$\n</tex-math></inline-formula> can be represented in <a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\">(12)</a>.<disp-formula id=\"deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\mathrm {\\alpha }\\prod \\limits _{j=0}^{k} {P\\left ({\\left.{ A_{j}=x_{j} }\\right |C_{i} }\\right)} P\\left ({C_{i} }\\right)\\tag{12}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\mathrm {\\alpha }\\prod \\limits _{j=0}^{k} {P\\left ({\\left.{ A_{j}=x_{j} }\\right |C_{i} }\\right)} P\\left ({C_{i} }\\right)\\tag{12}\\end{equation*}\n</span></span></disp-formula></p><p>Now, <a ref-type=\"disp-formula\" anchor=\"deqn13-deqn14\" href=\"#deqn13-deqn14\" class=\"fulltext-link\">(13)</a> is the representation of the probability of having class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C_{i}$\n</tex-math></inline-formula>, knowing that dataset <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$D$\n</tex-math></inline-formula> with a number of samples <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N\\left ({D }\\right)$\n</tex-math></inline-formula>, and the number of samples categorized to class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C_{i}$\n</tex-math></inline-formula> is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N\\left ({C_{i} }\\right)$\n</tex-math></inline-formula>. While <a ref-type=\"disp-formula\" anchor=\"deqn13-deqn14\" href=\"#deqn13-deqn14\" class=\"fulltext-link\">(14)</a> is the representation of the probability of having an attribute in a given class, as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N\\left ({C=C_{i},A_{j}=x_{j} }\\right)$\n</tex-math></inline-formula> represents the number of samples when attribute <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$A_{j}$\n</tex-math></inline-formula> is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x_{j}$\n</tex-math></inline-formula> in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C_{i}$\n</tex-math></inline-formula>.<disp-formula id=\"deqn13-deqn14\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} P(C_{i})=&amp;\\frac {N(C_{i})}{N(D)\\mathrm { }} \\tag{13}\\\\ P(A_{j}=&amp;x_{j} |C=C_{i})=\\frac {N(C=C_{i},A_{j}=x_{j})}{N(C_{i})\\mathrm { }}\\tag{14}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} P(C_{i})=&amp;\\frac {N(C_{i})}{N(D)\\mathrm { }} \\tag{13}\\\\ P(A_{j}=&amp;x_{j} |C=C_{i})=\\frac {N(C=C_{i},A_{j}=x_{j})}{N(C_{i})\\mathrm { }}\\tag{14}\\end{align*}\n</span></span></disp-formula> Finally, by substituting <a ref-type=\"disp-formula\" anchor=\"deqn13-deqn14\" href=\"#deqn13-deqn14\" class=\"fulltext-link\">(13), (14)</a> in <a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\">(12)</a>, the final formula for calculating NBC can be presented in <a ref-type=\"disp-formula\" anchor=\"deqn15\" href=\"#deqn15\" class=\"fulltext-link\">(15)</a>.<disp-formula id=\"deqn15\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\mathrm {\\alpha }\\prod \\limits _{j=0}^{k} \\frac {N\\left ({C=C_{i},A_{j}=x_{j} }\\right)}{N\\left ({C_{i} }\\right)} \\cdot \\frac {N\\left ({C_{i} }\\right)}{N\\left ({D }\\right)}\\tag{15}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} P\\left ({\\left.{ C_{i} }\\right |X }\\right)=\\mathrm {\\alpha }\\prod \\limits _{j=0}^{k} \\frac {N\\left ({C=C_{i},A_{j}=x_{j} }\\right)}{N\\left ({C_{i} }\\right)} \\cdot \\frac {N\\left ({C_{i} }\\right)}{N\\left ({D }\\right)}\\tag{15}\\end{equation*}\n</span></span></disp-formula> Using this equation, NBC can be used in the classification problem of the study.</p></div><div class=\"section_2\" id=\"sec3b7\"><h4>7) K-Nearest Neighbours</h4><p>In the case-based learning approach known as KNN (K-Nearest Neighbours), all training data is retained for classification as in <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_3b7\">[30]</a>, and by knowing the nearest sample classification, the prediction of the test sample can be determined. It cannot be used in many applications, such as dynamic web mining for a big repository, because it is a slow learning approach. Finding some representatives to represent the entire training dataset for classification is one strategy used to increase its effectiveness. Another method is to develop an inductive learning model from the training dataset and use this model (i.e., representatives) for classification. Numerous current techniques, like decision trees and neural networks, were first created to construct such a model. The performance of different algorithms is one of the evaluating criteria <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_3b7\">[31]</a>. The fact that KNN is a straightforward but effective method for classifying data and that it is persuading as one of the best methods on the corpus of newswire stories from Reuters that motivates us. The main concept behind KNN is to measure the Euclidian distance between samples to classify based on the majority votes of samples clustered to a specific class using <a ref-type=\"disp-formula\" anchor=\"deqn16\" href=\"#deqn16\" class=\"fulltext-link\">(16)</a>.<disp-formula id=\"deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} d(p,q)=\\sqrt {\\sum \\limits _{\\mathrm {i=1}}^{\\mathrm {n}} (q_{\\mathrm {i}}-p_{\\mathrm {i}})^{2}}\\tag{16}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} d(p,q)=\\sqrt {\\sum \\limits _{\\mathrm {i=1}}^{\\mathrm {n}} (q_{\\mathrm {i}}-p_{\\mathrm {i}})^{2}}\\tag{16}\\end{equation*}\n</span></span></disp-formula> While <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$q_{\\mathrm {i}}$\n</tex-math></inline-formula> is the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> feature in, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p_{\\mathrm {i}}$\n</tex-math></inline-formula> is the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> feature in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p$\n</tex-math></inline-formula>. So, by comparing the test sample with all data provided to the model, the first K samples class majority determines the class of the test sample. A new implementation for KNN is presented in <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Alg. 1</a>, in order to optimize computational power and be compatible with the proposed case study with a large dataset.<div class=\"algorithm\" id=\"alg1\"><h4>Algorithm 1 K-Nearest Neighbours</h4><p>Input: Dataset: <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathrm {D=}\\left \\{{X_{1},X_{2}\\mathrm {,\\ldots,}X_{n} }\\right \\}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{i}=[x_{1i},\\mathrm { }x_{2i},\\ldots,x_{ki}]$\n</tex-math></inline-formula>, is the matrix of features representing sample <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, and the test matrix represented as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{\\mathrm {test}}=[x_{1t},\\mathrm { }x_{2t},\\ldots,x_{kt}]$\n</tex-math></inline-formula>, beside the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> value that presents the number of neighbors that will be relay on to classify. Output: The classification of the test sample.</p><p>for (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i =1$\n</tex-math></inline-formula> to n) do //<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the number of data samples</p><p>for (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j =1$\n</tex-math></inline-formula> to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>) do //<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> is the number of features</p><p>calculate <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left ({x_{\\mathrm {ji}}-x_{jt} }\\right)^{2}$\n</tex-math></inline-formula>// equation 16</p><p>add Value to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$sq 5$\n</tex-math></inline-formula>. end</p><p>append <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sqrt {sq} $\n</tex-math></inline-formula> in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$distance$\n</tex-math></inline-formula> array with class of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{\\mathrm {i}} 7$\n</tex-math></inline-formula>. end</p><p>sort <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$distance$\n</tex-math></inline-formula> array based on <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sqrt {sq} $\n</tex-math></inline-formula> value</p><p>for (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c =1$\n</tex-math></inline-formula> to K) do</p><p>increment element <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${class}_{distance[c]}$\n</tex-math></inline-formula> in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$class$\n</tex-math></inline-formula> array</p><p>end</p><p>return <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$argmax(class)$\n</tex-math></inline-formula> //output index of the element with maximum value</p><p>/<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} $\n</tex-math></inline-formula> end KNN algorithm. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} $\n</tex-math></inline-formula>/</p></div></p><p>As it appears in <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Alg. 1</a>, KNN consists of two main parts. First, calculates the Euclidian distance between the test sample and all data samples and sort them based on this distance. Then, collecting the votes refer to the classification of the test sample. Since the features extracted were of high dimensionality and complexity, it was needed to implement KNN from scratch with a simple modification to optimize the computational power needed for the model in large datasets. The K value that has been chosen for KNN was 8.</p><p>This value was evaluated using 3-Folds cross-validation. The choice of 3-Folds was based on the splitting of data in the main division of the data which was 70:30 for training and testing respectively. Cross-validation is a statistical technique for calculating the effectiveness (or accuracy) of machine learning models. When the amount of data is likely to be restricted, it is employed to prevent overfitting in prediction models. In cross-validation, the data are folded (or partitioned) into a predetermined number of folds, the analysis is performed on each fold, and the total error estimate is then averaged as in <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_3b7\">[32]</a>.</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} i$\n</tex-math></inline-formula> is the index of samples.</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} j$\n</tex-math></inline-formula> is the index of features in each sample.</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} sq$\n</tex-math></inline-formula> variable to save total squaring of difference between features of the test sample and data samples</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} distance$\n</tex-math></inline-formula> is the array that stores Euclidian distance along with the class (dimension <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_3b7\">[2]</a> [n])</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\ast} class$\n</tex-math></inline-formula> is the array that stores the number of samples in each class in the nearest K samples.</p><p>Cross-validation is used commonly to tune hyperparameters of a machine learning model to achieve the best values that avoid overfitting without degrading model performance.</p></div><div class=\"section_2\" id=\"sec3b8\"><h4>8) Decision Tree</h4><p>The decision tree is a dataset structure made up of leaves and decision nodes. One of the listed qualities that are picked at the node is indicated by a decision node. A leaf represents a class value. An internal node is displayed for each test result that might be obtained <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_3b8\">[33]</a>. The Decision tree in this study was built based on the Gini impurity method by suggesting the optimal from root to nodes reaching to the leaves. Although the features have high dimensionality, the maximum depth for the decision tree is limited to 8 nodes for maximum depth. This value was chosen empirically to avoid overfitting in the model, as decision trees are normally tending to overfit. Providing a tree with high depth will lead to an unreliable model for real-life applications. After fitting the model to the data, the tree produced can be presented in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig. 7</a>.\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel7-3227505-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel7-3227505-small.gif\" alt=\"FIGURE 7. - Decision tree for Automated lab evaluation system.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Decision tree for Automated lab evaluation system.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3b9\"><h4>9) Random Forest</h4><p>Random forest (RF) is a decision tree mashup using majority voting to create a classification or regression prediction, which is an ensemble training approach. Due to their ease of use, adaptability, and interpretability, they are highly well-liked. RFs are fundamentally a vast array of if-else expressions. In an RF, the inputted value is \u201ctested\u201d at each node of a decision tree. The total sample space for the entered value is indicated by each branch at that node. Data is finely divided and further classified by the multiple decision trees as the entered value moves down the RF <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_3b9\">[34]</a>. An RF\u2019s decision-making process is allegedly more transparent and understandable than that of virtually any other algorithm since it is hypothetically feasible to visually graph each decision tree in an RF. Furthermore, the performance of RFs is unaffected by the different magnitudes of each feature, making feature scaling unimportant.</p><p>When analyzing test data, RFs are known to be computationally inexpensive however, the cost of assessment for RFs is significantly higher than the cost of training. A very deep RF may be needed for training on a huge data set since several decision trees must be developed to handle all potential scenarios. Therefore, rather than being used, the model\u2019s training incurs practically all the computational expense. When it comes to categorize data, RFs are fairly effective and affordable; the only expensive step is the creation of the model. In an ensemble training method, there are several trade-offs to take into account.</p><p>In most learning models, noise, bias, and variation are the major sources of mistakes. Therefore, there is a good possibility that any training technique that can effectively reduce these mistakes will increase the precision and robustness of machine learning algorithms. In order to reduce the mistakes from each model, ensemble training combines several decisions from many models. The algorithms previously mentioned are structurally independent and only use a single model\u2019s classification decision to categorize a data point. The aggregate judgment of these weak classifiers frequently results in a moderate, averaged output, thanks to ensemble training, which allows several weak classifiers to be combined into a family. It is reasonable to assume that each family of poor classifiers will make mistakes while using ensemble training. It is done in such a way that the correlation between each set of weak classifiers is kept to a minimum.</p><p>As a result, the likelihood of a certain mistake may not be as common in other groups as it is in one group. So, the main challenge in RF is to choose the best hyperparameter that reduces and degrades the value of weaker classifiers. Hence, after many iterations on different combinations of n estimators, and the maximum depth of each decision tree, it was found that the optimal hyperparameters are 1600 for the number of decision trees in the random forest, where each tree with a maximum depth of 9 nodes.</p></div><div class=\"section_2\" id=\"sec3b10\"><h4>10) Artificial Neural Network</h4><p>Artificial neural network (ANN) is one of the most straightforward applications of the neural network. ANNs are motivated by the current theory of how biological neural networks function. Raw input is altered by a sequence of affine non-linear transformations in the form of multiplicative weights and additive biases as it is propagated through the network. A first-order iterative optimization algorithm, most frequently gradient descent, can be used to minimize the difference between model predictions and target values as ANNs are trained in a supervised environment.</p><p>This loss function can be represented as a difference between model predictions and target values. ANNs can estimate endlessly complicated functions since they are universal approximators. Thus, they serve as a key tool and a point of comparison in the unique application of deep learning to a topic <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_3b10\">[34]</a>. The model consists of different kinds of layers as it appears in <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig. 8</a>, and in the next part, the research will discuss the importance of each layer and its specification. First, is the dense layer which is representing the main component of the vanilla plane neural network. The study used Rectified Linear Unit (ReLU) function as an activation function for most of its layers except the output layer. ReLU function can be represented simply using <a ref-type=\"disp-formula\" anchor=\"deqn17\" href=\"#deqn17\" class=\"fulltext-link\">(17)</a>.<disp-formula id=\"deqn17\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} f\\left ({x }\\right)=\\begin{cases} \\displaystyle 0,&amp;x&lt; 0 \\\\ \\displaystyle x,&amp;x\\ge 0 \\end{cases}\\tag{17}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} f\\left ({x }\\right)=\\begin{cases} \\displaystyle 0,&amp;x&lt; 0 \\\\ \\displaystyle x,&amp;x\\ge 0 \\end{cases}\\tag{17}\\end{align*}\n</span></span></disp-formula>\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel8-3227505-large.gif\" data-fig-id=\"fig8\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel8-3227505-small.gif\" alt=\"FIGURE 8. - ANN Architecture for the automated lab evaluation system.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p>ANN Architecture for the automated lab evaluation system.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>The most crucial element in a neural network that determines whether a neuron will be engaged and moved to the next layer is the activation function. Simply said, it will determine how the neuron\u2019s input to the network is significant to the prediction process. The other component of the purposed neural network is the dropout layer. Dropout is a type of layer that is used in neural networks for avoiding overfitting. Its way of working is so simple, as it only ignores the result of some neurons in the previous layer to avoid having some neurons activate a specific result based on the trained data only and enforcing all the neurons in contributing to the prediction of the neural network as in <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_3b10\">[35]</a>. The model architecture was as follows: 9 layers excluding the input layer, the number of neurons in each layer is duplicating through the ANN from the starting layer with 64 neurons reaching 612 neurons in the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$5^{\\mathrm {th}}$\n</tex-math></inline-formula> layer.</p><p>Then, gradually the neurons reduced again reaching 32 neurons in the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$8^{\\mathrm {th}}$\n</tex-math></inline-formula> layer then in the output layer, it has only 1 neuron with sigmoid function which can be represented in <a ref-type=\"disp-formula\" anchor=\"deqn18\" href=\"#deqn18\" class=\"fulltext-link\">(18)</a>.<disp-formula id=\"deqn18\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} f\\left ({x }\\right)=\\frac {1}{1+e^{-x}}\\tag{18}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} f\\left ({x }\\right)=\\frac {1}{1+e^{-x}}\\tag{18}\\end{equation*}\n</span></span></disp-formula></p><p>All layers are separated with a dropout layer with the rate of 0.1 (i.e., 10% of the neurons of this layer would be turned off randomly through each iteration) for reducing overfitting. Now, the loss function and optimization equation are used for updating the neural network. First, the model used binary cross-entropy loss as a loss function to calculate the difference between model prediction and the true result of each sample which is represented in <a ref-type=\"disp-formula\" anchor=\"deqn19\" href=\"#deqn19\" class=\"fulltext-link\">(19)</a>.<disp-formula id=\"deqn19\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} l=-\\frac {1}{N}\\sum \\limits _{i=1}^{N} y_{i} \\cdot \\log \\left ({\\hat {y}_{i} }\\right)+\\left ({1-y_{i} }\\right)\\cdot \\log \\left ({1-\\hat {y}_{i} }\\right)\\tag{19}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} l=-\\frac {1}{N}\\sum \\limits _{i=1}^{N} y_{i} \\cdot \\log \\left ({\\hat {y}_{i} }\\right)+\\left ({1-y_{i} }\\right)\\cdot \\log \\left ({1-\\hat {y}_{i} }\\right)\\tag{19}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> is the number of samples entered in the iteration, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{i}$\n</tex-math></inline-formula> is the truth value of sample <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {y}_{i}$\n</tex-math></inline-formula> is the predicted result of the model for sample <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>.</p><p>For optimization of the model and updating the model weights based on data provided, ADAM optimizer <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_3b10\">[36]</a> was used for the ANN as it has proved its efficiency and relatability recently with applications with small datasets as this research. The batch size used for the model was 43, and the number of epochs was 100 epochs, which was chosen empirically so that after this number the model reaches its steady state.</p><p>In conclusion, all the hyperparameters used for each model can be summarized in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nModels hyper parameters.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t2-3227505-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t2-3227505-small.gif\" alt=\"Table 2- &#10;Models hyper parameters.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Performance Metrics and Results</h2></div><p>As the evaluation has high dimensionality, 6 models were used in training and predicting the needs of students for performance evaluation and provided correct help from their mouse dynamics behavior. Six different AI models are used for training and a fair comparison is performed between them to choose the best one, which gives the best result of correctly running the automatic performance evaluation system for evaluating the performed experimental steps while performing each experimental lab task scenario separately.</p><p>The AI Tranning set is collecting through the experiment design phase where the Instructor and Teacher Assistant will run the experiment by all possible correct experiment conduction methods. Afterwards, this dataset is used to train the AI algorithms with correct solutions. Regarding the complex circuits, the same idea is performed.</p><p>The performance metrics, which are used to compare the performance of 6 AI algorithms are the average weighted accuracy, Precision, Recall, F1-Score, and ROC Curve. Knowing that <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$TP$\n</tex-math></inline-formula> is the True Positive prediction for the model, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$FP$\n</tex-math></inline-formula> is the False Positive prediction for the model, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$TN$\n</tex-math></inline-formula> is the True Negative prediction for the model, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$FN$\n</tex-math></inline-formula> is the False Negative prediction for the model.</p><p>First, the representation of the predictions has to be provided using the confusion matrix for all the tasks with each model. As can be observed in <a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig. 9.a</a>, <a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig. 9.b</a>, and <a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig. 9.c</a>, SVM has a fine confusion matrix with a concentration in diagonal which will be getting concentrated more by making more complicated tasks in the lab. This is happening due to the kernel added to SVM which is RBF. Also, after checking the results, the model is not underfitting as the result of the training and testing sample is too close.\n<div class=\"figure figure-full\" id=\"fig9\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel9-3227505-large.gif\" data-fig-id=\"fig9\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel9-3227505-small.gif\" alt=\"FIGURE 9. - SVM confusion matrix for 3 tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 9. </b><fig><p>SVM confusion matrix for 3 tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>NB was one of the worst ML models that can be used with such a problem. This has been observed based on <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig. 10.a</a>, <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig. 10.b</a>, and <a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig. 10.c</a>, as most of the samples were predicted to be not correct for task 1, and the opposite side in tasks 2, and 3.</p><p>This can be explained by understanding the nature of NB, which depends on conditional probability between class and feature, taking into consideration that this model treats every feature as independent from other features which makes it harder for the model to learn by building relations between features and each other.\n<div class=\"figure figure-full\" id=\"fig10\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel10-3227505-large.gif\" data-fig-id=\"fig10\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel10-3227505-small.gif\" alt=\"FIGURE 10. - NB confusion matrix for 3 tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 10. </b><fig><p>NB confusion matrix for 3 tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>As KNN is based on the concept of majority votes of nearby samples, it has made a quite progress in its results that is observed in <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig. 11.a</a>, <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig. 11.b</a>, and <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig. 11.c</a>. Due to the similarity in the positive samples and diversity in the negative samples, most of the false predictions have been made on the False positive side (predicted correct but truly not correct).\n<div class=\"figure figure-full\" id=\"fig11\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel11-3227505-large.gif\" data-fig-id=\"fig11\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel11-3227505-small.gif\" alt=\"FIGURE 11. - SVM confusion matrix for 3 tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 11. </b><fig><p>SVM confusion matrix for 3 tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>According to <a ref-type=\"fig\" anchor=\"fig12\" class=\"fulltext-link\">Fig. 12.a</a>, <a ref-type=\"fig\" anchor=\"fig12\" class=\"fulltext-link\">Fig. 12.b</a>, and <a ref-type=\"fig\" anchor=\"fig12\" class=\"fulltext-link\">Fig. 12.c</a>, the decision tree has evaluated the tests based on the tree in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig. 7</a>. Due to the concept of DF, the model is tending to overfit in such a task, which is not preferable lab evaluation problem due to the variety in the features and their intervals.\n<div class=\"figure figure-full\" id=\"fig12\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel12-3227505-large.gif\" data-fig-id=\"fig12\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel12-3227505-small.gif\" alt=\"FIGURE 12. - DT confusion matrix for 3 tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 12. </b><fig><p>DT confusion matrix for 3 tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>Using DF in such a problem may cause a long-term misclassification as the model will be related to the data trained on more than the problem concept itself. After evaluating the RF model using the test samples as shown in <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig. 13.a</a>, <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig. 13.b</a>, and <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig. 13.c</a>, part of the DF disadvantage has been solved. As RF is based on the majority votes among various DTs, the model has achieved acceptable results disregarding the nature of the lab task itself. This can be observed in <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig. 13.b</a>, and <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig. 13.c</a>, as the result didn\u2019t change or degraded, while the evaluated task became more complicated.\n<div class=\"figure figure-full\" id=\"fig13\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel13-3227505-large.gif\" data-fig-id=\"fig13\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel13-3227505-small.gif\" alt=\"FIGURE 13. - RF confusion matrix for 3 tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 13. </b><fig><p>RF confusion matrix for 3 tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>As noticed in <a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig. 14.a</a>, <a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig. 14.b</a>, and <a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig. 14.c</a>, ANN has promising confusion matrices. It is also observed that as the task gets complicated the model takes more advantage of the task. This can be demonstrated and explained due to the complicated architecture of ANN as its layers are trained to fit with the pattern of the problem. Hence, when the data is more complicated, the model has a more complicated pattern to learn which leads to better model predictions.\n<div class=\"figure figure-full\" id=\"fig14\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel14-3227505-large.gif\" data-fig-id=\"fig14\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel14-3227505-small.gif\" alt=\"FIGURE 14. - ANN confusion matrix for 3 tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 14. </b><fig><p>ANN confusion matrix for 3 tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>The Precision can be represented using <a ref-type=\"disp-formula\" anchor=\"deqn20\" href=\"#deqn20\" class=\"fulltext-link\">(20)</a>, which is a classification model\u2019s capacity to isolate just the pertinent data elements. Precision in mathematics is the product of the number of true positives and the sum of the true positives and false positives.<disp-formula id=\"deqn20\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathrm {Precision}=\\frac {\\mathrm {TP}}{\\mathrm {TP+FP}}\\tag{20}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathrm {Precision}=\\frac {\\mathrm {TP}}{\\mathrm {TP+FP}}\\tag{20}\\end{equation*}\n</span></span></disp-formula></p><p>The Recall can be calculated using <a ref-type=\"disp-formula\" anchor=\"deqn21\" href=\"#deqn21\" class=\"fulltext-link\">(21)</a>, which is the capacity of a model to locate each and every pertinent example in a data collection. The number of true positives is divided by the sum of the true positives and false negatives.<disp-formula id=\"deqn21\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathrm {Recall}=\\frac {\\mathrm {TP}}{\\mathrm {TP+FN}}\\tag{21}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathrm {Recall}=\\frac {\\mathrm {TP}}{\\mathrm {TP+FN}}\\tag{21}\\end{equation*}\n</span></span></disp-formula></p><p>The F1-Score can be reached using precision and recall as in <a ref-type=\"disp-formula\" anchor=\"deqn22\" href=\"#deqn22\" class=\"fulltext-link\">(22)</a>, which is the average of Precision and Recall, weighted. Therefore, both false positives and false negatives are included while calculating this score. Although F1 is typically more valuable than accuracy, especially if you have an uneven class distribution, it is not intuitively as simple to grasp as accuracy.<disp-formula id=\"deqn22\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathrm {F1-Score =2\\cdot }\\frac {\\mathrm {Precision \\cdot Recall }}{\\mathrm {Precision+Recall}}\\tag{22}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathrm {F1-Score =2\\cdot }\\frac {\\mathrm {Precision \\cdot Recall }}{\\mathrm {Precision+Recall}}\\tag{22}\\end{equation*}\n</span></span></disp-formula></p><p>The accuracy can be calculated using <a ref-type=\"disp-formula\" anchor=\"deqn23\" href=\"#deqn23\" class=\"fulltext-link\">(23)</a>, which is the easiest to understand performance metric, as it simply measures the proportion of properly predicted observations to all observations.<disp-formula id=\"deqn23\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathrm {Accuracy =}\\frac {\\mathrm {TP+TN}}{\\mathrm {TP+TN+FP+FN}}\\mathrm { }\\tag{23}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathrm {Accuracy =}\\frac {\\mathrm {TP+TN}}{\\mathrm {TP+TN+FP+FN}}\\mathrm { }\\tag{23}\\end{equation*}\n</span></span></disp-formula></p><p>The models with the best performance based on accuracy score are bolded to be clear for observation.</p><p>Regarding ROC Curve, it is plotted using a False Positive Rate (FPR), which can be calculated using <a ref-type=\"disp-formula\" anchor=\"deqn24\" href=\"#deqn24\" class=\"fulltext-link\">(24)</a>, True Positive Rate (TPR) (which is already defined as Recall).<disp-formula id=\"deqn24\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathrm {FPR =}\\frac {\\mathrm {FP}}{\\mathrm {FP+TN}}\\tag{24}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathrm {FPR =}\\frac {\\mathrm {FP}}{\\mathrm {FP+TN}}\\tag{24}\\end{equation*}\n</span></span></disp-formula></p><p>As appears in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>, <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>, and <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a>, the best overall performance can be reached with the ANN. Also, it is noticed that the model can have a better performance as the task is getting more complicated. As all the models had better overall performance in task 3 than in task 2 or task 1.</p><p>Regarding RF and SVM, they had close performance to each other. This can be due to their nature in dealing with high-dimensional features, as RF deals with numerous numbers of decision trees to have majority votes, and SVM uses the kernel to make it easier for the model to have a hyperplane.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nModels performance metrics for task 1.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t3-3227505-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t3-3227505-small.gif\" alt=\"Table 3- &#10;Models performance metrics for task 1.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nModels performance metrics for task 2.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t4-3227505-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t4-3227505-small.gif\" alt=\"Table 4- &#10;Models performance metrics for task 2.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nModels performance metrics for task 3.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t5-3227505-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel.t5-3227505-small.gif\" alt=\"Table 5- &#10;Models performance metrics for task 3.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Regarding KNN, it has acceptable performance, although it is simple. It has also appeared that DT can\u2019t have a stable performance among all tasks, which makes it not reliable all the time. This may lead to difficulty in expecting its performance without having multiple analyzes in various similar tasks. Unfortunately, NB doesn\u2019t perform well in all tasks which leads to disqualifying it from any further research in the field of mouse dynamics.</p><p>More illustrated results can be understood in <a ref-type=\"fig\" anchor=\"fig15\" class=\"fulltext-link\">Fig. 15</a>, <a ref-type=\"fig\" anchor=\"fig16\" class=\"fulltext-link\">Fig. 16</a>, and <a ref-type=\"fig\" anchor=\"fig17\" class=\"fulltext-link\">Fig. 17</a> using ROC Curve. To be more clear, as the peak of the model graph tends to the top left, the performance is going better, which means it has a higher Area Under the Curve (AUC). In the ROC curve of task 1 (<a ref-type=\"fig\" anchor=\"fig15\" class=\"fulltext-link\">Fig. 15</a>), all models have near results except NB, which had poor performance. ANN and RF had close results in this task. In the ROC curve of task 2 (<a ref-type=\"fig\" anchor=\"fig16\" class=\"fulltext-link\">Fig. 16</a>), all models have close TPR but differ in FPR. This causes the upper hand for ANN in task 2.</p><p>Finally, in the ROC curve of task 3 (<a ref-type=\"fig\" anchor=\"fig17\" class=\"fulltext-link\">Fig. 17</a>), SVM and RF have the same performance, which comes after ANN. Despite most of the models having better TPR, the cause of ANN leading in the best performance in all tasks came from lowering the FPR.\n<div class=\"figure figure-full\" id=\"fig15\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel15-3227505-large.gif\" data-fig-id=\"fig15\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel15-3227505-small.gif\" alt=\"FIGURE 15. - ROC Curve of Models with Task 1.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 15. </b><fig><p>ROC Curve of Models with Task 1.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig16\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel16-3227505-large.gif\" data-fig-id=\"fig16\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel16-3227505-small.gif\" alt=\"FIGURE 16. - ROC Curve of Models with Task 2.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 16. </b><fig><p>ROC Curve of Models with Task 2.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig17\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel17-3227505-large.gif\" data-fig-id=\"fig17\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel17-3227505-small.gif\" alt=\"FIGURE 17. - ROC Curve of Models with Task 3.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 17. </b><fig><p>ROC Curve of Models with Task 3.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p><p>From previous results, it is noticed that the Noval ANN has the best performance with 79.47% in task 1, 87.88% in task 2, and 88.58% in task 3, respectively. Coming after it RF and SVM respectively with 77.89% - 73.19% in task 1, 85.8% - 84.85% in task 2, and 87.67%- 87.67% in task 3.</p><p>And it is noticed throughout the whole study that as the task is developed to be more complicated, the performance rises for all the models, and this can be demonstrated that having a complex task to evaluate, makes it easier for the model to get its pattern reasonably. It is preferable to disqualify NB in any research regarding mouse dynamics in the future. Also, through the study, it is noticed that balanced data is a need to reach acceptable results that can be reliable for the problem.</p></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Experiment Survey Study</h2></div><p>An experiment survey study has been conducted in Helwan University among 25 students to test the test version of the system and fill out a survey form regarding the reliability, applicability, and usability of the software. The questions of survey were as follows: \n<ol><li><p>In a scale of 1 to 10, rate the applicability of the project.</p></li><li><p>In a scale of 1 to 10, rate the usability of the project.</p></li><li><p>In a scale of 1 to 10, Do you think this software can help students who are struggling in practical lab tasks?</p></li><li><p>In a scale of 1 to 10, Do you think this software will be reliable to grade the practical lab tasks for students?</p></li><li><p>In a scale of 1 to 10, Do you think this software will evolve to assist the Staff in the learning process?</p></li><li><p>Choose the best 3 advantages of this system.</p><ol><li><p>Help students with learning in case of educational staff shortage</p></li><li><p>Contribute in the efficiency of the learning process in general</p></li><li><p>Improve the e-learning process</p></li><li><p>Provide a generalized system to be applied to any software simulation</p></li><li><p>The lack of hardware dependency due to models simplicity</p></li><li><p>Ease of Use</p></li><li><p>Other, Explain</p></li></ol></li></ol> The form results were analyzed by providing a bar chart for the first 6 questions as presented in <a ref-type=\"fig\" anchor=\"fig18\" class=\"fulltext-link\">Fig. 18</a>, <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig. 19</a>, <a ref-type=\"fig\" anchor=\"fig20\" class=\"fulltext-link\">Fig. 20</a>, <a ref-type=\"fig\" anchor=\"fig21\" class=\"fulltext-link\">Fig. 21</a>, and <a ref-type=\"fig\" anchor=\"fig22\" class=\"fulltext-link\">Fig. 22</a>, respectively to visualize the distribution of students\u2019 opinions on the software system.\n<div class=\"figure figure-full\" id=\"fig18\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel18-3227505-large.gif\" data-fig-id=\"fig18\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel18-3227505-small.gif\" alt=\"FIGURE 18. - Results of the rate of applicability of the project to students.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 18. </b><fig><p>Results of the rate of applicability of the project to students.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig19\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel19-3227505-large.gif\" data-fig-id=\"fig19\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel19-3227505-small.gif\" alt=\"FIGURE 19. - Results of the rate of usability of the project to students.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 19. </b><fig><p>Results of the rate of usability of the project to students.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig20\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel20-3227505-large.gif\" data-fig-id=\"fig20\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel20-3227505-small.gif\" alt=\"FIGURE 20. - Results of how this software help students struggling in the practical lab tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 20. </b><fig><p>Results of how this software help students struggling in the practical lab tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig21\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel21-3227505-large.gif\" data-fig-id=\"fig21\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel21-3227505-small.gif\" alt=\"FIGURE 21. - Results of how this software reliable to grade practical lab tasks.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 21. </b><fig><p>Results of how this software reliable to grade practical lab tasks.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig22\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel22-3227505-large.gif\" data-fig-id=\"fig22\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel22-3227505-small.gif\" alt=\"FIGURE 22. - Results of how this software help assist staff in the learning process.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 22. </b><fig><p>Results of how this software help assist staff in the learning process.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig23\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel23-3227505-large.gif\" data-fig-id=\"fig23\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973300/abdel23-3227505-small.gif\" alt=\"FIGURE 23. - Results of the best 3 advantages of the system.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 23. </b><fig><p>Results of the best 3 advantages of the system.</p></fig></div><p class=\"links\"><a href=\"/document/9973300/all-figures\" class=\"all\">Show All</a></p></div></p></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VI.</div><h2>Conclusion</h2></div><p>Automated performance evaluation for the students\u2019 behavior while performing experiments online remotely is a challenging task that has been discussed for years. As it is hard to be applied in all software applications, varies from one lab type to another, and depending on the evaluation method, it may need high computational power. In this research, a method for automated performance evaluation for the experiments is proposed to provide an assistant for the students while performing the experiments. Furthermore, to evaluate the experimental steps performed by students to provide auto-grading for different types of questions solved by students in practical exams. As a consequence, the new performance evaluation module performs both the lab evaluation and assistance automatically, starting from the phase of fetching data, then feature extraction using statistical approaches. After that, features preprocessing to avoid overfitting and dropping unneeded features, then reaching the stage of implementing various ML models to use for evaluation and compare them trying to declare the best model for the problem. As the evaluation has high dimensionality, 6 models were used in training and predicting the needs of students for performance evaluation and a number of metrics are collected for three different tasks. It was found that the Artificial Neural Network (ANN) is the best AI algorithm that can be used in the performance evaluation module, with an average approximated precision value of 87% for three tasks. Furthermore, it achieved the highest average approximated accuracy and F1-score of 85% and 85%, respectively for the three tasks. Followed the Support Vector Machine (SVM), with an average approximated value of precision, accuracy, and F1-score equals 81%, 81%, and 82%, respectively for 3 tasks.</p><p>The new virtual service method is integrated within a completely new LLMS. This system provides the students and teachers with the flexibility of running any software within experiments remotely using a single interface. Also, the new LLMS provides an automatic performance evaluation system that studies the students\u2019 mouse behavior while performing experiments or exams remotely. This new performance evaluation system evaluates if the students performed the lab. steps correctly or not to help the auto-grading module to provide the correct grade to help the teacher in evaluating all types of experiments or exams. Furthermore, the new performance evaluation system triggers the Virtual Assistant Service that is used to provide the students with needed help and run one of the Session Collaborator\u2019s features through the Experimental Runtime Engine Module of the LLMS system. Furthermore, the new LLMS system provided the student with the ability to control the laboratory\u2019s equipment physically exists in the laboratory remotely.</p></div>\n</div></div></response>\n"
}