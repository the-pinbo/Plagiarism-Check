{
    "abstract": "Assessing the healthiness of food items in images has gained attention in both the computer vision and the nutrition fields. However, such task is generally a difficult one as food images are captured in various settings and thus are usually non-homogeneous. Moreover, assessing how healthy a food item is requires nutritional expertise and knowledge of the constituents of the food item and how it i...",
    "articleNumber": "9975313",
    "articleTitle": "DeepNOVA: A Deep Learning NOVA Classifier for Food Images",
    "authors": [
        {
            "preferredName": "Shady Elbassuoni",
            "normalizedName": "S. Elbassuoni",
            "firstName": "Shady",
            "lastName": "Elbassuoni",
            "searchablePreferredName": "Shady Elbassuoni"
        },
        {
            "preferredName": "Hala Ghattas",
            "normalizedName": "H. Ghattas",
            "firstName": "Hala",
            "lastName": "Ghattas",
            "searchablePreferredName": "Hala Ghattas"
        },
        {
            "preferredName": "Jalila El Ati",
            "normalizedName": "J. E. Ati",
            "firstName": "Jalila El",
            "lastName": "Ati",
            "searchablePreferredName": "Jalila El Ati"
        },
        {
            "preferredName": "Zoulfikar Shmayssani",
            "normalizedName": "Z. Shmayssani",
            "firstName": "Zoulfikar",
            "lastName": "Shmayssani",
            "searchablePreferredName": "Zoulfikar Shmayssani"
        },
        {
            "preferredName": "Sarah Katerji",
            "normalizedName": "S. Katerji",
            "firstName": "Sarah",
            "lastName": "Katerji",
            "searchablePreferredName": "Sarah Katerji"
        },
        {
            "preferredName": "Yorgo Zoughbi",
            "normalizedName": "Y. Zoughbi",
            "firstName": "Yorgo",
            "lastName": "Zoughbi",
            "searchablePreferredName": "Yorgo Zoughbi"
        },
        {
            "preferredName": "Aline Semaan",
            "normalizedName": "A. Semaan",
            "firstName": "Aline",
            "lastName": "Semaan",
            "searchablePreferredName": "Aline Semaan"
        },
        {
            "preferredName": "Christelle Akl",
            "normalizedName": "C. Akl",
            "firstName": "Christelle",
            "lastName": "Akl",
            "searchablePreferredName": "Christelle Akl"
        },
        {
            "preferredName": "Houda Ben Gharbia",
            "normalizedName": "H. B. Gharbia",
            "firstName": "Houda Ben",
            "lastName": "Gharbia",
            "searchablePreferredName": "Houda Ben Gharbia"
        },
        {
            "preferredName": "Sonia Sassi",
            "normalizedName": "S. Sassi",
            "firstName": "Sonia",
            "lastName": "Sassi",
            "searchablePreferredName": "Sonia Sassi"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227769",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9975313/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>With the proliferation of ubiquitous devices such as smart phones and wearable cameras, documenting dietary intake through images has become a common practice. Automatically assessing the healthiness of food items in images is a challenging computer vision task.<a ref-type=\"fn\" anchor=\"fn1\" class=\"footnote-link\">1</a> Food images can generally be non-homogenous as they can be taken in various settings and with different resolutions and qualities. They might also contain multiple food items in addition to other non-food related ones. Finally, assessing the healthiness of each food item in an image requires a knowledge of the constituents of the food items and how they are processed, which can be only accurately done by trained nutritionists who are familiar with the food items captured in the images.</p><p>To address all of the above mentioned challenges, we propose an end-to-end deep learning approach that can assess the healthiness of multiple food items in images without making any assumptions on how or where the images were taken. Our approach consists of two models: 1) a food item detection model that detects and localizes food items in an image that contains multiple food items, and 2) a classification model that classifies a detected food item into one or more of the four NOVA food groups, namely Unprocessed or Minimally Processed Food (Group 1); Processed Culinary Ingredients (Group 2); Processed Food (Group 3), and Ultra-processed Food (Group 4) <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. Ultra-processed food is usually considered unhealthy as it is associated with increased risk for obesity and other chronic diseases <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>. Using these two models, our approach can thus assess the healthiness of various food items in any given image. Our proposed approach is depicted in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>. It takes as input an image containing one or more food items and passes it to the food detection model, which detects and localizes each food item in the images using bounding boxes. The localized food items are then extracted using their bounding boxes and each food item image is then passed to the NOVA classifier, which classifies the food item into one or more of the four NOVA groups based on the processing level it went through. In <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>, the predicted groups for each detected food item are indicated under the food item, where 1 indicates the first NOVA group, 2 the second one and so on.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas1-3227769-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas1-3227769-small.gif\" alt=\"FIGURE 1. - Overview of approach.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Overview of approach.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p><p>Our food item detection model is based on a customized YOLOv3 (You Only Look Once, Version 3) model for general object detection <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>. To train our model, we used two public datasets, which are the EgocentricFood dataset <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a> and the UECFood-256 dataset <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>, and a custom dataset we created ourselves, which we refer to as the NOVA dataset. The first dataset, the EgocentricFood dataset, consists of food images captured using wearable cameras, and which contain food items of various types (general food, glass, cup, jar, can, mug, bottle, dish, and basket). The food items in the images in this dataset are localized using bounding boxes. The second dataset we used to train the food item detection model, the UECFood-256 dataset, consists of images of Asian food (e.g., Miso soup, Ramen Noodles and Fired Rice) that were crawled from the Web. The food items in each image in this dataset are also localized using bounding boxes. The third dataset we used to train our food item detection model, the NOVA dataset, consists of images of Tunisian food captured through wearable cameras. The images were then annotated on the crowdsourcing platform Labelbox<a ref-type=\"fn\" anchor=\"fn2\" class=\"footnote-link\">2</a> to detect food items in the images, again using bounding boxes. The reason we used these various datasets to train our food item detection model is to add more variability in the training data of the model to obtain a general model that can detect and localize food items in any food image no matter how or where it was captured. Our food item detection model achieved a mean Average Precision (mAP) of 0.90 on test data from the NOVA dataset.</p><p>Once food items have been detected and localized in a given food image, our approach extracts each food item using its bounding box and passes it to the NOVA classification model to estimate its healthiness. There have been many attempts to estimate the healthiness of food items using their corresponding images. However, most of these are quantitative approaches that are based on volume and calories estimation, which face many limitations. For instance, many of these approaches make unrealistic assumptions about the food images such as assuming the food images are all captured from specific predefined angles, or assuming the presence of reference objects in each image that can be used to estimate the volume of the food items. To this end, many expert nutritionists are advocating for food classification systems that are based on the food processing level rather than using calories and volume to assess the healthiness of food items <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>. Our proposed approach in this manuscript follows this school of thought by doing a qualitative assessment of the healthiness of food items rather than a quantitative one based on calories estimation.</p><p>Our NOVA classification model classifies food items into four groups according to the nature, extent and aim of the industrial processes that were applied to the food items. The first group is the Unprocessed or Minimally Processed Food, which includes natural food items such as vegetables, fruits, eggs, milk, water, etc. The second group is the Processed Culinary Ingredients, which is usually acquired from the first group and includes butter, oil, honey, etc. The third group is the Processed Food, which includes products made by adding salt, sugar or other Group 2 substances to Group 1 food such as unpackaged bread, canned fish, canned vegetables, etc. Finally, the fourth group is the Ultra-Processed Food, which includes food items that are produced using a series of industrial processes such as chips, chocolate, soft drinks, hotdog, etc. Obviously, a single food item might contain constituents that belong to more than one of these four groups and thus our NOVA classification model is a <i>multi-label</i> classification model.</p><p>Our NOVA classification model is based on the MobileNetV2 deep learning architecture <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>. To train our model, we used the NOVA dataset that we also used as part of the training data for the food item detection model. The dataset consists of image of Tunisian food that were captured using wearable cameras and that were annotated using crowdsourcing. In addition to localizing food items in the images using bounding boxes, the food items were also labeled with one or more of the NOVA groups depending on the level of processing the food items underwent. Since such task requires knowledge about the ingredients of food items and how they are processed, the NOVA dataset was fully annotated by expert Tunisian nutritionists. Our NOVA classification model described in this manuscript can thus assess the healthiness of Tunisian food items based on their images. However, our approach itself is general enough that it can be used to assess the healthiness of any food items, provided that accurately-labeled training data is obtained. Such data can be obtained using crowdsourcing as we did in the case of Tunisian food, as long as the annotators have sufficient knowledge about the food items in the images. Our NOVA classification model achieved an average F1-score of 0.86 on test data from the NOVA dataset.</p><p>Our main contributions in the manuscript can thus be summarized as follows:\n<ol><li><p>we build a general deep-learning-based food item detection model that can be used to detect and localize food items in any food image,</p></li><li><p>we build a deep-learning-based multi-label classification model that can be used to classify a food item based on its image into one or more of the NOVA food groups, and</p></li><li><p>we provide a prototype to acquire training data for these two models using crowdsourcing.</p></li></ol></p><p>The manuscript is organized as follows. In <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>, we give an overview of related work that addresses the problem of assessing the healthiness of food items using their images. In <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>, we describe our proposed deep learning approach to assess the healthiness of food items based on their images. <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a> describes the experiments we conducted to evaluate our proposed approach, their results and the error analysis of the proposed approach. Finally, we conclude and provide future directions in <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a>.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Related Work</h2></div><p>There is a wealth of work on food analysis from food images. These works can be broadly categorized into works that focus on food item detection, works that focus on food healthiness assessment, or both.</p><div class=\"section_2\" id=\"sec2a\"><h3>A. Food Item Detection</h3><p>Most of the works that aim to analyze food based on their images require algorithms and models for food item detection, recognition, and segmentation. For example, Akhi et al. <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2a\">[8]</a> proposed a Convolutional Neural Network (CNN) model based on the ResNet-5 pre-trained model <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2a\">[9]</a>, which was used to extract features from fast-food images. The extracted features were then used to train a multi-class Support Vector Machines (SVM) classifier that classifies food images into 10 classes. Similalry, Liu et al. <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\">[10]</a> proposed a deep learning approach based on CNNs that classifies food images that are captured in the real world. Aguilar et al. <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2a\">[11]</a> developed a framework that addresses the problem of automatic food-tray analysis in restaurants. Their framework is based on CNNs, and is composed of food localization, recognition, and segmentation models. The first part of their framework is a food segmentation model that is based on a Fully Convolutional Network (FCN), and it aims to separate food items from the background (i.e, the tray). The second part of the framework then detects food items by using the YOLOv2 model <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2a\">[12]</a>.</p><p>Bolanos et al. <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_2a\">[4]</a> proposed another approach for generic simultaneous food localization and recognition. First, they trained the GoogleNet CNN model <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2a\">[13]</a> to distinguish between food and non-food images. Second, they enhanced the previous model by adding Global Average Pooling (GAP) layer that aims to generate heat maps of food probabilities. Finally, bounding boxes were generated for the regions with a probability above a certain threshold. After detecting the food items, they fine-tuned the GoogleNet model to classify the items into various types. In addition to that, they built the EgocentricFood dataset, which contains food images that were captured using wearable cameras.</p><p>Unlike most of the approaches described above, <i>our food item detection model</i> proposed as part of the approach described in this manuscript does not make any assumptions about how the food images were captured or what they contain. It is thus able to detect food items of different shapes, sizes, and types in images that are taken in real settings, and with various resolutions and qualities. Our model is based on a customized version of YOLOv3 <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_2a\">[3]</a> that is able to detect food items on three different image scales with very high accuracy as indicated by our experiments.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Food Healthiness Assessment</h3><p>Assessing the healthiness of food items present in an image is a challenging computer vision task. Most works that address such problem rely on estimating the amount of calories in the food items to assess their healthiness. For instance, Liang et al. <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2b\">[14]</a> proposed a calorie estimation approach that takes two images as an input: a top and a side view of a food item that include on its side a coin, which is used as a calibration object. They used a Faster r-CNN model <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2b\">[15]</a> to detect food items using bounding boxes. They then applied image segmentation on the detected food items for background removal using the GrabCut algorithm <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2b\">[16]</a>. The segmented images are then used to estimate the volume and mass of the detected food items, which are in turn used to estimate the amount of calories in each food item.</p><p>Similarly, Myers et al. <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2b\">[17]</a> developed the Im2Calories system that estimates the amount of calories in food dishes. They started by training a GoogLeNet model on the Food101 multi-labeled dataset <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2b\">[18]</a>. They then used the DeepLab system <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2b\">[19]</a> for semantic image segmentation to localize food items and segment them. Using the voxel representation and the segmentation mask of food items, they estimated the volume of each food item, and consequently predicted the amount of calories using the calorific density of each type of food. The authors, however, faced the problem of lack of sufficient calorie-annotated training data and thus could not do extensive evaluation of their approach because the texture properties and the color of the images in the Food101 dataset are different from the ones of real food images.</p><p>Another related work is the one by Lu et al. <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>, where the authors proposed an AI system that is able to estimate the nutrient intake of hospital patients. They built a dataset consisting of 660 images by setting up a table that contains a camera on the top with a specific distance from the food items. In addition to that, they created a database that contains the recipes and the nutrient intake of the consumed meals. They used a Multi-Task Fully Convolutional Network (MTFCN) model <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2b\">[21]</a> for image segmentation that aims to estimate the volumes of food items, which in turn helped in estimating the nutrient intakes based on the created database.</p><p>Gao et al. <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2b\">[22]</a> proposed MUSEFood, which is a food-volume estimation approach that is different from all of the previous volume-estimation approaches. Their proposed approach does not require any training using food images with their corresponding volume information, and in addition eliminates the need to place a reference object of known size when capturing the images. Instead, they used microphones and speakers to calculate the vertical distance from the camera to the food items, which helped in estimating the actual volume of the food items and in turn estimating the amount of calories in the food items.</p><p>Chokr and Elbassuoni <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2b\">[23]</a> also proposed an approach for calories estimation from food images. Their approach uses a machine learning model to predict the type of a food item in an image based on the image visual features. Their approach also predicts the size of the food item (in grams) and then based on these two predicted values as well as the original features of the image, it estimates the amount of calories in the food item. However, the authors only trained and tested their model on images that contain a single food item that belongs to only one of six different categories (burger, chicken, doughnut, pizza, salad and sandwich).</p><p>Overall, using volume and calories estimation approaches for assessing the healthiness of food items has many limitations including 1) the fact that the images of the food items should be captured from specific angles, 2) the need for reference objects, which are used in volume estimation, to be present in the food images, 3) training these models typically requires a large number of annotated images for each food type, and 4) there should be a specific predefined database that contains the nutrient information of the food items that exist in the images.</p><p>Sudo et al. <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2b\">[24]</a> proposed a different healthiness assessment approach that is based on a feature extraction deep learning model that is followed by a ranking algorithm. First, they built a dataset of 850 images of meals that were taken from a top view. These images were ranked by registered nutritionists based on the healthiness of the whole meal from best to worst. Second, they built a feature extraction model that uses a CNN followed by a pyramid scene parsing network (PSPNET) <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2b\">[25]</a>, which outputs pixel-based feature maps. The extracted features were then used as an input to the ranking algorithm that uses another CNN. However, the authors reported that the correlation coefficient between the rankings of the nutritionists and the ground truth rank that is based on the nutritional facts of the meals was relatively low. The authors explained that their approach did not perform well because assessing the healthiness of food items by ranking them from best to worst without a specific criteria is not highly correlated with the ground truth healthiness of the food items.</p><p><i>Our healthiness assessment approach</i> we propose in this manuscript addresses all the limitations of the above described approaches by utilizing a qualitative approach rather than a quantitative one. Instead of estimating the amount of calories in food items depicted in images, it classifies the food items into one or more of the four NOVA food groups based on their processing level. Our NOVA classification model can thus be used to accurately assess the healthiness of multiple food items in generic images without making any unrealistic assumptions about how the food images were taken. Moreover, our model does not require extensive annotation efforts as is the case with most of the above surveyed approaches.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Approach</h2></div><p>In this section, we describe our end-to-end deep learning approach that can assess the healthiness of multiple food items in images. Our approach consists of two models: 1) a food item detection model that detects and localizes food items in an image, and 2) a classification model that classifies a detected food item into one or more of the four NOVA food groups. We describe each model separately next.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Food Item Detection Model</h3><p>Our food item detection model is a customized object detection model that localizes food items in an image using bounding boxes. The food items can be of any type, shape, and size. Moreover, they can be in a dish, bowl, cup, or held by a person, etc. We first describe the data we used to train such a model, then we describe the architecture of the model itself afterwards.</p><div class=\"section_2\" id=\"sec3a1\"><h4>1) Datasets</h4><p>Training our food item detection model requires a dataset consisting of images that contain food items localized using bounding boxes. For example, <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a> shows a sample training example consisting of an image with multiple food items and where the food items are localized using bounding boxes. We used three different datasets to train our food item detection model. The first two are the EgocentricFood dataset <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_3a1\">[4]</a> and the UECFood-256 dataset <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_3a1\">[5]</a>, which are both publicly available datasets, and the third is a custom dataset that we created ourselves and that contains food images taken using wearable cameras and in which food items were localized via crowdsourcing. The reason we used three different datasets is to ensure 1) we use a sufficiently large amount of data to train a deep-learning model, which usually require large amounts of data, and 2) we have sufficient variability in the training data so that our model is able to detect and localize all food items in any image regardless of their shape, type or size and regardless of how or where the image was taken.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas2-3227769-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas2-3227769-small.gif\" alt=\"FIGURE 2. - Training example for the food item detection model.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Training example for the food item detection model.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p><p>Our first dataset, the EgocentricFood dataset, includes 5,038 images that were taken by wearable cameras. The images contain 7,294 different food items that are localized using bounding boxes. The dataset contains nine categories of food items, which are glasses, cups, jars, cans, mugs, bottles, dishes, baskets and others (food items that do not belong to any of the other categories). The distribution of food items across these categories is shown in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a> shows a sample of images from this dataset.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nEgocentricFood Dataset: Distribution of Food Items Across Categories</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t1-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t1-3227769-small.gif\" alt=\"Table 1- &#10;EgocentricFood Dataset: Distribution of Food Items Across Categories\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas3-3227769-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas3-3227769-small.gif\" alt=\"FIGURE 3. - Sample images from the EgocentricFood dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Sample images from the EgocentricFood dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p><p>Our second dataset, the UECFOOD-256 dataset, is an Asian food dataset that consists of food images crawled from the Web. Similar to the first dataset, the food items in this dataset are also localized in the images using bounding boxes. The dataset consists of a total of 28,898 images that contain 31,395 food items belonging to 256 different categories. The dataset contains at least 100 images for each category. <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> shows the 10 categories with the highest number of items in the dataset. <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a> shows sample images from the UECFOOD-256 dataset. Since the dataset consists of images crawled from the Web, some of these images are not taken in real-life settings such as commercial images of food items.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nUECFood-256 Dataset: Top-10 Categories With the Highest Number of Items</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t2-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t2-3227769-small.gif\" alt=\"Table 2- &#10;UECFood-256 Dataset: Top-10 Categories With the Highest Number of Items\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas4-3227769-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas4-3227769-small.gif\" alt=\"FIGURE 4. - Sample images from the UECFood-256 dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Sample images from the UECFood-256 dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p><p>The third and final dataset we used to train our food item detection model, which we refer to as the NOVA dataset, consists of 1,800 food images. The images contain various Tunisian food items as they were taken by school children in Tunisia using wearable cameras. <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a> shows sample images from the NOVA dataset. To localize the food items in the images, we created a custom interface on the crowdsourcing platform Labelbox<a ref-type=\"fn\" anchor=\"fn3\" class=\"footnote-link\">3</a> that allows annotators to localize each food item in a given image by drawing a bounding box around it. Each image was annotated by two different trained nutritionists. After all the images in the NOVA dataset were annotated on Labelbox, we ended up with 4,201 localized food items in the 1,800 images.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas5-3227769-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas5-3227769-small.gif\" alt=\"FIGURE 5. - Sample images from the NOVA dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Sample images from the NOVA dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3a2\"><h4>2) Model</h4><p>Our generic food item detection model is based on YOLOv3 model that was developed by Redmon and Farhadi <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_3a2\">[3]</a>. YOLOv3 is a one stage real-time object detection model that localizes general objects in images and videos using bounding boxes. This model has been shown to outperform other object detection models in terms of both detection quality and time <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_3a2\">[3]</a>.</p><p>Similar to YOLOv3, our food detection model is made up of two main components, which are a feature extractor and a feature detector. The feature extractor is a CNN referred to in YOLOv3 as Darknet-53. It is made up of 53 layers (hence the name Darknet-53) with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$3\\times 3$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1\\times 1$\n</tex-math></inline-formula> convolutional layers followed by residual connections <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_3a2\">[9]</a>. 53 additional layers are also added to the Darknet-53 network that serve as a detection head, resulting in a total of 106 convolutional layers. The detection head of the model performs object detection on three different image scales by applying <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1\\times 1$\n</tex-math></inline-formula> detection kernels on their corresponding feature maps <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_3a2\">[3]</a>. The three scales of each image are determined by the stride parameters in the CNN, which are responsible for down-sampling the images by factors of 32, 16, and 8, respectively. Since all the images in our three training datasets have a resolution of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$416\\times 416$\n</tex-math></inline-formula>, we ended up with three different resolutions for each image: <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$52\\times 52$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$26\\times 26$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$13\\times 13$\n</tex-math></inline-formula>. This technique of performing object detection on three different scales of each image helps in improving the accuracy of detecting food items of different sizes as we show in our experiments in <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a>.</p><p>After getting the feature maps, the input image is divided into <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$S\\times S$\n</tex-math></inline-formula> grid according to the extracted feature map size. For example, a <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$416\\times 416$\n</tex-math></inline-formula> image with a <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$26\\times 26$\n</tex-math></inline-formula> feature map will result in an image divided into <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$26\\times 26$\n</tex-math></inline-formula> cells. Each of the cells predicts three bounding boxes, objectness scores (i.e., the probability that there is an object in a bounding box), and the classes the detected objects belong to. The model outputs a bounding box coordinate <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(t_{x},t_{y},t_{w},t_{h})$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(t_{x},t_{y})$\n</tex-math></inline-formula> is the center of the bounding box and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(t_{w},t_{h})$\n</tex-math></inline-formula> is the width and height of the box. The bounding boxes are calculated with the help of the anchor boxes, which are predefined bounding boxes that are used to predict the bounding boxes coordinates by predicting the offsets to the anchor boxes. <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a> shows the predicted bounding box coordinates in green and the anchor box in red.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas6-3227769-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas6-3227769-small.gif\" alt=\"FIGURE 6. - Bounding box coordinates prediction.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Bounding box coordinates prediction.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p><p>The anchor boxes are calculated using the k-means clustering algorithm <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_3a2\">[26]</a>, which starts by choosing <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> random points as initial clustering centroids. It then calculates the distance from each point to each of the centroids, and finally assigns each point to its nearest centroid. The algorithm then proceeds to update the centroids until the algorithm converges (i.e., the centroids do not change anymore). In our model, the input to the clustering algorithm is the widths and heights of the bounding boxes and we set <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k=9$\n</tex-math></inline-formula> since we need three anchor boxes per each of the three image scales. To calculate the distance from a centroid to a bounding box, we subtract 1 from the Intersection over Union (IoU) of the box and the centroid as shown in <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Equation 1</a>:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} distance(Box,Centroid)=1-IoU(Box,Centroid) \\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} distance(Box,Centroid)=1-IoU(Box,Centroid) \\tag{1}\\end{equation*}\n</span></span></disp-formula> where IoU is a measure that calculates the similarity between two bounding boxes using Jaccard index by dividing the intersection of the shapes by their union as shown in <a ref-type=\"disp-formula\" anchor=\"deqn2\" href=\"#deqn2\" class=\"fulltext-link\">Eq. 2</a>:<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} IoU=\\frac {|A \\cap B|}{|A \\cup B|} \\tag{2}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} IoU=\\frac {|A \\cap B|}{|A \\cup B|} \\tag{2}\\end{equation*}\n</span></span></disp-formula></p><p>To calculate the bounding boxes coordinates, the model first transforms the output of the CNN <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(t_{x},t_{y},t_{h},t_{w})$\n</tex-math></inline-formula> to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(b_{x},b_{y},b_{w},b_{h})$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(b_{x},b_{y})$\n</tex-math></inline-formula> is calculated by applying sigmoid function (<a ref-type=\"disp-formula\" anchor=\"deqn3-deqn4\" href=\"#deqn3-deqn4\" class=\"fulltext-link\">Eq. 3</a> and <a ref-type=\"disp-formula\" anchor=\"deqn3-deqn4\" href=\"#deqn3-deqn4\" class=\"fulltext-link\">Eq. 4</a>) on the predicted <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(t_{x},t_{y})$\n</tex-math></inline-formula> and adding <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(c_{x},c_{y})$\n</tex-math></inline-formula>, which is the top-left offset of our grid from the current cell of the feature map:<disp-formula id=\"deqn3-deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} b_{x}=&amp;\\sigma \\left ({t_{x}}\\right)+c_{x} \\tag{3}\\\\ b_{y}=&amp;\\sigma \\left ({t_{y}}\\right)+c_{y} \\tag{4}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} b_{x}=&amp;\\sigma \\left ({t_{x}}\\right)+c_{x} \\tag{3}\\\\ b_{y}=&amp;\\sigma \\left ({t_{y}}\\right)+c_{y} \\tag{4}\\end{align*}\n</span></span></disp-formula> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(b_{w},b_{h})$\n</tex-math></inline-formula> is the width and height of the predicted bounding box, which are calculated using <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(p_{w},p_{h})$\n</tex-math></inline-formula>, which in turn is the anchor box\u2019s coordinates as can be seen in <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn6\" href=\"#deqn5-deqn6\" class=\"fulltext-link\">Eq. 5</a> and <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn6\" href=\"#deqn5-deqn6\" class=\"fulltext-link\">Eq. 6</a>:<disp-formula id=\"deqn5-deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} b_{w}=&amp;p_{w} e^{t_{w}} \\tag{5}\\\\ b_{h}=&amp;p_{h} e^{t_{h}} \\tag{6}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} b_{w}=&amp;p_{w} e^{t_{w}} \\tag{5}\\\\ b_{h}=&amp;p_{h} e^{t_{h}} \\tag{6}\\end{align*}\n</span></span></disp-formula></p><p>In addition to the bounding box coordinates, the model outputs an objectness score, which is calculated using logistic regression (<a ref-type=\"disp-formula\" anchor=\"deqn7\" href=\"#deqn7\" class=\"fulltext-link\">Eq. 7</a>) and indicates the probability that there is an object inside a certain bounding box:<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\sigma (x)=\\frac {1}{\\left ({1+e^{-x}}\\right)} \\tag{7}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\sigma (x)=\\frac {1}{\\left ({1+e^{-x}}\\right)} \\tag{7}\\end{equation*}\n</span></span></disp-formula></p><p>Moreover, the model also predicts classes for the detected objects using a sigmoid function (i.e., multi-label classification where the model can predict more than one class per bounding box). Note that in our case, all objects are assumed to belong to one class, namely food, as compared to the general YOLOv3 model, which is typically used to detect objects that belong to multiple classes (e.g., car, pedestrian, truck, tree, etc.). That is, in our case, the objectness score represents the probability that there is a food item inside a bounding box, and the classification is a binary one (i.e., a detected object is either a food item or not).</p><p>The YOLOv3 model calculates the bounding box error using Mean Squared Error (MSE) of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t-\\hat {t}$\n</tex-math></inline-formula> <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_3a2\">[3]</a>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> is the ground truth coordinates, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {t}$\n</tex-math></inline-formula> is the predicted ones. In our model, we use the Generalized IoU (GIoU) proposed by Rezatofighi et al. <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_3a2\">[27]</a> as a loss function. GIoU is an extension of IoU that addresses its limitations as pointed out in <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_3a2\">[27]</a>. First, If <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${|A \\cap B|}=0$\n</tex-math></inline-formula>, then <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$IoU=0$\n</tex-math></inline-formula> and therefore IoU will not reflect if the bounding boxes are near or far from each other. Second, IoU does not actually reflect the overlap between the bounding boxes. The GIoU metric was proposed to solve these problems and the evaluation in <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_3a2\">[27]</a> shows that using the GIoU loss improves the performance of many object detection models, including YOLOv3, on popular object detection benchmarks such as the COCO dataset <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_3a2\">[28]</a>. GIoU is calculated using <a ref-type=\"disp-formula\" anchor=\"deqn8\" href=\"#deqn8\" class=\"fulltext-link\">Eq. 8</a>:<disp-formula id=\"deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} GIoU(A,B)=IoU(A,B)-\\frac {|C \\backslash (A \\cup B)|}{|C|} \\tag{8}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} GIoU(A,B)=IoU(A,B)-\\frac {|C \\backslash (A \\cup B)|}{|C|} \\tag{8}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> is the smallest box enclosing <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$|C \\backslash (A \\cup B)|$\n</tex-math></inline-formula> calculates the area occupied by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> without <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$A$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>.</p><p>The values of IoU are in the range [0, 1], whereas the values of GIoU are in the range <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$[-1,1]$\n</tex-math></inline-formula>, where 1 is the maximum value when two bounding boxes overlap and \u22121 is the minimum value when the bounding boxes are not overlapping. GIoU loss is calculated by subtracting 1 from the value of GIoU as shown in <a ref-type=\"disp-formula\" anchor=\"deqn9\" href=\"#deqn9\" class=\"fulltext-link\">Eq. 9</a>:<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathcal {L}_{GIoU}=1-GIoU \\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathcal {L}_{GIoU}=1-GIoU \\tag{9}\\end{equation*}\n</span></span></disp-formula></p><p>Unlike the standard YOLOv3, which uses BCE loss for objectness scores and class prediction, we use BCE with Logits Loss (BCEWithLogitsLoss) as shown in <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">Eq. 10</a>:<disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\text {BCEWithLogitsLoss }=&amp;-\\frac {1}{n} \\times \\sum _{i}\\left ({y_{i} \\times \\log \\left ({\\sigma \\left ({\\hat {y}_{i}}\\right)}\\right)}\\right. \\\\&amp;+\\left.{\\left ({1-y_{i}}\\right) \\times \\log \\left ({1-\\sigma \\left ({\\hat {y}_{i}}\\right)}\\right)}\\right) \\\\{}\\tag{10}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\text {BCEWithLogitsLoss }=&amp;-\\frac {1}{n} \\times \\sum _{i}\\left ({y_{i} \\times \\log \\left ({\\sigma \\left ({\\hat {y}_{i}}\\right)}\\right)}\\right. \\\\&amp;+\\left.{\\left ({1-y_{i}}\\right) \\times \\log \\left ({1-\\sigma \\left ({\\hat {y}_{i}}\\right)}\\right)}\\right) \\\\{}\\tag{10}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y$\n</tex-math></inline-formula> is the true label of the image, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {y}$\n</tex-math></inline-formula> is the predicted probability, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> is the sigmoid function that maps the values between 0 and 1. This is a more stable version of BCE loss, which takes too long to converge compared to BCEWithLogitsLoss that uses sigmoid before applying the BCE loss, resulting in more stable results <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3a2\">[29]</a>.</p><p>Our final loss function that we need to minimize while training the model is thus a sum of the object (i.e., food item) localization loss, the classification loss (whether a localized object is a food item or not) and the objectness loss (the probability of a food item being present in a bounding box) as shown in <a ref-type=\"disp-formula\" anchor=\"deqn11\" href=\"#deqn11\" class=\"fulltext-link\">Eq. 11</a>:<disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathcal {L}_{model}= \\mathcal {L}_{Localization}+\\mathcal {L}_{Classification}+\\mathcal {L}_{Objectness} \\tag{11}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathcal {L}_{model}= \\mathcal {L}_{Localization}+\\mathcal {L}_{Classification}+\\mathcal {L}_{Objectness} \\tag{11}\\end{equation*}\n</span></span></disp-formula> where:<disp-formula id=\"deqn12-deqn14\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {L}_{Localization}=&amp;\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} \\mathcal {L}_{GIoU}(b^{j}_{i},\\hat {b}^{j}_{i})\\tag{12}\\\\ {L}_{Objectness}=&amp;\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} {\\mathsf {1}}^{obj}_{i,j} \\big [c^{j}_{i} \\times \\log \\left ({\\sigma \\left ({\\hat {c}^{j}_{i}}\\right)}\\right) \\\\&amp;-\\left ({1-c^{j}_{i}}\\right) \\times \\log \\left ({1-\\sigma \\left ({\\hat {c}^{j}_{i} }\\right)}\\right) \\big] \\\\&amp;+\\lambda _{noobj}\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} {\\mathsf {1}}^{noobj}_{i,j} \\big [c^{j}_{i} \\times \\log \\left ({\\sigma \\left ({\\hat {c}^{j}_{i}}\\right)}\\right) \\\\&amp;-\\left ({1-c^{j}_{i}}\\right) \\times \\log \\left ({1-\\sigma \\left ({\\hat {c^{j}_{i}} }\\right)}\\right) \\big] \\tag{13}\\\\ {L}_{Classification}=&amp;\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} {\\mathsf {1}}^{obj}_{i,j} \\sum _{c \\in class} \\big [p(c^{j}_{i}) \\\\&amp;\\times \\log \\left ({\\sigma \\left ({p(\\hat {c}^{j}_{i})}\\right)}\\right) -\\left ({1-p(c^{j}_{i})}\\right) \\\\&amp;\\times \\log \\left ({1-\\sigma \\left ({p(\\hat {c}^{j}_{i}) }\\right)}\\right) \\big] \\tag{14}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {L}_{Localization}=&amp;\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} \\mathcal {L}_{GIoU}(b^{j}_{i},\\hat {b}^{j}_{i})\\tag{12}\\\\ {L}_{Objectness}=&amp;\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} {\\mathsf {1}}^{obj}_{i,j} \\big [c^{j}_{i} \\times \\log \\left ({\\sigma \\left ({\\hat {c}^{j}_{i}}\\right)}\\right) \\\\&amp;-\\left ({1-c^{j}_{i}}\\right) \\times \\log \\left ({1-\\sigma \\left ({\\hat {c}^{j}_{i} }\\right)}\\right) \\big] \\\\&amp;+\\lambda _{noobj}\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} {\\mathsf {1}}^{noobj}_{i,j} \\big [c^{j}_{i} \\times \\log \\left ({\\sigma \\left ({\\hat {c}^{j}_{i}}\\right)}\\right) \\\\&amp;-\\left ({1-c^{j}_{i}}\\right) \\times \\log \\left ({1-\\sigma \\left ({\\hat {c^{j}_{i}} }\\right)}\\right) \\big] \\tag{13}\\\\ {L}_{Classification}=&amp;\\sum _{i=0}^{S^{2}} \\sum _{j=0}^{B} {\\mathsf {1}}^{obj}_{i,j} \\sum _{c \\in class} \\big [p(c^{j}_{i}) \\\\&amp;\\times \\log \\left ({\\sigma \\left ({p(\\hat {c}^{j}_{i})}\\right)}\\right) -\\left ({1-p(c^{j}_{i})}\\right) \\\\&amp;\\times \\log \\left ({1-\\sigma \\left ({p(\\hat {c}^{j}_{i}) }\\right)}\\right) \\big] \\tag{14}\\end{align*}\n</span></span></disp-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s^{2}=(s\\times s)$\n</tex-math></inline-formula> is the number of cells of the feature map, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B$\n</tex-math></inline-formula>, which is set to 3 in our model, is the number of bounding boxes generated by each cell. In the localization loss equation (<a ref-type=\"disp-formula\" anchor=\"deqn12-deqn14\" href=\"#deqn12-deqn14\" class=\"fulltext-link\">Eq. 13</a>), <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b^{j}_{i}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {b}^{j}_{i}$\n</tex-math></inline-formula> are the true and predicted bounding boxes coordinates, respectively. The objectness loss (<a ref-type=\"disp-formula\" anchor=\"deqn12-deqn14\" href=\"#deqn12-deqn14\" class=\"fulltext-link\">Eq. 13</a>) is calculated using BCEWithLogitsLoss, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c^{j}_{i}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {c}^{j}_{i}$\n</tex-math></inline-formula> are the true and predicted confidences, respectively. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathsf {1}}^{noobj}_{i,j}$\n</tex-math></inline-formula> is used to determine if the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j^{th}$\n</tex-math></inline-formula> bounding box of the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> cell is not responsible for the detection of the object. In addition, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{noobj}$\n</tex-math></inline-formula> is the weight of GIoU loss, which is set to 0.5 in our experiments. Similarly, the classification loss (<a ref-type=\"disp-formula\" anchor=\"deqn12-deqn14\" href=\"#deqn12-deqn14\" class=\"fulltext-link\">Eq. 14</a>) is calculated using BCEWithLogitsLoss, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p(c^{j}_{i})$\n</tex-math></inline-formula> is the ground truth probability that the object in the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> cell belongs to class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula> (in our case the single class food), and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p(\\hat {c}^{j}_{i})$\n</tex-math></inline-formula> is the predicted probability. Similar to the objectness loss, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathsf {1}}^{obj}_{i,j}$\n</tex-math></inline-formula> checks if the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j^{th}$\n</tex-math></inline-formula> bounding box of the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{th}$\n</tex-math></inline-formula> cell is the one responsible for the detection. It is equal to 1 if the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$GIoU(Bounding Box, Ground Truth)$\n</tex-math></inline-formula> is 1 (i.e., the maximum value possible), and 0 otherwise.</p></div></div><div class=\"section_2\" id=\"sec3b\"><h3>B. NOVA Classification Model</h3><p>Once food items have been detected and localized in an image using our food item detection model described above, they are fed to the NOVA classification model, which classifies each food item into one or more of the four NOVA groups <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_3b\">[1]</a> based on their nature and the extent of the industrial processes that were applied to the food items. The first group is the Unprocessed or Minimally Processed Food, which includes natural food items such as vegetables, fruits, eggs, milk, water, etc. The second group is the Processed Culinary Ingredients, which is usually acquired from the first group and includes butter, oil, honey, etc. The third group is the Processed Food, which includes products made by adding salt, sugar or other Group 2 substances to Group 1 food such as unpackaged bread, canned fish, canned vegetables, etc. Finally, the fourth group is the Ultra-Processed Food, which includes food items that are produced using a series of industrial processes such as chips, chocolate, soft drinks, hotdog, etc. Since a food item can belong to more than one of these groups at the same time, our NOVA classification model is a multi-label classifier. We first describe the dataset we used to train the model and then describe the model itself afterwards.</p><div class=\"section_2\" id=\"sec3b1\"><h4>1) Dataset</h4><p>To train our NOVA classification model, we used the NOVA dataset we created ourselves as part of the training data for the food item detection model. To the best of our knowledge, no public datasets are available that contain images of food items with their corresponding NOVA food groups. Recall that the NOVA dataset consists of 1,800 images of Tunisian food items captured through wearable cameras in real-life settings. Each one of these images was annotated by two different trained Tunisian nutritionists using the Labelbox crowdsourcing platform. The annotators localized each food item in each image by drawing a bounding box around it. In addition, for each localized food item, each of the two annotators assigned it to one or more of the four NOVA food groups based on its processing level as perceived by the annotator. In case it was not possible for an annotator to assign a certain food item to any of the NOVA groups based on its image, the annotator indicated this by not assigning the food item to any of the groups.</p><p>The agreement between the two annotators was then calculated over all the food items detected in all the images in the NOVA dataset using 1) the IoU of the bounding boxes provided by the two annotators to localize each detected food item, and 2) the agreement over the assigned NOVA groups between the two annotators for each detected food item. The agreement score between the two annotators was 85%. As can be observed, there was a disagreement on the food groups for a small portion of the detected food items between the two annotators, which can be mainly attributed to the fact that some food items contain some ingredients that were not visible in the images such as oil and salt. To address this, the two annotators were asked to go over all the food items with an agreement less than 95% until they agreed on their NOVA groups.</p><p>After the just described annotation process was completed, we extracted cropped images of all the detected food items in the original images using their bounding boxes\u2019 coordinates. We thus ended up with a dataset that contains 4,201 images of different food items that belong to the different NOVA groups. Out of these, 185 food items were not assigned to any NOVA food group, and were thus removed from the dataset. We also removed any images of food items that were too blurry or too small from the dataset. Thus, the final dataset that we used to train the NOVA classification model consisted of 3,728 images of food items that belong to the different NOVA groups as shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>. As a food item can belong to multiple NOVA groups at the same time, in the table, the column corresponding to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$xy$\n</tex-math></inline-formula> indicates the number of food items that belong to both groups <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y$\n</tex-math></inline-formula>. For example, the number of food items in the NOVA dataset that belong to groups 1 and 2 is equal to 510 food items.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nNOVA Dataset: Distribution of Food Items Over the NOVA Food Groups</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t3-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t3-3227769-small.gif\" alt=\"Table 3- &#10;NOVA Dataset: Distribution of Food Items Over the NOVA Food Groups\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3b2\"><h4>2) Model</h4><p>Our NOVA classification model is based on the MobileNetV2 architecture <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_3b2\">[7]</a>. The architecture of MobileNetV2 contains an initial fully convolutional layer with 32 filters, followed by 19 residual bottleneck layers. It uses ReLU6 as a non-linearity because of its robustness when used with low-precision computations. It uses a kernel size of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$3 \\times 3$\n</tex-math></inline-formula> as is standard in modern networks, and utilizes dropout and batch normalization during training. To adapt MobileNetv2 to our case and use it to classify a food item into one or more of the NOVA groups, we built a model that is made up of MobileNetV2 architecture loaded with frozen ImageNet weights, and we added a classifier on the top of it. The classifier consisted of a global average pooling layer followed by a dense layer of 250 neurons and a dropout layer with a dropout rate of 0.5. The output layer is a dense layer with four neurons representing the four NOVA groups. Each of the four neurons uses a sigmoid activation function to output the probability of a food item belonging to each of the four NOVA classes. Finally, to train the full model, we used Binary Cross Entropy loss as a loss function.</p></div></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Experiments</h2></div><p>In this section, we describe how we trained our two models, the food item detection model and the NOVA classification model. We report on the performance of each model, and then provide some error analysis for each one.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Food Item Detection Model</h3><p>For the food item detection model, we trained three different versions of the model described in <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a> using our three training datasets, the UECFood256 dataset, the EgocentricFood dataset, and the NOVA dataset. For the first model, we obtained the pretrained weights for the base YOLOv3 model on the COCO dataset <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_4a\">[28]</a> and retrained the whole food item detection model using the UECFood256 dataset. The dataset was split into 80% for training (23,119 images) and 20% for validation (5,780 images). We refer to this model as the <i>UECFood256 model</i>.</p><p>For our second model, we started with the UECFood256 model, and froze the weights of the backbone network, Darknet-53, which is used as the feature extractor. We then only trained the head of our food item detection model via transfer learning using the EgocentricFood dataset. Again, this dataset was split into 80% for training (4,038 images) and 20% for validation (1,000 images). We refer to this second model as the <i>EgocentricFood model</i>.</p><p>Finally, for our third model, we again started with the UECFood256 model and did transfer learning by freezing the backbone of our model, Darknet-53. We then trained only the head of the model, however on a combination of the NOVA dataset and the EgocentricFood dataset, but after removing a random sample of 500 images from the NOVA dataset, which is used as our test data. We split the combined data into 80% for training and 20% for testing. We refer to this model as the <i>NOVA and EgocentricFood model</i>.</p><p>The three above-described models were all trained for 100 epochs on images of size <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$416\\times 416$\n</tex-math></inline-formula>. We used a learning rate of 0.01 and a decay weight of 0.0005. Moreover, we used different anchor boxes, depending on the dataset that was used for training. That is, we generated nine anchor boxes for each dataset using the k-means algorithm as explained in <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>. <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a> shows the anchor boxes for each dataset on three scales.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nThe Generated Anchor Boxes for Each Dataset</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t4-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t4-3227769-small.gif\" alt=\"Table 4- &#10;The Generated Anchor Boxes for Each Dataset\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>To evaluate the three models, we used the mean Average Precision (mAP) metric. mAP is the mean of AP over the classes and since we only have one class, namely food, the mAP will be the same as AP. To compute mAP, we need to compute the precision (<a ref-type=\"disp-formula\" anchor=\"deqn15-deqn16\" href=\"#deqn15-deqn16\" class=\"fulltext-link\">Eq. 15</a>) and the recall (<a ref-type=\"disp-formula\" anchor=\"deqn15-deqn16\" href=\"#deqn15-deqn16\" class=\"fulltext-link\">Eq. 16</a>). We also need to specify an IoU threshold value. For a threshold value of 0.5, a detected object is a True Positive (TP) if <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$IoU\\geq 0.5$\n</tex-math></inline-formula>, otherwise it is a False Positive (FP). On the other hand, a False Negative (FN) is a food item that was not detected and a True Negative (TN) is any part of the image that does not contain any food item.<disp-formula id=\"deqn15-deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Precision=&amp;\\frac {TP}{TP+FP} \\tag{15}\\\\ Recall=&amp;\\frac {TP}{TP+FN} \\tag{16}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Precision=&amp;\\frac {TP}{TP+FP} \\tag{15}\\\\ Recall=&amp;\\frac {TP}{TP+FN} \\tag{16}\\end{align*}\n</span></span></disp-formula> In our evaluation, we used two mAP versions: the Pascal Visual Object Classes (VOC) metric <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_4a\">[30]</a> and the COCO metric <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_4a\">[28]</a>. The Pascal VOC metric, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$mAp\\text{@}0.5$\n</tex-math></inline-formula> calculates the mAP for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$IoU\\geq 0.5$\n</tex-math></inline-formula>. The COCO metric, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$mAP\\text{@}[{0.5:0.95}]$\n</tex-math></inline-formula>, calculates the average of the mAP over different IoU thresholds that range from 0.5 to 0.95 with a step size of 0.05.</p><p>In order to compare the three versions of our food item detection model describe above, we tested them on 500 images that were sampled from the NOVA dataset and were not part of the training or validation. <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a> shows the results for the three models using different metrics. We can clearly see that the <i>NOVA and EgocentricFood model</i> outperforms the first two models for all metrics. The UECFood256 model achieved the worst results since the images in this dataset were crawled from the Web and are all homogeneous, which thus does not generalize well to other more realistic images such as those captured through wearable cameras. The EgocentricFood model achieved better results since the images in this dataset are taken from wearable camera, which are very similar to the test images.<div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nTest Results for Three Food Item Detection Models</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t5-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t5-3227769-small.gif\" alt=\"Table 5- &#10;Test Results for Three Food Item Detection Models\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>As a baseline, we also trained a standard YOLOv3 model on the combined NOVA and EgocentricFood datasets (with the same train-validation split as the NOVA and EgocentricFood model) and compared it with our best performing model, the NOVA and EgocentricFood model. <a ref-type=\"table\" anchor=\"table6\" class=\"fulltext-link\">Table 6</a> shows the localization loss and the objectness loss for each model on the validation set. As can be seen from the table, the NOVA and EgocentricFood model outperformed the baseline YOLOv3 model for both loss functions (smaller is better) as well as in terms of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$mAP\\text{@}o.5$\n</tex-math></inline-formula> (with 3% improvement). This can be mainly attributed to the use of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathcal {L}_{GIoU}$\n</tex-math></inline-formula> instead of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$MSE$\n</tex-math></inline-formula> when calculating the error between the ground truth bounding boxes and the predicted ones.<div class=\"figure figure-full table\" id=\"table6\"><div class=\"figcaption\"><b class=\"title\">TABLE 6 </b>\nValidation Results for the YOLOv3 vs. the NOVA and EgocentricFood Models</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t6-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t6-3227769-small.gif\" alt=\"Table 6- &#10;Validation Results for the YOLOv3 vs. the NOVA and EgocentricFood Models\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Finally, in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7</a>, we show the results obtained by applying the NOVA and EgocentricFood model on a sample test images from the NOVA dataset. As can be seen from the figure, the model was able to detect most of the food items that appear in the sample images shown. On the other hand, the model was not able to detect some of the food items that are occluded by other objects such as the dish in the top image of set (a), which is occluded by a water bottle. On the other hand, overall the model was robust, as it was able to detect small food items that are even far from the table, as can be seen in the top image of set (b). This is mainly due to the fact that our food item detection model is applied on three different image scales (small, medium, and large). This actually had a negative effect on the reported precision, by increasing the number of false positives, since the annotators did not localize food items that are far away from the table when generating the ground truth. Finally, some of the food items had overlapping bounding boxes(such as the bottom image in set (b)), and we addressed this by excluding the ones with a low confidence score (less than 0.40), as is custom in object detection in general.\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas7-3227769-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas7-3227769-small.gif\" alt=\"FIGURE 7. - Results of the food item detection model on sample test images.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Results of the food item detection model on sample test images.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Nova Classification Model</h3><p>The NOVA classification model was trained using the NOVA dataset described in <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>. The dataset was split into 80% for training, 10% for validation and 10% for testing. Recall that the model consists of two parts, a base MobileNetV2 backbone and a classifier on top of it that consisted of a global average pooling layer followed by a dense layer of 250 neurons and a dropout layer with a dropout rate of 0.5. The final output layer of the model is a dense layer with four neurons representing the four NOVA groups. To train the model, the MobileNetV2 backbone was loaded with pretrained ImageNet weights (i.e., transfer learning), its layers were frozen and the rest of the model was trained for 20 epochs using the Adam optimizer with a learning rate of 0.001. After that, we did fine-tuning by unfreezing the last 55 layers of the model and retraining the model for 10 more epochs with a learning rate of 0.0001.</p><p>Since the NOVA dataset contains images of various sizes, we used different image sizes as a hyperparameter (128, 160, 192, 224). We resized the images using Bilinear Interpolation, which is a resampling method that calculates a new pixel value based on the distance weighted average of the nearest four pixels <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_4b\">[31]</a>. After training our NOVA classification model with different image sizes, the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$224\\times 224$\n</tex-math></inline-formula> image size was the best fit for the model based on the validation set. The results of this model on the testing data is shown in <a ref-type=\"table\" anchor=\"table7\" class=\"fulltext-link\">Table 7</a>, which shows the precision, the recall, and the F1-score for each of the four NOVA groups.<div class=\"figure figure-full table\" id=\"table7\"><div class=\"figcaption\"><b class=\"title\">TABLE 7 </b>\nNOVA Classification Model Test Results</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t7-3227769-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas.t7-3227769-small.gif\" alt=\"Table 7- &#10;NOVA Classification Model Test Results\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>While the overall performance of the model on the test data was relatively high, however, some of the images were misclassified due to the complexity of the food items. We noticed that our model was not able to predict all the ground truth NOVA groups for some of the images that contain ingredients that are not visible to the model such as salt and oil. <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Figure 8</a> shows a sample of misclassified images by the NOVA model. For example, the first image\u2019s ground truth NOVA groups are 1 and 2 since it is a salad, and 4 because it contains cheese. The model was able to correctly predict that the food item belong to groups 1 and 2, however, it did not predict group 4 since most of the salad related images in the training set belong to groups 1 and 2 only. Another example is image 3, where the ground truth indicates that the food item belongs to the NOVA groups 1, 3, and 4 since it contains bread, tomatoes, and cheese. The model correctly predicted that it belongs to groups 1 and 3 and it missed group 4. These results explain why we did not obtain a very high recall for some of the NOVA groups.\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas8-3227769-large.gif\" data-fig-id=\"fig8\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975313/elbas8-3227769-small.gif\" alt=\"FIGURE 8. - Sample misclassified food items by the NOVA classification model.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p>Sample misclassified food items by the NOVA classification model.</p></fig></div><p class=\"links\"><a href=\"/document/9975313/all-figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion and Future Work</h2></div><p>In this manuscript, we proposed DeepNOVA, a novel end-to-end deep learning approach that can assess the healthiness of food in images based on the NOVA classification system. Our approach can be used by nutritionists to study the dietary intake of a target population, which is traditionally done using interviews and questionnaires, and is known to suffer from recall bias. Our approach consists of two main models, a food item detection model followed by a NOVA classification model. The food item detection model is a custom object detection model that is trained to detect and localize any food item in an image, while the NOVA classification model is a multi-label classification model that assigns a food item to one or more of the NOVA food groups based on its processing level. Both models were trained using a combination of public datasets and a custom dataset that we generated using wearable cameras and that was annotated by trained nutritionists.</p><p>In future work, we plan to use DeepNOVA in real-world nutritional case studies. Since the NOVA classification model was trained on images of Tunisian food, and was labeled by Tunisian nutritionist, the model has to be fine-tuned based on the type of food in the case study. We do not consider this a limitation as it mimics human behavior when assessing the healthiness of food, which requires local knowledge about the food being assessed and how it is processed. However, our model can be used as a pretrained model and fine-tuned using other food types via transfer learning.</p></div>\n<h3>ACKNOWLEDGMENT</h3><p>The authors would like to thank the School and Community Drivers of Child Diets in Arab Cities; Identifying Levers for Intervention SCALE Research Group for supporting this work (Akik Chaza, Doggui Radhouene, El-Helou Nehmat, Jamaluddine Zeina, Safadi Gloria, and Trabelsi Tarek).</p></div></div></response>\n"
}