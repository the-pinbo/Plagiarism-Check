{
    "abstract": "Wargame is an important tool that enables training units to develop various strategies by allowing them to experience unexpected situations. There are three methodologies that determine the behavior of the Computer Generated Forces(CGF) in wargame\u2014rule-based, agent-based, and learning-based methodologies. The military determines the behaviors of the CGF mainly based on the rules because a doctrine...",
    "articleNumber": "9976053",
    "articleTitle": "Experimental and Computational Study on the Ground Forces CGF Automation of Wargame Models Using Reinforcement Learning",
    "authors": [
        {
            "preferredName": "Minwoo Choi",
            "normalizedName": "M. Choi",
            "firstName": "Minwoo",
            "lastName": "Choi",
            "searchablePreferredName": "Minwoo Choi"
        },
        {
            "preferredName": "Hoseok Moon",
            "normalizedName": "H. Moon",
            "firstName": "Hoseok",
            "lastName": "Moon",
            "searchablePreferredName": "Hoseok Moon"
        },
        {
            "preferredName": "Sangwoo Han",
            "normalizedName": "S. Han",
            "firstName": "Sangwoo",
            "lastName": "Han",
            "searchablePreferredName": "Sangwoo Han"
        },
        {
            "preferredName": "Yongchan Choi",
            "normalizedName": "Y. Choi",
            "firstName": "Yongchan",
            "lastName": "Choi",
            "searchablePreferredName": "Yongchan Choi"
        },
        {
            "preferredName": "Minho Lee",
            "normalizedName": "M. Lee",
            "firstName": "Minho",
            "lastName": "Lee",
            "searchablePreferredName": "Minho Lee"
        },
        {
            "preferredName": "Namsuk Cho",
            "normalizedName": "N. Cho",
            "firstName": "Namsuk",
            "lastName": "Cho",
            "searchablePreferredName": "Namsuk Cho"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227797",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9976053/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Wargame has been proved to be a valuable tool for understanding the uncertainty of the battlefield and the changing paradigm of war, especially in terms of training troops and verifying operational plans <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. As a result, the role of wargame has become more critical in the future battlefield environment, which is changing rapidly.</p><div class=\"section_2\" id=\"sec1a\"><h3>A. Importance of Automating CGF</h3><p>Computer Generated Forces (CGF) describes the behavior of combatants or weapons systems in the wargame model <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1a\">[2]</a>, and CGF\u2019s behavior is essential in simulating a realistic battlefield environment and combat situation <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1a\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1a\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1a\">[5]</a>. One of the most critical roles of CGF is to make training participants perceive CGF to be realistic, and therefore, it is necessary to automate CGF because of the following reasons. First, in the current wargame model, CGFs do not work well in an undefined situation, because they behave under predefined conditions such as military doctrine, operation plan, and simulation logic of existing models. Second, outcomes of the wargame may be affected by a difference in the ability of the gamers controlling CGF, even though only the operation plan established by the Operation Planning Process(OPP), the Course Of Action(COA) of the commander, and staff should be the factors determining the success or failure of the wargame. Third, the atypical training situation in wargame caused by the automation of CGF can enhance the wargame\u2019s immersion of wargame and increase the combatants\u2019 training effect <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1a\">[6]</a>. Last, automating CGF (a) reduces the operational requirements of game operators, (b) increases the efficiency of troop operations, and (c) reduces administrative requirements, such as inputting a scenario <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1a\">[7]</a>.</p></div><div class=\"section_2\" id=\"sec1b\"><h3>B. CGF Automation Methodology and Reinforcement Learning</h3><p>There are three representative methods of automating CGF: rule-based, agent-based, and learning-based.</p><p>Rule-based is a methodology in which CGF operates according to defined rules. This methodology makes the behavior of CGFs intuitive and easy to implement, since the military writes manuals or plans. However, it is difficult to express all situations as a rule, and gamers who understand the rule of CGF operation can perform actions which are not possible in the real battlefield to win the wargame. For example, when troops are trained through actual wargame, the gamers of the troop can easily understand the rules of the opposing forces CGF and use tactics to win the game, because the rules of behavior of CGF implemented in wargame are relatively simple. For this reason, the effectiveness of training is sometimes reduced <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1b\">[7]</a>.</p><p>The agent-based methodology provides minimal rules to agents and determines behaviors through mutual information exchange. Agent-based methods have fewer rules to be defined than rule-based methodologies, in that they can express undefined behavior through information exchange between agents. In Context-Based Reasoning (CxBR) and Belief-Desire-Intention (BDI), which are two representative methods of the agent-based wargame model, all goals and actions of the agent must be defined as scenarios. When creating a scenario, domain knowledge must be communicated well for the agent to function properly. However, as the size of the unit grows, scenario creation becomes more complex, which makes it difficult to implement the scenario into a model <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1b\">[8]</a>.</p><p>Learning-based methodologies use machine learning, such as supervised, unsupervised, or reinforcement learning. This method best expresses the cognitive ability of combatants, in that it can autonomously judge agents\u2019 behavior. Although there is a study of training two combatants CGF with a supervised learning <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1b\">[9]</a>, supervised and unsupervised learnings are not widely used, as they require a large amount of data. In addition, it is difficult to judge whether a certain combat activity is right or wrong by using those learning methods <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1b\">[10]</a>. On the other hand, reinforcement learning has the advantage in that it does not require correct answers, as agents build data by repeating episodes in the environment and improve behaviors in correct directions through rewards. As agents can learn various tasks even in a multi-agent environment, the description of CGF using reinforcement learning is becoming a reality <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1b\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1b\">[12]</a>.</p><p>So far, we have described three methodologies for CGF automation. Since the military possesses well established doctrines and operation plans, it mainly uses the rule-based methodology in the wargame model and has shown its effectiveness in combat readiness. Nevertheless, it is necessary to convey CGF research with reinforcement learning, as expressing every situation as a rule is impossible, and it is cumbersome to define a rule and input it into the wargame every time a new weapon system or tactic comes out. The agent-based method can be thought as an alternative, but this method requires defining rules as much as the rule-based method does, along with writing a battle scenario. Therefore, we studied the automation of CGF by using a reinforcement learning method that does not require separate rules and scenarios. By applying this advanced artificial intelligence technologies, we expect to find improvements in CGF.</p><p>Our study aims to create a simulation environment in consideration of combat situations, and make CGF learn by reinforcement learning algorithms to confirm the automation possibility of ground forces CGF. While setting environments, we considered the general situations of the military, such as close combat, a reconnaissance operations, and logistic support missions. The structure of this study is as follows: in <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a>, we described the literature review of CGF automation by applying the three methodologies described above. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a> describes the environment established to use the reinforcement learning methodology, and <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a> presents experimental results of this study. Finally, in <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a>, we present conclusions.</p></div></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Literature Review</h2></div><p>This section describes CGF automation research by using rule-based, agent-based, and reinforcement learning methodologies.</p><p>In rule-based methodology, CGF determines the correct behavior by taking into account the real-time situation of the model through predefined rules. However, it is difficult for CGF to decide on the suitable action as military knowledge and manuals are vast, and a variety of situations can occur on the actual battlefield. Since the military has well-established rules such as doctrine and operational plans, it is more important to study the framework that allows the CGF to determine the action by efficiently processing the rules rather than to study the rules themselves. As a framework study, there are studies the operation of CGF on the framework by modularizing the battle decision process and the algorithm that can operate in the framework <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\">[14]</a>. In addition, the rule-based system has a limitation in that it dose not consider the uncertainty of the battlefield. In <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2\">[15]</a>, the CGF acted by using a sequential decision-making model that probabilistically considers uncertain or partially observable situations.</p><p>In the agent-based methodology, agents decide their actions through information exchange with other agents under minimal rules. Therefore, agent-based studies include organizing agents as well as communication of agent-to-agent information well. For example, in the study of an agent management, a small combat model framework under a multi-agent environment was established by configuring agents that determine major tasks, and sub-agents that perform functions such as maneuver and detection <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2\">[16]</a>. And in <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2\">[17]</a>, the researcher created a virtual command center agent to induce cooperation of agents, so as to make CGFs cooperate under a multi-agent environment. Some countries in NATO are studying agent based models by applying CxBR and BDI methods. The idea of CxBR is that humans use only a fraction of their knowledge when they infer, by classifying situations faced by agents into context and making a limited choice over one of the actions that fit the context. BDI is another methodology that describes human reasoning methods. Belief represents information about the agent\u2019s situation, and Desire means the goal that the agent should achieve. Intention stands for the process of the agent selecting an action by considering the Belief and the Desire. Huet al. <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2\">[8]</a> compared of the performances of the C2(Command and Control) system in cases when CxBR and BDI were respectively applied to it. There are other CGF studies done with CxBR and BDI, such as <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2\">[18]</a> and <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2\">[19]</a>.</p><p>CGF automation using reinforcement learning methodology focuses mainly on Air Force fighters as CGF. The researchers automated the fighter\u2019s behavior in a two-to-one aerial combat situation through reinforcement learning <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2\">[20]</a>. In the works in <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2\">[21]</a>, researchers compared the performance of trained fighter CGFs and human-manipulated CGFs. In this study, trained CGF lost to skilled pilots but won against beginner-level pilots, suggesting the possibility of the CGF automation using reinforcement learning methodology. In addition to the combat behavior of the fighter CGF, there was a study done to learn maneuver. The researchers in <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2\">[22]</a> studied the maneuver of fighter CGF in an environment where an Anti-Aircraft Defense system exists, by using a curriculum learning of making CGF reach the goal while solving sub-problems. Also, reinforcement learning research on ground forces CGF compared algorithms in a simple environment considering Rendezvous with obstacle avoidance <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2\">[23]</a>.</p><p>CGF research using reinforcement learning also operates under a casual game environment. For example, the agent learned from the classic game Pong or Breakout from the Atari2600 game environment <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2\">[24]</a>. Some studies have been conducted in 3D environments that are more complex than 2D environments. For example, in Doom, a 3D-based FPS (First Person-Shooting) game, the agent learned how to shoot. As a result, the trained agent performed better than the rule-based agent and the human-manipulated agent <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2\">[25]</a>. Whereas the previous two studies have only considered a single agent, and had relatively simple environments, some studies such as Google DeepMind\u2019s case of StarCraft reinforcement learning were done under more challenging conditions <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2\">[27]</a>. Despite a complex environment of StarCraft in which all settings change in real-time and multiple agents have to choose actions, CGF has succeeded to learn. In addition, individual units of StarCraft have learned with multi-agent algorithms. StarCraft research has been continued, and the current level of learning is to a degree of winning against professional gamers.</p><p>Applying reinforcement learning to CGF for wargame differentiates from using reinforcement learning on casual game players, as wargames and casual games have different purposes and methods. Casual games aim to make players complete the final mission, but wargames aim to make them establish various tactics and strategies by providing an indirect experience of the combat situation of the training unit. For this reason, there are differences in the way the casual game and war game progress. Casual games are linear and game progress step-by-step, in that a player completes the final mission while solving sub-missions with correct answers. On the other hand, there is no right answer in wargames, as the behavior of the CGF can vary depending on the tactics used even in the same battle situation.</p><div class=\"section_2\" id=\"sec2a\"><h3>A. The Contribution of Research</h3><p>The main contribution of our research, which applies reinforcement learning for the automation of CGF in the wargame model, is that we experimented with various combat environments after creating environments based on the combat functions of the ground forces. The details are as follows.\n<ul style=\"list-style-type:disc\"><li><p>First, we applied reinforcement learning to automatically simulate the CGF of the ground forces wargame model. Until now, most of the cases of simulating the wargame\u2019s CGF as reinforcement learning were made for the Air Force\u2019s fighters. Although there are previous studies on CGF of ground forces, but most of them were conducted by (a) explaining the necessity of applying reinforcement learning methodologies, (b) theoretically predicting the effects without specific experiments when applying the reinforcement learning method (c) or relatively simplifying the composition of objects and environments.</p></li><li><p>Second, our study reflects the various characteristics of the ground forces. It is difficult to apply the fighter CGF research to the ground forces because the combat of the ground forces is based on the Combined Arms Combat (CAC) in which two or more branches cooperate. In addition, the process of describing ground combat is more complex than that of describing air combat. Moreover, combat functions such as maneuver, intelligence, and logistics are systematically performed in ground combat, making it necessary to model and test these combat functions. In order to fulfill this condition, we have tested the implementation of CAC, multi-drones, and CSS(Combat Service Support) environments.</p></li><li><p>Finally, we can suggest the applicability of reinforcement learning CGF to wargame models from an expert\u2019s point of view. To apply reinforcement learning CGF to the wargame model, the experimental results must be analyzed from the perspective of the military. Our study verified whether the ground forces CGF behaves correctly based on a high level of understanding in the military field by comparing it with its doctrine.</p></li></ul></p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Methodology</h2></div><p>This section describes the environment for experiments and defines states, actions, and rewards according to the environment.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Experimental Design</h3><p>We considered the combat function of the ground forces to create a wargame environment. Combat function refers to the military roles and activities that must be performed to achieve the concept of operational execution in ground operations. Also, it consists of six functions: command and control, intelligence, maneuver, protection, firepower, and logistic support. Ground forces must perform and integrate these functions when conducting operations to exert combat power. Therefore, we configured the experimental environment as follows.</p><p>There are four experimental environments for reinforcement learning of ground forces CGF: close combat, military training, reconnaissance drones, and logistic support. <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> shows the details of each environment. First of all, close combat is the situation that is the most frequently encountered by ground forces on the battlefield. A battle between infantries and a battle under artillery fire support are the examples. Therefore, we composed the close combat environment for the following four cases, while assuming situations that could take place on the battlefield.\n<ul style=\"list-style-type:disc\"><li><p>Case1: Case1 is a 1:1 infantry combat environment where a single infantry agent is trained. The purpose of this agent\u2019s learning is to learn how to recognize the opponent as an enemy and attack them to win the combat. The experimental environment is shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1(a)</a>.</p></li><li><p>Case2: Case2 is a 2:2 infantry combat environment where infantry agents learn to cooperate. Cooperation between combatants is essential in real combat because a squad consists of several combatants.</p></li><li><p>Case3: Case3 is a 2:2 environment where infantry teams fight against artillery teams. Both Infantry and artillery CGF, as agents, are expected to learn to combat differently reflecting the characteristics of different weapons they have. To this end, we made the firing range and firing interval of the artillery CGF to be three and five times longer than that of the infantry CGF, respectively.</p></li><li><p>Case4: Case4 is a 2:2 environment in which one infantry and one artillery fight as a team, and the team\u2019s CGFs are expected to learn cooperative combat between agents of different branches in this environment. This cooperation is called Combined Arms Combat, and it increases the power of the team. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1(b)</a> visualized this experimental environment.</p></li></ul><div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nExperimental Design</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t1-3227797-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t1-3227797-small.gif\" alt=\"Table 1- &#10;Experimental Design\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho1abcdef-3227797-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho1abcdef-3227797-small.gif\" alt=\"FIGURE 1. - Experimental environments.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Experimental environments.</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div></p><p>While Case1 to Case4 dealt with a maneuver and a firepower among the combat functions of the ground forces, the following three cases consist of environments which are associated with intelligence, logistics, and military training.\n<ul style=\"list-style-type:disc\"><li><p>Case5: Case5 is an environment set to make trainee CGF learn. Although the combat behavior specified in the manual does not necessarily guarantee making optimal choice, it is still necessary to train trainee CGF as specified in the doctrine. Trainee CGF has to engage the enemy and reach the goal. The purpose of this learning is that trainee CGF would do both things\u2014and therefore, it will not consider reaching the target without engaging the enemy. The <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1(c), (d)</a> shows the environment.</p></li><li><p>Case6: Case6 is an environment for learning drone CGFs. Automation research on drone CGF is essential, because drones are highly valuable as reconnaissance and attacking weapons. Basically, the leader drone and the follower drone learn how to find and attack the enemy within the operational area, while the follower drones additionally learn to maintain a certain distance from the leader drone during the fight. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1(e)</a> shows the environment of Case6, with the drone group consisting of 1 leader drone and 5 follower drones, and the target group consisting of infantry and tanks.</p></li><li><p>Case7: In Case7, we trained two CSS CGFs. Combat Service Support (CSS) is a battlefield function which is vital for combat continuity. The CSS CGF\u2019s task is to deliver ammunition according to the principle of logistics: to deliver the right amount of ammunition, at the right time, and to the support units that need it the most. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1(f)</a> shows the environment of Case7, consisting of two CSS CGFs carrying ammunition, Ammunition Supply Point (ASP), and five units requiring ammunition supply. The environment of Case7 is different from the general delivery service environment, in that the supply amount and supply location were not planned in advance.</p></li></ul></p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Tools and Algorithms</h3><p>Configuring the environment is important in reinforcement learning, as it determines the learning direction of CGF. There are two main ways of creating the environment in reinforcement learning. One is to use open-sources that provide pre-built environments. For example, OpenAI Gym, Gym Roboschool, Gym Extensions, or PyBullet can be considered <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_3b\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_3b\">[29]</a>. The other way is for researchers to use general programming languages such as Python, Matlab, or Net-Logo <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_3b\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_3b\">[30]</a>. We chose the Unity <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_3b\">[31]</a> to construct 7cases of <a ref-type=\"sec\" anchor=\"sec3a\" class=\"fulltext-link\">Section III-A</a>. It\u2019s best to use currently used wargame as an environment, but unfortunately, there is no wargame model that supports reinforcement learning. In such an aspect, research using Unity has the following advantages. First, Unity can describe a variety of environments, including physical laws, so Unity can implement CGF and resolutions most similar to wargame model. Second, Unity is a highly reliable simulation tool, as it has been used widely in various fields. Last, Unity provides the latest reinforcement learning package that researchers can use with convenience, thereby reducing administrative requirements for organizing experiments.</p><p>The following is information about the algorithms used in our study. Research on reinforcement learning can be usually divided in two parts; first, the part of improving the performance of algorithms, and second, the part of applying algorithms to solve problems under a specific domain. The purpose of this study is consistent with the second reason. Therefore, we use the following representative algorithms that are proven to be fair. The algorithm we used for each case is shown in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>.\n<ul style=\"list-style-type:disc\"><li><p>Proximal Policy Optimization (PPO) is based on an Actor-Critic algorithm <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_3b\">[32]</a>, and it shows an excellent performance as an algorithm that increases learning speed through importance sampling techniques and data reuse. More detailed information can be found on <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_3b\">[33]</a>.</p></li><li><p>Multi-Agent Posthumous Credit Assignment (MAPOCA) is a multi-agent algorithm. As multi-agents are generally rewarded at a group level, all agents will be rewarded if an agent team wins the game. To reward in such a way, centralized critiques, which are a neural network that serve as a \u2019coach\u2019 for the entire group of agents, were trained and improved to even reward agents who did not directly contribute to the victory. Further details of the algorithm are provided on <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_3b\">[34]</a>.</p></li><li><p>Self-play is a method that increases the effectiveness of learning in environments of competing with opponents, such as chess, tennis, and soccer. It is selectively applicable to PPO and MA-POCA algorithms, which are used in our study <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_3b\">[35]</a>, <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_3b\">[36]</a>.</p></li></ul></p><p>In our experimental environment, we used the PPO algorithm when learning a single agent and the MA-POCA algorithm when learning multiple agents. In addition, in an environment where agents of the same type battle each other, a self-play algorithm was additionally applied, and the algorithm used in each case is shown in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>.</p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. State, Action, Reward</h3><p>State is information observed by the agent, and the agent determines the action by considering the state. For example, in an environment where infantry CGF learns, a researcher can set the location of enemies and terrain as state information, and movement and attack as actions. Next, the reward is the value the agent receives from the environment when the agent selects an action. Because agents update policy in the direction of maximizing reward, researchers control the behavior of CGF through reward. For example, the researcher gives infantry CGFs a positive reward if they win a battle with an enemy and a negative reward if they lose. For the experiment, the state, action, and reward for each environment are set as follows.\n<ol><li><p>State</p><p>In our study, CGF collects state information using Unity\u2019s radar function called Raycast. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula> is the state space of the environment. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula> is the observation space of all radars <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula>:=<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O^{1} \\times \\ldots \\times O^{n}$\n</tex-math></inline-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O^{i}$\n</tex-math></inline-formula> denotes the observation space of radar <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>(<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>=1,2,\u2026, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>). At time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$o_{t}^{i} \\in O^{i}$\n</tex-math></inline-formula> is the local observation of radar <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> which is correlated with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{t} \\in S$\n</tex-math></inline-formula>. Finally, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B_{j} $\n</tex-math></inline-formula> is the binary variable which indicates whether object <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>(<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>=1,2,\u2026, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula>) is detected or not.<disp-formula id=\"\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} B_{j}= \\begin{cases} \\displaystyle 1, &amp; \\text {If the object j is detected by radar } \\\\ \\displaystyle 0, &amp; \\text {otherwise} \\end{cases}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} B_{j}= \\begin{cases} \\displaystyle 1, &amp; \\text {If the object j is detected by radar } \\\\ \\displaystyle 0, &amp; \\text {otherwise} \\end{cases}\\end{align*}\n</span></span></disp-formula></p><p>Therefore, the local observation can be defined as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$o_{t}^{i}$\n</tex-math></inline-formula>=(<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B_{1}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B_{2}$\n</tex-math></inline-formula>,\u2026, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$B_{m}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$H$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$D$\n</tex-math></inline-formula>) where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$H$\n</tex-math></inline-formula> is binary variable indicating whether any object is hit by radar <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$D$\n</tex-math></inline-formula> is distance between radar <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and object hit by radar <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>. For instance, if the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>th radar\u2019s local observation at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$o_{t}^{n}$\n</tex-math></inline-formula>=(1,0,0,1,5), then the first object at distance 5 has been detected by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>th radar. The collect information meaning CGF state information for Cases1 to 7 is shown in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>.</p></li><li><p>Action</p><p>At time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>, we define action vector <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t} =$\n</tex-math></inline-formula> (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}^{1}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}^{2}$\n</tex-math></inline-formula>,\u2026, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}^{l}$\n</tex-math></inline-formula>) where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}^{k}$\n</tex-math></inline-formula> is the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>th action the agent can take (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>=1,2,\u2026, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula>). Also, actions that can be taken are divided into three types: continuous, discrete, and binary. If the action is continuous, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}$\n</tex-math></inline-formula> has a real value between 0 to 1; if it is binary, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}$\n</tex-math></inline-formula> has a value of 0 or 1. The defined actions of the agents in our environment are shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>. For example, in Case1 of <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>, if <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t} =$\n</tex-math></inline-formula> (0.7, 0.9, 0.1, 0) at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>, it signifies that the agent moves vertically by 0.7, horizontally by 0.9, and rotates by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$0.1\\times360$\n</tex-math></inline-formula> degrees, while the agent does not carry out the attack.</p></li><li><p>Reward</p><p>We set all values for rewards empirically. Also, two considerations were taken into account when we decided under what circumstances to give the rewards and how much the rewards should be. First, the definition of a reward must conform to the doctrine; and second, the rewards should conform to the reward policy of other researchers.</p><p>Cases1 to 4 are the environments where the episode ends with a positive reward for winning and a negative reward for losing. Case1 is a single agent environment where the agent receives +1 rewards if it wins and \u22121, if loses. Cases2 to 4 are multi-agent environments, and therefore, we configured an additional reward for the attack behavior, in order to trigger the combat behavior of the agents on the team. In Cases2 to 4, the reward function for the reward, which is denoted by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r$\n</tex-math></inline-formula> can be viewed in <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Equation (1)</a>.<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {r= \\begin{cases} 1, &amp; \\text {count(enemy forces) = 0}\\\\ -1, &amp; \\text {count(friendly forces) = 0}\\\\ 0.1, &amp; \\text {agent attack the enemy} \\end{cases}}\\tag{1}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {r= \\begin{cases} 1, &amp; \\text {count(enemy forces) = 0}\\\\ -1, &amp; \\text {count(friendly forces) = 0}\\\\ 0.1, &amp; \\text {agent attack the enemy} \\end{cases}}\\tag{1}\\end{align*}\n</span></span></disp-formula> where count(agent) means the number of agents in the environment.</p><p>In Case5, rewards are given according to the importance of the task that the agent must perform. It gives a high reward for reaching the goal, which is to accomplish an important mission, while giving a relatively low reward for the mission to attack the enemy. Also, the reward is determined by calculating the distance between two objects in the reward function, where the distance <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d(a,b)$\n</tex-math></inline-formula> is defined as the Euclidean distance between <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b$\n</tex-math></inline-formula>. The reward function of Case5 is defined as <a ref-type=\"disp-formula\" anchor=\"deqn2\" href=\"#deqn2\" class=\"fulltext-link\">Equation (2)</a>.<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {r= \\begin{cases} \\displaystyle 1, &amp; \\text {$d$(trainee, goal) = 0}\\\\ \\displaystyle -1, &amp; \\text {$d$(trainee, goal) = $0~\\wedge $count(enemy) = 1}\\\\ \\displaystyle -1, &amp; \\text {moving outside the environment}\\\\ \\displaystyle 0.1, &amp; \\text {agent attack the enemy} \\end{cases}}\\!\\!\\!\\!\\!\\!\\!\\!\\! \\tag{2}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {r= \\begin{cases} \\displaystyle 1, &amp; \\text {$d$(trainee, goal) = 0}\\\\ \\displaystyle -1, &amp; \\text {$d$(trainee, goal) = $0~\\wedge $count(enemy) = 1}\\\\ \\displaystyle -1, &amp; \\text {moving outside the environment}\\\\ \\displaystyle 0.1, &amp; \\text {agent attack the enemy} \\end{cases}}\\!\\!\\!\\!\\!\\!\\!\\!\\! \\tag{2}\\end{align*}\n</span></span></disp-formula></p><p>In Case6, the team rewards received by the leader drone and the follower drone are the same. The team reward is given when the drones find or attack a target, and the rewards differ depending on the type of a target. Additionally, an additional individual reward will be given to the follower drone according to its distance from the leader drone. Through this, each follower drone will be able to maintain the communication distance with the leader drone. <a ref-type=\"disp-formula\" anchor=\"deqn3\" href=\"#deqn3\" class=\"fulltext-link\">Equation (3)</a> shows the reward function of Case6.<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {r= \\begin{cases} \\displaystyle 10, &amp; \\text {count(targets) = 0}\\\\ \\displaystyle 2, &amp; \\text {eliminates(or search) a tank}\\\\ \\displaystyle 1, &amp; \\text {eliminates(or search) an infantry}\\\\ \\displaystyle 0.001, &amp; d ({\\text {leader, follow}}) &lt; m \\end{cases} }\\tag{3}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {r= \\begin{cases} \\displaystyle 10, &amp; \\text {count(targets) = 0}\\\\ \\displaystyle 2, &amp; \\text {eliminates(or search) a tank}\\\\ \\displaystyle 1, &amp; \\text {eliminates(or search) an infantry}\\\\ \\displaystyle 0.001, &amp; d ({\\text {leader, follow}}) &lt; m \\end{cases} }\\tag{3}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula> is desired operating distance between leader and follower drone.</p><p>In Case7, we define <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$U_{\\text {capacity}}$\n</tex-math></inline-formula> as a variable indicating the remaining amount of ammunition of a unit. CSS CGFs obtain a reward when supplying ammunition to a unit, and the size of the reward is determined based on this variable, as shown in <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">Equation (4)</a>. According to the reward structure, CGFs gain a large reward when <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$U_{\\text {capacity}}$\n</tex-math></inline-formula> is small, so that they would serve ammo with consideration over the remaining ammunition of each unit. In other words, CGFs are expected to prioritize the allocation to units that lack the ammunition.<disp-formula id=\"deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {r= \\begin{cases} \\displaystyle 20, &amp; U_{\\text {capacity}} &lt; 20\\\\ \\displaystyle 10, &amp; 20 \\leq ~U_{\\text {capacity}} &lt; 40\\\\ \\displaystyle 1.5, &amp; 40 \\leq ~U_{\\text {capacity}} &lt; 60\\\\ \\displaystyle 0.5, &amp; 60 \\leq ~U_{\\text {capacity}} &lt; 80\\\\ \\displaystyle 0.1, &amp; 80 \\leq ~U_{\\text {capacity}}\\\\ \\displaystyle -1, &amp; \\text {out of area} \\end{cases} }\\tag{4}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {r= \\begin{cases} \\displaystyle 20, &amp; U_{\\text {capacity}} &lt; 20\\\\ \\displaystyle 10, &amp; 20 \\leq ~U_{\\text {capacity}} &lt; 40\\\\ \\displaystyle 1.5, &amp; 40 \\leq ~U_{\\text {capacity}} &lt; 60\\\\ \\displaystyle 0.5, &amp; 60 \\leq ~U_{\\text {capacity}} &lt; 80\\\\ \\displaystyle 0.1, &amp; 80 \\leq ~U_{\\text {capacity}}\\\\ \\displaystyle -1, &amp; \\text {out of area} \\end{cases} }\\tag{4}\\end{align*}\n</span></span></disp-formula> where a unit\u2019s ammo amount ranges from 0 to 100.</p></li></ol><div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nState Description for Each Case</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t2-3227797-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t2-3227797-small.gif\" alt=\"Table 2- &#10;State Description for Each Case\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nAction Description for Each Case</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t3-3227797-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t3-3227797-small.gif\" alt=\"Table 3- &#10;Action Description for Each Case\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3d\"><h3>D. Hyper-Parameters</h3><p>This section describes the hyperparameters shown in <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>. Since selecting hyperparameters affects the algorithm performance considerably, it is common to find the optimal hyperparameters through a sensitivity analysis <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_3d\">[37]</a>. However, since our study aims on applying the algorithm, rather than improving it, we used the hyperparameters value of other studies that are similar with ours.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nHyperparameters Description for Each Case</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t4-3227797-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho.t4-3227797-small.gif\" alt=\"Table 4- &#10;Hyperparameters Description for Each Case\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Hyperparameters consist of the following five elements. First, batch_size indicates the number of experiences (size of data) in each iteration of gradient descent search, while buffer_size denotes the number of experiences that should be collected before updating the policy model. Also, beta is a value for the randomness of the policy and increasing it will result in an increased number of random actions. Epsilon is a value that indicates how rapidly policy evolves. For instance, if epsilon is small, it means that the policy is stable, but the training process would be slow. Finally, lambda is the value showing how much of the previous reward was reflected when evaluating the future reward, and gamma is the discount rate for the future reward.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Results</h2></div><p>This section describes the experimental results of the 7 cases presented in <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>. Although we have presented only a few important graphs in analyzing the results in the main text, all graphs can be viewed in <a ref-type=\"app\" anchor=\"app1\" class=\"fulltext-link\">Appendix A</a>. Our experiment was performed in a computer environment with Apple M1 chip\u2019s 8-core CPU, 7-core GPU, and 8GB of RAM.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Case1 and Case2</h3><p>In Case1 and Case2, we trained infantry CGF\u2014Case1 was a 1:1 environment, and Case2 was a 2:2 environment. The detailed results are as follows.\n<ul style=\"list-style-type:disc\"><li><p>Case1: CGF was successfully trained in the Case1 environment, where CGF battles against an agent with CGF\u2019s previous policy, as shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1 (a)</a>. An interesting observation is that the reward value converges to a certain number, which is 0.3 in this case. It is able to understand the meaning of the number through knowing that Case1 is a zero-sum environment. Case1 is the combat situation with a clear winner and loser. Therefore, if the agent wins all battles, the reward will converge to 1; however, if the agent keeps losing, reward will approach to -1. 0.3 is a number derived when the agent wins 6.5 out of 10 times and loses 3.5 times; in other words, when the win rate is about 65%. Also, since the values show the clear convergence, it was concluded that 65% is the best policy found by the agent in the current experimental environment.</p></li><li><p>Case2: This experiment, which aimed to make CGFs learn how to cooperate, was successful; we observed the CGFs trying to occupy an advantageous position over the enemy in combat. To evaluate the effectiveness of the model learning cooperation, 3000 battle simulations were performed against the CGF trained in Case1. As a result, the agent in Case2 won 2160 times, which means that the win rate was 72%. From this number, we were able to estimate the benefits of the cooperation.</p></li></ul></p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Case3 and Case4</h3><p>In Cases3 and 4, artillery is newly joined as an agent to test the learning of agents with different classes or weapon systems. In Case3, we organized infantry CGF and artillery CGF into different teams, while in Case4 we set infantry CGF and artillery CGF in the same team. The results of the experiment are as follows.\n<ul style=\"list-style-type:disc\"><li><p>Case3: In Case3, a team of two infantry and a team of two artillery fought against each other. As the learning progressed, we could observe the pattern of artillery with a long weapon range fighting from a distance, while infantry tried to fight at a close distance. This is because infantry CGFs learned that it is the most profitable for them to approach the artillery CGF for a combat, while the artillery CGFs learned that the optimal policy for them is to fight while maintaining distances from the infantry CGFs. <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a> shows the dynamics of rewards during the learning. As Self-Play was applied in a zero-sum environment, the reward increased when the CGFs trained, while it decreased when they did not. For instance, the infantry team could not receive a reward at the beginning of the learning, because they did not train. However, after the learning began, the reward value started to increase. <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a> shows how the agents learn by applying Self-Play.</p><p>In Step1 of <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a>, the infantry team learns the policy, and the artillery team performs random actions as part of the environment. In Step2, the infantry team acts as part of the environment with policy #1 learned in Step1, and the artillery team begins learning the policy. CGF repeats this process until the simulation ends.</p></li><li><p>Case4: In this experiment, which aimed to confirm the combat behavior of CGFs when infantry and artillery CGFs are a team, CGFs learn cooperative battle by adopting the characteristics of weapons. Infantry CGF carried out close combat under the fire support of artillery CGF, which corresponded with the actual tactics of infantry on the battlefield. Also, the artillery CGF learned the behavior of fire support, which corresponds with the actual artillery\u2019s roles. Although this experiment only reflected the property of weapons in CGF, it has its meaning in showing that CGFs are able to implement doctrinal features.</p></li></ul>\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho2-3227797-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho2-3227797-small.gif\" alt=\"FIGURE 2. - Dynamic of reward for Case1.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Dynamic of reward for Case1.</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho3-3227797-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho3-3227797-small.gif\" alt=\"FIGURE 3. - Dynamic of reward for Case3 (Blueline indicates Artillery team, Redline indicates Infantry team).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Dynamic of reward for Case3 (Blueline indicates Artillery team, Redline indicates Infantry team).</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho4-3227797-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho4-3227797-small.gif\" alt=\"FIGURE 4. - Self-play operation procedure.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Self-play operation procedure.</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4c\"><h3>C. Case5</h3><p>In Case5, the agent had a main mission to arrive at the goal. In the process of conducting the mission, an accidental situation, that is the appearance of an enemy, occurred. Therefore, the agent learned how to deal with contingencies, so that it could complete the main mission. Therefore, what we expected to see from the result was that the agent has learned these behaviors by exploring under circumstances without any interference. However, as a result, the agent ended the episode without reaching the main goal, despite it successfully eliminated the enemy. We attributed that the problem was that the agent did not acquire rewards often, which is called \u201cthe sparse reward problem.\u201d</p><p>The sparse reward problem indicates the condition of an agent in reinforcement learning receiving only a sparse reward signal from the environment, so that it will be difficult for the agent to connect the signals with future rewards. This problem can reduce the learning speed or rate. In order to solve the sparse reward problem, the researcher should take steps that allow the agent to receive the reward better.</p><p>Although the representative method for this is Curriculum Learning, it was difficult for us to solve our problem using this method, because it makes CGF learn to arrive at the goal while solving sub-problems. However, if the CGF only learns to reach the target after the process of eliminating the enemy (left in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>), it loses the opportunity to learn how to get to the goal directly without eliminating enemies, that may be the better move than the former depending on the situation (As shown on the right in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>).\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho5-3227797-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho5-3227797-small.gif\" alt=\"FIGURE 5. - Dynamic of reward for Case5 after customized (Blue indicates the existing environment, red indicates the reconstructed environment).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Dynamic of reward for Case5 after customized (Blue indicates the existing environment, red indicates the reconstructed environment).</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho6ab-3227797-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho6ab-3227797-small.gif\" alt=\"FIGURE 6. - Differences between Curriculum Learning(Left) and Our Learning(Right).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Differences between Curriculum Learning(Left) and Our Learning(Right).</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div></p><p>From the point of view of the military, CGF must be able to decide whether to eliminate the enemy or not, which is the process that depends heavily on the distance between the goal and the enemy. Therefore, we reorganized the environment, so that CGF can first learn the important things and then learn additional situations.</p><p>In the new environment, Case5 is divided into 3 steps, and the environment of the next stage is called when when learning in each steps is completed. Sequentially, the procedure is as follows: in Step1, CGF learns to get to the goal; in Step2, it randomizes the position of the goal; and in Step3, it creates an enemy. Through this step-by-step action of adding or changing the environment, the agent was able to learn the new policy while maintaining the existing policy. <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a> shows the experimental results after reconfiguration of the environment. The red line in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a> shows the reward after the reconfiguration, and it shows the significant improvement compared to the previous learning which is shown through the blue line. Point (B) and (C) on the graph shows when Step2 and Step3 started respectively; therefore, it is revealed the reward decreased significantly at the start points and then it increases again.</p></div><div class=\"section_2\" id=\"sec4d\"><h3>D. Case6</h3><p>In this experiment, which aimed to eliminate all enemies through the collaboration between drones, the drones have learned the optimal policy. From Case6, we could describe three interesting observations. First, the follower drones learned to find the enemy while they maintained the communication distance with the leader drone. Second, the drones first identified the tank with the higher reward value. Third, drones made the decision of what suits the most for the team\u2019s interests between detecting and attacking actions. For instance, we could observe that when the number of all drones was reduced to two, the remaining two drones performed reconnaissance rather than high-risk attacks, to increase survivability.</p></div><div class=\"section_2\" id=\"sec4e\"><h3>E. Case7</h3><p>In Case7, CSS CGF aimed to supply ammunition to the units by considering the ammunition amount of the units that change in real time, in order that the amount for each will not get exhausted. At the beginning of the learning, CGF supplied ammunition only to nearby units. The reason was because CGF received the same amount of the reward by supplying ammunition to the units regardless of the distance. Hence, CGF focused on the behavior of supplying itself, rather than considering the supply amount of each unit. To solve this problem, we derived a new reward structure as shown in <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">Equation (4)</a> described in <a ref-type=\"sec\" anchor=\"sec3c\" class=\"fulltext-link\">Section III-C</a>, that made the amount of ammunition supplied to be more important.</p></div><div class=\"section_2\" id=\"sec4f\"><h3>F. Observations</h3><p>Through the experiment results, we have discovered several remarkable points that show us the possibility of the intelligence of CGF. Although some parts of the observations include the above, they were reinterpreted in consideration of military use in wargames.\n<ol><li><p>First, with a multi agent algorithm, CGF learned to cooperate for the team\u2019s victory. <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7(a)</a> shows CGFs that learned with a multi-agent algorithm (blue square) driving other CGFs that learned with a single agent algorithm (red circle) into a corner, so that the former could prevent the latter from escaping.</p></li><li><p>Second, CGF can learn combat skills without rules. In the close combat experiment, CGFs learned to engage in range combat by reflecting the weapon properties. In <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figures 7(b) and (c)</a>, CGFs performed a close combat and a Stand-Off battle by considering the range of weapons.</p></li><li><p>Third, CGFs learned that the most appropriate action for the team\u2019s victory is to first remove the opponent\u2019s high-value target. <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7(d)</a> shows team blue\u2019s winning strategy in an environment where infantry and artillery are on the same team. The infantry and the artillery increased the chance for winning by attacking the red team\u2019s artillery first.</p></li><li><p>Fourth, from a military perspective, it is necessary for CGF to learn under artificially intervened experimental environments. This is because it is not easy for CGF to learn in an environment that requires completing multiple tasks. Therefore, although there are other ways to solve this issue, it is most appropriate for CGF to be trained under the environment that changes step by step as suggested in Case5, when it comes to the military training of CGF.</p></li><li><p>Fifth, in the multi-drone environment of Case6, the follower drones searched for the targets while keeping operation distance from the leader drone, as shown in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7(e)</a>. Although the agents had not provided with the rule that the leader drone and the follower drones should maintain a certain distance, they automatically started to operate in this way, as we set maintaining distance as the condition of the reward. This is because the drones learned that this method allows them to maximize the search range while they could maintain the proper distance from each other.</p></li><li><p>Last, in the military support environment of Case7, CSS CGFs determined the priority of supply and started supplying ammunition first to the prioritized place. <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7(f)</a> is a diagram that shows how the CGF decided which unit should be prioritized by checking the ammo amount of the units. Although we had not provided CGF with a rule of how it should set supply priority, but it made a correct judgement by itself as a reaction to the reward.</p></li></ol>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho7abcdef-3227797-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho7abcdef-3227797-small.gif\" alt=\"FIGURE 7. - Summary of key experimental results.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Summary of key experimental results.</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho8-3227797-large.gif\" data-fig-id=\"fig8\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976053/cho8-3227797-small.gif\" alt=\"FIGURE 8. - Dynamic of results for all Cases.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p>Dynamic of results for all Cases.</p></fig></div><p class=\"links\"><a href=\"/document/9976053/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4g\"><h3>G. Findings</h3><p>In this section, we described our experimental findings on the automation potential of CGF and proposed several important findings.\n<ol><li><p>First, the following experimental results suggest the possibility of applying reinforcement learning CGF to wargame models. (a) CGFs acted to \u2019maneuver\u2019 in the enemy\u2019s direction and project \u2019firepower\u2019 to the enemy to win the combat. b) Drone CGFs implemented the \u2019intelligence\u2019 function and reconnoitered high value targets first. (c) CSS CGFs implemented the \u2019combat support\u2019 function. (d) Infantry CGFs and artillery CGFs performed cooperative battles with actions that are suitable to their weapon characteristics, which demonstrates the possibility of cooperation between various branches.</p></li><li><p>Second, the CGF implements doctrinal behavior without rules. In the close combat environment, CGF performed actions such as close combat, Stand-Off tactics, and high-value target identification, although we have defined only 4 types of actions and 3 types of rewards. This suggests the possibility in the future that various doctrinal behaviors of CGF can be implemented in more complex environments, with only action and reward definitions and without rules.</p></li><li><p>Third, implementing the cooperative behavior of CGF by setting a group reward is possible. CGFs in the experiment performed combat actions that benefit the group, through the multi-agent algorithm. As CGFs prioritized actions that benefit the team over ones that benefited the individuals\u2019 interests, there is a possibility for ground forces CGFs to engage in cooperative combat.</p></li><li><p>Fourth, we can expect that reinforcement learning will enable CGF to behave beyond human thinking. The combat actions of the ground forces are established as doctrines and operational plans which were made based on war history and battle simulations; in other words, they are from the human\u2019s intelligence. Also, in our study, we considered behaviors of CGF to be correct if they complied with the doctrine. However, in some cases, a behavior that differentiates from the doctrine might not be wrong. CGF is capable of performing actions that have not occurred in the human thought process through accumulating numerous experiences from the learning process. Therefore, we can even expect that in the future behaviors of CGFs which are learned through reinforcement learning will improve the doctrine.</p></li><li><p>Finally, this study has a value as basic research. Although our experiment was not done with the actual wargame model, and thus the research results must be confirmed before they are applied to the existing wargame model, we suggest the possibility of applying reinforcement learning for the automation of the ground force CGF, as we used a verified program, and experts in military theory participated in the experiment.</p></li></ol></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion</h2></div><p>In this study, we implemented the doctrinal behavior of CGF by applying reinforcement learning in various environments and confirmed the automation potential of CGF. In addition, we were able to learn CGF in an environment with multiple missions by presenting the learning method that can be applied when training agents militarily. CGFs implemented with reinforcement learning can act intelligently because they can make different decisions based on the behavior of the training unit. Accordingly, if we apply the learned CGF to the war game model, the training unit can experience a lot of unexpected situations, and it is expected that various tactics and strategies can be developed through this.</p></div>\n\n<div class=\"section\" id=\"app1\"><h2/><h1>Appendix A Additional Graph for Each Cases</h1><p>In this section, we provided additional figures derived from the experimental results for each case. In analyzing the experimental results, not only reward but also other elements of the experiment such as loss or episode length are important indicators. In Cases1 to 6, the times at which the episode ended gradually decreased as the reward values increased during the learnings, and the loss values were measured to be less than 1, indicating that the learning proceeded normally. However, in Case7, the episode length, and the loss value increased as well as the reward value. The reason for the increase in episode length is that in Case7\u2019s environment, the episode ends only when the CGF leaves the area or reaches the set simulation time. In other words, the increase in episode length means that the CGF has served its mission for a long time.</p></div>\n</div></div></response>\n"
}