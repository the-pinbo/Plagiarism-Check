{
    "abstract": "The high cost of acquiring annotated histological slides for breast specimens entails exploiting an ensemble of models appropriately trained on small datasets. Histological Image Classification ensembles strive to accurately detect abnormal tissues in the breast samples by determining the correlation between the predictions of its weak learners. Nonetheless, the state-of-the-art ensemble methods, ...",
    "articleNumber": "9978635",
    "articleTitle": "Ensemble Optimization for Invasive Ductal Carcinoma (IDC) Classification Using Differential Cartesian Genetic Programming",
    "authors": [
        {
            "preferredName": "Eid Alkhaldi",
            "normalizedName": "E. Alkhaldi",
            "firstName": "Eid",
            "lastName": "Alkhaldi",
            "searchablePreferredName": "Eid Alkhaldi"
        },
        {
            "preferredName": "Ezzatollah Salari",
            "normalizedName": "E. Salari",
            "firstName": "Ezzatollah",
            "lastName": "Salari",
            "searchablePreferredName": "Ezzatollah Salari"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3228176",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9978635/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Histopathology images carcinoma classification for breast specimens, stained with hematoxylin and eosin (H&amp;E), has been investigated broadly due to the significance of the problem, the shortage of annotated images, and the increased expense of hiring radiologists <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. Breast cancer is one of the most deadly types of cancer. Around four-fifths of the deaths induced by breast cancer are caused by Invasive Ductal Carcinoma (IDC) <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. IDC\u2019s diagnosis and prognosis demand examining Whole Slide Images (WSI) of breast biopsies stained with H&amp;E for visual features improvements. The WSI are high-resolution images of the sample, usually divided into smaller sub-images for training machine learning (ML) classifiers.</p><p>As a result of the importance of the breast cancer automatic detection, several Machine Learning techniques have been suggested in the histopathology image classification literature.</p><p>Machine Learning has been broadly employed to solve scientific computational problems, data science, and predictive modeling in various domains over the past decades <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>. Deep Learning has also been used in the area of medical image classification as well as countless applications such as brain-computer interface using Graph Convolutional Neural Networks GCNs-Net <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>.</p><p>The earliest methods relied on uniting several useful features to represent the image better. These features are handcrafted by observing how the negative IDC patches differ visually from the positive ones <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. Many of these approaches centered around the nuclei features via nuclei segmentation such as nuclei position, centroid, density, glands segmentation, Voronoi-based features, Gabor filter, and the HSI color space <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. Most of the handcrafted-based approaches depend on a composite of numerous characteristics mentioned earlier that extend the distinguishability of the categories. Cruz-Roa et al. <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a> presented one of the foremost Convolutional Neural Network (CNN) models to categorize the BHI samples with a CNN <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. CNN revealed an unprecedented advancement over the handcrafted feature approaches in histopathology image classification <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. Nevertheless, the IDC classification of histopathology images stays challenging as a result of the insufficiency of data and the increased need of CNNs for labeled samples. Accordingly, more evolved techniques are needed to fetch the most precise predictions, such as transfer learning, domain adaptation, and ensemble optimization <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>.</p><p>Equivalent to blending the handcrafted features, ensemble learning is concentrated on integrating a set of diverse classifiers into a more accurate and robust model. The concept is to reduce variance and biases in the classifiers leading to a more generalized model that would function better on unseen samples.</p><p>Ensemble optimization has been successfully applied into numerous applications such as anomaly detection, cyber security, image classification and a diverse set of applications <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>. Ensemble learning is one of the most successful machine learning practices <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>. Linking a group of complementary classifiers will generally result in a model that is at least more accurate than any of its components <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>. Complementary classifiers have to be both diverse and accurate enough for the resulting ensemble to be more robust.</p><p>While ensemble learning produces more accurate networks, training multiple classifiers is time-consuming <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>. To overcome this impediment, Huang et al. <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a> proposed saving snapshots of the model during training instead of training multiple heterogenous models. Huang et al. method reduces the training time significantly by generating a group of the same model with different weights. However, the problem with homogeneous classifiers is that they are more inclined to overfit the dataset\u2019s peculiarities and are deprived of the diversity of the models. Additional ensemble learning methods were presented in the literature, such as Bayesian Averaging, bagging, boosting, vertical voting, and horizontal stacking <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>.</p><p>Due to the successful implementations of Evolutionary Algorithms (EA) in optimizing stacked ensembles established in the literature, a more adaptable EA seems exceptionally advantageous <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>.</p><p>Biologically inspired methods have been verified to be robust and extensible. Numerous bio-inspired techniques have been extended to different variants that suit specific problems. For instance, PSO was improved to solve complex multi-objective optimization problems, which extended PSO to Multi-Objective Particle Swarm Optimization (MOPSO) algorithm by introducing the Space Expanding Strategy (SES) and mutation. The improved MOPSO was used to solve multiple optimization problems and was superior to three other commonly used multi-objective PSO methods <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a>. The Random Neighbor Elite Guided Differential Evolution (RNEGDE) algorithm is another example of Evolutionary Algorithms extensibility <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a>. RNEGDE produced optimal solutions compared to other state-of-the-art methods <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a>. Hence, we emphasize leveraging the existing differential Cartesian Genetic Programming Artificial Neural Network (dCGPANN) to reach a competitive solution to the ensemble optimization problem for histological image classification.</p><p>The domain of medical image classification has acquired immense emphasis from the research community in the past decades. Numerous methods have been suggested to solve the invasive ductal detection problem. One of the earlier methods to automate the detection of IDC in histological images was proposed by Cruz-Roa et al. <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. Cruz-Roa et al. compared the performance of two primary approaches for IDC classification which were a three-layer CNN and a variety of extracted morphological and additional visual features <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. The CNN surpassed the feature engineering strategy <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>.</p><p>Zhang et al. examined diverse techniques for large-scale medical classification <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>. Zhang et al. presented the Combined Deep and Handcrafted Visual Feature (CDHVF) algorithm to categorize medical images into 30 classes using the ImageCLEF 2016 dataset <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>. The procedure consists of four phases. The first phase is to extract deep features by fine-tuning three pre-trained classifiers <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>. The second phase is to compute multiple handcrafted descriptors, while the third phase is to apply the Principal Component Analysis (PCA) to downsize the feature space <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>. The last phase is to produce an ensemble that optimally blends the selected features to conclude the correct category <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>.</p><p>Harshvardhan et al. employed various ResNet, VGG, MobileNet, DenseNet, and LeNet-5 configurations, as well as an optimized CNN architecture to solve the problem of IDC detection <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a>. They reported that the best optimized CNN (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\text{C}_{\\text {best}}$\n</tex-math></inline-formula>) delivered the best sensitivity, and VGG16 had the most satisfactory performance on all other metrics <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a>.</p><p>Zhang et al. used a Multi-Scale Residual Convolutional Neural Network (MSRCNN) as a feature extractor and implemented an SVM model to classify the deep features <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>. They first tested with a different number of MSRCNN blocks to observe the improvement in the accuracy of the test data <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>. As the number of MSRCNNs increases, the accuracy rises until they reach five blocks, after which the accuracy declines by 5% <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>. They analogized the performance of four MSRCNN blocks with and without an SVM classifier <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>. They declared that using an SVM classifier slightly enhanced the accuracy and F1-score <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>.</p><p>This paper illustrates the utilization of an evolutionary algorithm to automate selecting the best Neural Network (NN) configuration with its corresponding weights during meta-training. The meta-training shrinks the hyper-parameters search space considerably, accelerating convergence during the comprehensive training stage. It furthermore displays an intuitive measure of heterogeneity and automatic optimization of the horizontally stacked prediction vector\u2019s wights.</p><p>The existing state-of-the-art transfer learning techniques for microscopy image classification lack the systematic detachment of the task-dependent layers from the transferrable ones, resulting in overfitting when training over a limited quantity of samples. Moreover, their ensembles have a high number of hyper-parameters, making it challenging and time-consuming to pick the optimal ones manually.</p><p>It was established in earlier research that ensemble learning is indispensable for improving the accuracy of the overall system. Ensemble learning aspires to enhance the performance of a meta-classifier which is composed of weak classifiers. The arrangement of the meta-classifier delivers more acceptable prediction accuracy than any of the individual models. Furthermore, it was also used for transient stability of power systems and demonstrated superior performance to other state-of-the-art methods in term of time efficiency, feature space reduction and accuracy <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a>.</p><p>This paper presents a novel approach of utilizing the Cartesian Genetic Programming Algorithm (CGP) for stacked ensemble optimization. Our pipeline consists of two primary parts. The first part deals with training Convolutional Neural Networks with different setups on the Breast Histopathology Imaging (BHI) dataset. In the second portion, we define a cartesian Neural Network with a number of the NN topology parameters and evolve them using the Differential Cartesian Genetic Programming. <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a> describes our proposed methods. Our approach outperforms previously published algorithms as demonstrated in <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">section IV</a>.</p><p>The remaining sections are arranged as follows: <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a> elaborates on the training of individual classifiers and the meta-training of the ensemble strategy. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a> illustrates in detail the experimental setup, the evaluation metrics, the benchmark dataset and the used computational resources. <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a> exhibits the experimentations and their results compared with diverse state-of-the-art published methodologies. <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a> concludes the paper with our findings, the constraints of our approach, and future research suggestions.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Proposed Method</h2></div><div class=\"section_2\" id=\"sec2a\"><h3>A. Overview</h3><p>This section comprehensively describe the differential Cartesian Genetic Programming Artificial Neural Network (dCGPANN) and how it was employed to acquire the optimum topology and utilize the gradients for error back-propagation to update the weights and biases. It also examines the implementation details of the individual classifier\u2019s training. Besides, it depicts the multiple stages of training the ensemble.</p><p>Evolutionary Algorithms, in general, are used to optimize the parameters of a particular system or to conceive a better architecture <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2a\">[18]</a>. The principal emphasis of the earlier proposed methods was to fine-tune the parameters of the ensemble strategy. In numerous algorithms, the numbers of nodes and the connections are fixed, while the ANN weights are evolved. Different algorithms only evolve the weights of the ANN using EA. Nonetheless, stochastic gradient descent is far more potent in correcting the weights in the course of training than any other technique.</p><p>For the ensemble to be more accurate than its composing classifiers, it must be diverse and accurate. Nevertheless, diverse and accurate classifiers do not always guarantee a better performing ensemble <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_2a\">[7]</a>. Assembling a diversity-accuracy balanced ensemble is not a straightforward process. Thus, a more advanced ensemble technique is needed. This chapter concentrates on deciding the most optimum architecture of the stacked ensemble as well as its weights and biases through the standard error back-propagation using the gradient information obtained by the dCGPANN algorithm <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2a\">[19]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2a\">[20]</a>, <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2a\">[21]</a>.</p><p>The proposed method is composed of two stages. The first stage is to fine-tune and train end-to-end multiple heterogeneous classifiers on the training datasets to ensure diversity amongst models. Then we select the best-performing models. The test dataset patches are then augmented to 5000 images. The classifiers\u2019 predictions of the 5000 images are used to train the stacked ensemble using dCGPANN. The CGP inputs are an <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$8\\times 1$\n</tex-math></inline-formula> vector of stacked prediction of the best performing models. The test dataset is used to evaluate the performance of our method against different cutting-edge algorithms.</p><p>The genes are then evolved using crossover and mutation operators based on predefined parameters as explained in II-B. The motive of this method is to overcome the limitation that the previous methods had by using the derivatives of the loss function. We believe that the reduction of software bloat and the representational capability of the dCGPANN can produce state-of-the-art competitive ensemble topologies.</p><p>One of the merits of our approach is its scalability. The accelerated technological advancements in GPUs and the increasing number of open medical datasets necessitate design scalability <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2a\">[22]</a>. The accuracy increases as the number of heterogeneous weak learners in an ensemble grow. The more data and computational resources become available, the more learners can be fine-tuned and, therefore, leading to larger CGP-based Neural Network ensemble. Due to the separation between the fine-tuning and the ensemble network, the learners can be trained synchronously using parallel data approaches. Also, the learning done in a particular CGP ensemble can be further extended easily by cascading it to another CGP ensemble network. The above fact ensures that as the computational resources or the available data for histological image classification increase, the training that was already done can be re-utilized and incorporated into even more extensive ensembles. Hence, our approach permits data-parallel training to utilize large-scale computational resources through distributed computing efficiently.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. The Differential Cartesian Genetic Programming</h3><p>Genetic Programming (GP) is one of the most successfully applied Evolutionary Algorithm in the optimization domain <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2b\">[11]</a>. It is exceptionally suitable for binary classification problems due to its syntax <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2b\">[11]</a>. One of the constraints that impedes the performance of GP is program bloat and duplicative calculation of the same node every time it is needed <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2b\">[23]</a>. Another drawback is their lack of ability to evolve topologies. Since its invention, researchers have developed numerous versions of GP. Several GP algorithms have been developed such as stacking, tree-based Genetic Programming, Grammatical Evolution, linear Genetic Programming and CGP.</p><p>The Cartesian Genetic Programming (CGP) was first formed to evolve circuit design in the late 90s <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2b\">[24]</a>, <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2b\">[25]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2b\">[26]</a>. The CGP technique offers more flexibility to realize better ensemble topologies. The most prominent advantage of CGP over other variants of Genetic Programming is its ability to express the solution candidates as a cyclic graphs rather than the tree-based variant in the standard GP. CGP is known to reduce redundant computations <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2b\">[25]</a>. Unlike standard tree-based GP, a solution can be represented in a Cartesian form, as shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> displays a randomly generated ensemble topology by the Cartesian Genetic Programming.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha1-3228176-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha1-3228176-small.gif\" alt=\"FIGURE 1. - A randomly generated ensemble topology by CGP.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>A randomly generated ensemble topology by CGP.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div></p><p>The capacity of the CGP to represent a candidate solutiona in two-dimensional gene was revolutionary. Comparable to electrical circuits for which the CGP was used to evolve, the evolution of an ANN was investigated. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a> portrays the standard CGP encoding of the ANN where Miller et al. used CGP to evolve the ANN architecture <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha2-3228176-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha2-3228176-small.gif\" alt=\"FIGURE 2. - The standard CGP representation of the Neural Network and the encoded gene [27].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>The standard CGP representation of the Neural Network and the encoded gene <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a> displays how the Neural Network is encoded in standard CGP. Each node represents a non-linear activation function randomly selected from a predefined list <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>. The Cartesian network grid with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r \\times c$\n</tex-math></inline-formula> nodes has connections that link the nodes according to the set arity <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula>, which is the number of inputs to each node <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>. The first column from the left of the NN is the input layer, which consists of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> number of features. The last layer is the output layer. Where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r$\n</tex-math></inline-formula> is the number of the rows, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula> is the number of the columns, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the number of the features, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula> is the number of the classes.</p><p>The inputs of the Neural Network is the output predictions of the four models that compose the ensemble. The output of the Neural Network is the final output probabilities of the whole ensemble. It is known in the literature that the weights are better updated using the gradient descent methods which established its supremacy over the past decades <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2b\">[19]</a>. Another deficiency of the original use of CGP is the absence of biases which is vital to the learning process. Izzo et al. <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2b\">[19]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>, <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2b\">[21]</a> established a remarkably innovative idea that permits the evolution of the topology with CGP and updating the weights and biases using Stochastic Gradient Descent at the same time <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>. The concept is to split the gene into two parts. The first portion encodes the neural network nodes, connections, and activation functions. The second portion of the gene encodes the weights and the biases of the ANN connections <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>. <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a> pictures how the modified CGP incorporated the weights and biases.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha3-3228176-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha3-3228176-small.gif\" alt=\"FIGURE 3. - The dCGP representation of the Neural Network and the additions of weights and biases to the encoded gene [20].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>The dCGP representation of the Neural Network and the additions of weights and biases to the encoded gene <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div></p><p>In the dCGPANN variant, a gene <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula> representing a candidate solution consists of two portions. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{I}$\n</tex-math></inline-formula> contains the evolutionary part of the gene which is evolved by the CGP, while <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{R}$\n</tex-math></inline-formula> contains the weights and biases of the ANN as indicated in <a ref-type=\"disp-formula\" anchor=\"deqn1-deqn2\" href=\"#deqn1-deqn2\" class=\"fulltext-link\">equation 1</a> and <a ref-type=\"disp-formula\" anchor=\"deqn1-deqn2\" href=\"#deqn1-deqn2\" class=\"fulltext-link\">2</a> respectively <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>.<disp-formula id=\"deqn1-deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} X_{I}=&amp;[F_{0},C_{0,0},C_{0,1},\\ldots,C_{0,a},F_{1},C_{1,0}, \\\\&amp;\\quad \\ldots,C_{1,a},\\ldots, O_{1},\\ldots,O_{m}] \\tag{1}\\\\ X_{R}=&amp;[b_{0},w_{0,0},\\!w_{0,1},\\ldots,w_{0,a_{0}},b_{1},\\!w_{1,0},\\ldots,w_{1,a_{1}},\\ldots]\\tag{2}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} X_{I}=&amp;[F_{0},C_{0,0},C_{0,1},\\ldots,C_{0,a},F_{1},C_{1,0}, \\\\&amp;\\quad \\ldots,C_{1,a},\\ldots, O_{1},\\ldots,O_{m}] \\tag{1}\\\\ X_{R}=&amp;[b_{0},w_{0,0},\\!w_{0,1},\\ldots,w_{0,a_{0}},b_{1},\\!w_{1,0},\\ldots,w_{1,a_{1}},\\ldots]\\tag{2}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{I}~\\in $\n</tex-math></inline-formula> natural number is a vector that encodes the evolutionary part of the ANN, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{R}~\\in $\n</tex-math></inline-formula> real numbers is a vector for the biases <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b$\n</tex-math></inline-formula> and weights <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$w$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$F$\n</tex-math></inline-formula> represents functions, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> represent the connections and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O$\n</tex-math></inline-formula> represent the terminal output nodes.</p><p>Based on the dCGPANN, the output of each node in the ANN is expressed in <a ref-type=\"disp-formula\" anchor=\"deqn3\" href=\"#deqn3\" class=\"fulltext-link\">equation 3</a> <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2b\">[20]</a>.<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} N_{i} = F_{i} \\left({\\sum _{j=0}^{a_{i}} w_{i,j} C_{N_{i,j}} + b_{i} }\\right)\\tag{3}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} N_{i} = F_{i} \\left({\\sum _{j=0}^{a_{i}} w_{i,j} C_{N_{i,j}} + b_{i} }\\right)\\tag{3}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N_{i}$\n</tex-math></inline-formula> is the output of the node <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> with id <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$F_{i}$\n</tex-math></inline-formula> is the activation function, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{i}$\n</tex-math></inline-formula> is the arity of the node <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> which is the number of connections to that node, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C_{i,j}$\n</tex-math></inline-formula> is the connection between note <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> in the current layer and node <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> in the previous layer and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b_{i}$\n</tex-math></inline-formula> is the bias associated with the activation function <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$F_{i}$\n</tex-math></inline-formula> in node <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N_{i}$\n</tex-math></inline-formula>. The number of connections to each node, arity, is assumed equal by default in all nodes unless it gets defined by a list that specifies the number of connections each node should have in a particular layer. The arity list is a {1x<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula>} vector that assigns the arity of the nodes in each column <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula>.</p><p>The evolutionary operators <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu _{\\text {c}}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu _{\\text {a}}$\n</tex-math></inline-formula> are applied on the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{I}$\n</tex-math></inline-formula> part of the gene that expresses the dCGPANN. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu _{\\text {c}}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu _{\\text {a}}$\n</tex-math></inline-formula> are fractions to be mutated in active connections genes and active function genes respectively. Each mutant goes through a predefined number of stochastic gradient descent (SGD) training epochs during which only <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{R}$\n</tex-math></inline-formula> is learned, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{I}$\n</tex-math></inline-formula> is fixed. <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Algorithm 1</a> illustrates the steps of how the dCGPANN was operated to optimize the structure of the ensemble as well as its weights and biases <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\theta $\n</tex-math></inline-formula>. The loss function which the dCGPANN is minimizing is the categorical cross-entropy which is defined in <a ref-type=\"disp-formula\" anchor=\"deqn4\" href=\"#deqn4\" class=\"fulltext-link\">equation 4</a> <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2b\">[28]</a>.<disp-formula id=\"deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} Loss = - \\sum _{i=1}^{N} \\sum _{j=1}^{C} y_{i,j} \\times \\log \\hat {y_{i,j}}\\tag{4}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} Loss = - \\sum _{i=1}^{N} \\sum _{j=1}^{C} y_{i,j} \\times \\log \\hat {y_{i,j}}\\tag{4}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> is the number of the input images, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> is the number of classes, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{i,j}$\n</tex-math></inline-formula> is the true label of image <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> belongs to class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> which is 1 if <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{i}~\\in ~j$\n</tex-math></inline-formula> and 0 otherwise and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {y_{i,j}}$\n</tex-math></inline-formula> is the output probability that image <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i~\\in $\n</tex-math></inline-formula> class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>.<div class=\"algorithm\" id=\"alg1\"><h3>Algorithm 1 dCGPANN</h3><p><b>Require:</b> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Kernels$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$epochs$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$cycles$\n</tex-math></inline-formula></p><p>Randomly Initialize N dCGPANNs</p><p><b>for</b> dCGPANN in population N <b>do</b></p><p>Compute the Loss</p><p><b>end for</b></p><p><b>for</b> j in cycles <b>do</b></p><p>Select the best dCGPANN of the previous generation</p><p>Delete other dCGPANNs</p><p><b>for</b> i in population N <b>do</b></p><p>Mutate the best dCGPANN\u2019s functions using <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu _{a}$\n</tex-math></inline-formula></p><p>Mutate the best dCGPANN\u2019s connections using <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mu _{c}$\n</tex-math></inline-formula></p><p><b>for</b> epoch in epochs <b>do</b></p><p>Run SGD on <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathrm {dCGPANN}}_{i}$\n</tex-math></inline-formula></p><p><b>end for</b></p><p><b>end for</b></p><p><b>end for</b></p></div></p></div><div class=\"section_2\" id=\"sec2c\"><h3>C. Implementation Details</h3><p>This section outlines the implementation details and the handpicked classifiers\u2019 and the dCGPANN\u2019s learning hyper-parameters. <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>, <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">2</a>, <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">3</a> and <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">4</a> tabulate the classifiers\u2019 hyper-parameters used for the first phase of our approach. <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a> lists the the dCGPANN hyper-parameters used for the ensemble optimization in the second phase of the proposed method. The first stage of the training, multiple classifiers with different configurations were trained on the IDC dataset. Amongst the trained classifiers, the best four performing models were chosen to construct the ensemble. The first model was a Resnet50 pretrained on ImagNet which was trained using Transfer Learning <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2c\">[29]</a>, <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2c\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2c\">[31]</a>. We froze all layers except the last 69 layer which included the last new fully connected layer that was adopted to the new binary labels. The learning rate (LR) was high due to the fact that most of the domain-specific layers were frozen. The second model was also a Resnet50 but was trained end-to-end. The third and the fourth models were VGG19 and Densenet121 respectively whose training parameters are shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a> and <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a> <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2c\">[32]</a>.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nHyper-Parameters for Classifier 1</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t1-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t1-3228176-small.gif\" alt=\"Table 1- &#10;Hyper-Parameters for Classifier 1\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nHyper-Parameters for Classifier 2</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t2-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t2-3228176-small.gif\" alt=\"Table 2- &#10;Hyper-Parameters for Classifier 2\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nHyper-Parameters for Classifier 3</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t3-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t3-3228176-small.gif\" alt=\"Table 3- &#10;Hyper-Parameters for Classifier 3\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nHyper-Parameters for Classifier 4</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t4-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t4-3228176-small.gif\" alt=\"Table 4- &#10;Hyper-Parameters for Classifier 4\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\ndCGP Parameters</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t5-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t5-3228176-small.gif\" alt=\"Table 5- &#10;dCGP Parameters\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>The possible kernels used for the dCGPANN are the Sig, ReLu, tanh, ELU and ISRU functions which are defined by <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn9\" href=\"#deqn5-deqn9\" class=\"fulltext-link\">equations 5</a>, <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn9\" href=\"#deqn5-deqn9\" class=\"fulltext-link\">6</a>, <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn9\" href=\"#deqn5-deqn9\" class=\"fulltext-link\">7</a>, <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn9\" href=\"#deqn5-deqn9\" class=\"fulltext-link\">8</a> and <a ref-type=\"disp-formula\" anchor=\"deqn5-deqn9\" href=\"#deqn5-deqn9\" class=\"fulltext-link\">9</a> respectively <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2c\">[36]</a>, <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2c\">[37]</a>.<disp-formula id=\"deqn5-deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Sig(x)=&amp;\\frac {1}{1+\\exp (-x)} \\tag{5}\\\\ ReLu(x)=&amp;max(0,x) = \\begin{cases} x_{i} &amp; if~x_{i}\\geq 0 \\\\ 0 &amp; if~x_{i} &lt; 0 \\end{cases} \\tag{6}\\\\ tanh(x)=&amp;\\frac {\\exp (x)-\\exp (-x)}{\\exp (x)+\\exp (-x)} \\tag{7}\\\\ ELU(x)=&amp;\\begin{cases} x &amp; if~x &gt; 0 \\\\ \\alpha \\exp (x) -1 &amp; if~x_{i} \\leq 0 \\end{cases} \\tag{8}\\\\ ISRU(x)=&amp;x \\frac {1}{\\sqrt {(}1 + \\alpha x^{2})}\\tag{9}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Sig(x)=&amp;\\frac {1}{1+\\exp (-x)} \\tag{5}\\\\ ReLu(x)=&amp;max(0,x) = \\begin{cases} x_{i} &amp; if~x_{i}\\geq 0 \\\\ 0 &amp; if~x_{i} &lt; 0 \\end{cases} \\tag{6}\\\\ tanh(x)=&amp;\\frac {\\exp (x)-\\exp (-x)}{\\exp (x)+\\exp (-x)} \\tag{7}\\\\ ELU(x)=&amp;\\begin{cases} x &amp; if~x &gt; 0 \\\\ \\alpha \\exp (x) -1 &amp; if~x_{i} \\leq 0 \\end{cases} \\tag{8}\\\\ ISRU(x)=&amp;x \\frac {1}{\\sqrt {(}1 + \\alpha x^{2})}\\tag{9}\\end{align*}\n</span></span></disp-formula></p><p>Setting up the parameters for the dCGPANN is a challenging task and demands a trial-and-error approach. Nonetheless, the literature on Cartesian Genetic Programming provides a general direction for parameter selection. CGP Research has shown that setting the mutation to 1% for each 100 nodes is recommended <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2c\">[27]</a>. However, setting the mutation to half of the recommended value during experimentation has produced more competitive topologies. Multiple experiments have also shown that setting <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula> to 20 and 5 provided the best designs for this particular ensemble. The number of iterations and epochs were set to 100 and 150 to limit the time required to train the models and to avoid over-training the ensemble, which might result in overfitting.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Experimental Setup</h2></div><div class=\"section_2\" id=\"sec3a\"><h3>A. Dataset and the Experimental Setup</h3><p>The dataset contains patches extracted from the whole slides of Two hundred seventy-nine patients were diagnosed with IDC. The total number of the extracted non-overlapping of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$50\\times 50$\n</tex-math></inline-formula> pixels patches is 277524, which are labeled into IDC and non-IDC regions. The number of patches and the percentage of IDC annotations per patient vary significantly; thus, the labels of the images are not equal. <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig 4</a> shows the patches and IDC annotation percentage histogram. Some visual features distinguish IDC from non-IDC patches, such as tissue coloration as shown in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a> and <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>, which shows negative and positive IDC, respectively. <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7</a> shows the color maps of annotated whole slides where the yellow regions indicate the presence of IDC, and <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Figure 8</a> shows a whole slide that was reconstructed using the coordinates information provided by the dataset. The darker mask points to the IDC regions on the whole slide image (WSI) <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>. Data visualization of the training image set is shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figures 4</a>, <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">5</a>, <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">6</a>, <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">7</a> and <a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">8</a> <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha4-3228176-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha4-3228176-small.gif\" alt=\"FIGURE 4. - Dataset Statistics [38].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Dataset Statistics <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha5-3228176-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha5-3228176-small.gif\" alt=\"FIGURE 5. - IDC negative samples [38].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>IDC negative samples <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha6-3228176-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha6-3228176-small.gif\" alt=\"FIGURE 6. - IDC positive samples [38].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>IDC positive samples <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha7-3228176-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha7-3228176-small.gif\" alt=\"FIGURE 7. - IDC Colormap in a WSI [38].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>IDC Colormap in a WSI <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha8-3228176-large.gif\" data-fig-id=\"fig8\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha8-3228176-small.gif\" alt=\"FIGURE 8. - Reconstructed Annotated WSI [38].\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 8. </b><fig><p>Reconstructed Annotated WSI <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_3a\">[38]</a>.</p></fig></div><p class=\"links\"><a href=\"/document/9978635/all-figures\" class=\"all\">Show All</a></p></div></p><p>The dataset was divided into three sets. 70% of the patients\u2019 slides were used as a training dataset, and the remaining patients\u2019 slides were divided into a validation dataset and a test dataset with 15% of the slides each. Our model and the base models to which our method was compared to were implemented using PyTorch, dcgpy and scikit-learn <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_3a\">[21]</a>, <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_3a\">[39]</a>, <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_3a\">[40]</a>. Our method was compared to two voting strategies which are the maximum and the weighted average.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Evaluation Metrics</h3><p>The evaluation metrics used for our assessment are the F1-score, the Balanced Accuracy and the overall accuracy as defined in <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">equations 10</a>, <a ref-type=\"disp-formula\" anchor=\"deqn14-deqn15\" href=\"#deqn14-deqn15\" class=\"fulltext-link\">15</a> and <a ref-type=\"disp-formula\" anchor=\"deqn12-deqn13\" href=\"#deqn12-deqn13\" class=\"fulltext-link\">13</a> respectively <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_3b\">[41]</a>, <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_3b\">[42]</a>.<disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} F_{1} = \\frac {2}{\\frac {1}{Recall} + \\frac {1}{Precision}} = \\frac {2 Precision \\times Recall}{Precision+Recall}\\tag{10}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} F_{1} = \\frac {2}{\\frac {1}{Recall} + \\frac {1}{Precision}} = \\frac {2 Precision \\times Recall}{Precision+Recall}\\tag{10}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Recall$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Precesion$\n</tex-math></inline-formula> are defined in 11 and 12 <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_3b\">[41]</a>.<disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} Recall = Sensitivity = \\frac {TP}{TP+ FP}\\tag{11}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} Recall = Sensitivity = \\frac {TP}{TP+ FP}\\tag{11}\\end{equation*}\n</span></span></disp-formula> Note that <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$TP$\n</tex-math></inline-formula> is number of correctly positively classified samples and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$FP$\n</tex-math></inline-formula> is the incorrectly positively classified ones.<disp-formula id=\"deqn12-deqn13\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Precision=&amp;\\frac {TP}{TP+ FN} \\tag{12}\\\\ BAC=&amp;\\frac {Sensitivity + Specificity}{2}\\tag{13}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Precision=&amp;\\frac {TP}{TP+ FN} \\tag{12}\\\\ BAC=&amp;\\frac {Sensitivity + Specificity}{2}\\tag{13}\\end{align*}\n</span></span></disp-formula> where Specivity is defined by <a ref-type=\"disp-formula\" anchor=\"deqn14-deqn15\" href=\"#deqn14-deqn15\" class=\"fulltext-link\">equation 14</a>.<disp-formula id=\"deqn14-deqn15\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Specivity=&amp;\\frac {TN}{TN+FP} \\tag{14}\\\\ Accuracy=&amp;\\frac {TP+TN}{TP+TN+FP+FN}\\tag{15}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Specivity=&amp;\\frac {TN}{TN+FP} \\tag{14}\\\\ Accuracy=&amp;\\frac {TP+TN}{TP+TN+FP+FN}\\tag{15}\\end{align*}\n</span></span></disp-formula>\n<ul style=\"list-style-type:disc\"><li><p>Macro-average: is the sum of each class\u2019 precision divided by the number of classes.</p></li><li><p>Micro-average: is the sum of the true positives of all classes divided by the number of samples.</p></li></ul></p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Results</h2></div><p>This section shows the results of our experiments. The performance of phase 1 trained classifiers as well as the base models on the validation set are shown in <a ref-type=\"table\" anchor=\"table6\" class=\"fulltext-link\">Table 6</a>.<div class=\"figure figure-full table\" id=\"table6\"><div class=\"figcaption\"><b class=\"title\">TABLE 6 </b>\nConfusion Matrices of Various Models on the Validation Set</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t6-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t6-3228176-small.gif\" alt=\"Table 6- &#10;Confusion Matrices of Various Models on the Validation Set\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Our experiments were conducted using an AMD Ryzen Threadripper 1950X 16-Core Processor, which operates on two threads per core and 32 Numa nodes, and an NVIDIA GeForce RTX 2080 with 8GB of video RAM. The maximum and average voting ensembles didn\u2019t demand training. The evolution of the DGP network was significantly time-efficient compared to the time cost of training the weak classifiers. The training time of each architecture was 1.2450 seconds. Each iteration required 8.71533 seconds and the overall training of the ensemble took 880.24 seconds.</p><p>Our technique was compared to the average voting and the Maximum voting schemes. The outcomes on the held-out test dataset are reported in <a ref-type=\"table\" anchor=\"table7\" class=\"fulltext-link\">Table 7</a>, <a ref-type=\"table\" anchor=\"table8\" class=\"fulltext-link\">8</a>, and <a ref-type=\"table\" anchor=\"table9\" class=\"fulltext-link\">9</a>. The results demonstrate a clear advantage of the dCGPANN ensemble over all other voting strategies, as shown in <a ref-type=\"table\" anchor=\"table9\" class=\"fulltext-link\">Table 9</a>.<div class=\"figure figure-full table\" id=\"table7\"><div class=\"figcaption\"><b class=\"title\">TABLE 7 </b>\nPerformance of the Weighted Voting on the Test Dataset</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t7-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t7-3228176-small.gif\" alt=\"Table 7- &#10;Performance of the Weighted Voting on the Test Dataset\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table8\"><div class=\"figcaption\"><b class=\"title\">TABLE 8 </b>\nPerformance of the Maximum Voting on the Test Dataset</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t8-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t8-3228176-small.gif\" alt=\"Table 8- &#10;Performance of the Maximum Voting on the Test Dataset\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table9\"><div class=\"figcaption\"><b class=\"title\">TABLE 9 </b>\nPerformance of dCGPANN Voting on the Test Dataset</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t9-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t9-3228176-small.gif\" alt=\"Table 9- &#10;Performance of dCGPANN Voting on the Test Dataset\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>To the best of our knowledge, the dCGPANN approach has never been implemented to optimize ensemble ANN for histology image classification. The findings of our experiments were compared with the state-of-the-art methods that were proposed in <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_4\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_4\">[15]</a>, and <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_4\">[16]</a> in <a ref-type=\"table\" anchor=\"table10\" class=\"fulltext-link\">Table 10</a>. The number of test samples used to evaluate our proposed method was 38183, which resembles the extracted <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$50\\times 50$\n</tex-math></inline-formula> patches from the slides of 42 randomly selected patients. The number of IDC test samples was 13434, while the number of No IDC test samples was 24749. As explained earlier, the classes of the dataset are unbalanced. There are far more negative patches than positive ones. <a ref-type=\"table\" anchor=\"table10\" class=\"fulltext-link\">Table 10</a> demonstrate the significance of the improvement of our method compared to other competitive models reported in the literature. Our Method yielded superior results in accuracy, balanced accuracy and F1 score by a significant margin. The accuracy of our technique exceeded the best performing method by 5% percent. More importantly, the BAC and F1-score improved by 4.3% and 5.6% respectively. Moreover, the ensemble yielded much higher improvement in the confusion matrix to the weak classifiers as shown in <a ref-type=\"table\" anchor=\"table6\" class=\"fulltext-link\">Table 6</a> and <a ref-type=\"table\" anchor=\"table9\" class=\"fulltext-link\">9</a>. <a ref-type=\"table\" anchor=\"table10\" class=\"fulltext-link\">Tables 10</a> and <a ref-type=\"table\" anchor=\"table11\" class=\"fulltext-link\">11</a> provide a summary of the performance of our proposed method compared to the state-of-the-art methods in terms of the most reported evaluation metrics in this field.<div class=\"figure figure-full table\" id=\"table10\"><div class=\"figcaption\"><b class=\"title\">TABLE 10 </b>\nComparison of the Proposed Method With Other Published Methods in the Standard Evaluation Metrics</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t10-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t10-3228176-small.gif\" alt=\"Table 10- &#10;Comparison of the Proposed Method With Other Published Methods in the Standard Evaluation Metrics\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table11\"><div class=\"figcaption\"><b class=\"title\">TABLE 11 </b>\nComparison of the Proposed Method With Other Published Methods in Recall Precesion, and Specificity</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t11-3228176-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978635/alkha.t11-3228176-small.gif\" alt=\"Table 11- &#10;Comparison of the Proposed Method With Other Published Methods in Recall Precesion, and Specificity\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion</h2></div><p>The accuracy of the Deep Learning models is a cornerstone to the CAD systems. It is evident that the advancements of Deep Learning and GPUs enabled computers to perform better or at least equal to human experts. Thus, computerized cancer detection can at the very least increase the speed of diagnosis. Our proposed method addresses the scarcity of labeled data by implementing a bio-inspired algorithm to construct a more reliable ensemble. The ensemble learns the correct class based on the misclassifications of the pre-trained models. The differential Cartesian Genetic Programming was used to acquire the most optimum ensemble rule to compensate for the insufficient quantity of images available for multi-stage training, which is required to infer accurate mapping between intricate features and correct classes. Due to its cabability to assign weights and biases to ANNs and to use SGD for learning them, the dCGPANN proves to be a powerful tool to optimize the ensemble topology as well as its hyperparameters.</p><p>We demonstrated the accuracy and time efficiency of our proposed method to classify invasive ductal carcinoma using the breast histology image dataset. A statistical evaluation of our results was also provided on the benchmark dataset to further demonstrate the applicability of our methods. The experimental findings exceeded previous hyperparameters search methods. The evidence from this study implies that dCGPANN ensemble produce better results than any individual weak classifier as well as other recently published ensemble methods.</p><p>Based on the results reported, we recommend future research to be focused on the application of CGP on optimizing CNNs. The viability of designing GCP-based ensemble networks using Convolutional kernels that combine features extracted by fine-tuned pre-trained learners should be investigated.</p></div>\n<h3>ACKNOWLEDGMENT</h3><p>The authors highly appreciate the time and effort devoted by the reviewers to review their study. They are deeply indebted for their comprehensive review\u2019s insights and different perspectives.</p></div></div></response>\n"
}