{
    "abstract": "In the present, we can use more than 200 different methods and algorithms for automatic recognition systems, the basis of which is the recognition of a face in an image, subsequent extraction of areas of interest and classification of individual parameters using neural networks or other classifications. Currently, one of the most discussed research topics connecting the fields of psychology and ar...",
    "articleNumber": "9975322",
    "articleTitle": "Investigation of Behavioral Characteristic Parameters Intended for Emotional State Classification",
    "authors": [
        {
            "preferredName": "M. Magdin",
            "normalizedName": "M. Magdin",
            "firstName": "M.",
            "lastName": "Magdin",
            "searchablePreferredName": "M. Magdin"
        },
        {
            "preferredName": "J. Kapusta",
            "normalizedName": "J. Kapusta",
            "firstName": "J.",
            "lastName": "Kapusta",
            "searchablePreferredName": "J. Kapusta"
        },
        {
            "preferredName": "\u0160. Koprda",
            "normalizedName": "\u0160. Koprda",
            "firstName": "\u0160.",
            "lastName": "Koprda",
            "searchablePreferredName": "\u0160. Koprda"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227627",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9975322/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Emotions are a reflection of a person\u2019s mental state. They are a manifestation of our behaviour in a specific life situation. From the point of view of psychology, various categorization models are used in classifying (determining) an emotional state. One of the oldest is Darwin\u2019s method of classification by describing and classifying groups of terms into individual categories by similarities. According to Darwin, expressions can be defined by deformations - by changing the expression on the face, for example, during communication with another person, when we effectively express our emotions, opinions and intentions. Darwin\u2019s categorization model includes more than 40 different emotional states <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. Currently, one of the most widely used methods of classification is the model described by Ekman, which consists of six basic discrete emotions: anger, fear, sadness, surprise, disgust, and happiness <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>. Other commonly used models include Plutchik\u2019s Wheel of Emotions <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>. The Ekman\u2019s classification is one of the most common ways to classify an emotional state. However, the problem with Ekman\u2019s classification is the fact that it can only identify those emotional states that are directly visible from the human face. It has been criticized several times for this approach. Other most preferred approach to the classification of emotional states is the application of Russell\u2019s Circumplex Model <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>. It is an alternative description of human emotions, where individual states are represented by a part of a circle in a two-dimensional bipolar space (pleasant - unpleasant, calm - active). Russell\u2019s model works with so-called valence and arousal. In the Circumplex Russel model, valence represents a value from the interval [\u22121,1], where individual values from this interval are understood as positive or negative emotions. Together with arousal, which also reaches values from the interval [\u22121,1], these emotions can be categorized as emotions representing fear, anger, but also, for example, a state where a person can feel comfortable. According to this model, the emotion of anger (which is found in Ekman\u2019s classification) can be perceived as a combination of the influence of extreme resentment and medium-high arousal. Emotions can be mapped to a 2D space, which is defined by arousal and valence. The core of the space contains the most important influences on the basis of which individual emotions are evoked. Ekman\u2019s model is strict and is mostly used for verbal expression, i.e., for classifying the emotional state from the face, based on the extracted areas of interest. The advantage of Russell\u2019s model is the possibility of classifying also other emotional states as in Ekman\u2019s model. In Russell\u2019s model, all emotional states from the Ekman classification are present, but at the same time Russell\u2019s model provides us with an unconventional view of how to identify such emotions that are hidden (not directly visible in the face) and last too short (Russel states that this is called affect). We have incorporated this fact also into our paper.</p><p>The classification of emotional state using behavioural characteristics is currently one of the new research areas of psychology and artificial intelligence. From the point of view of artificial intelligence, it is possible to create an automatic recognition system with a certain success of classification, which, however, is strongly dependent on the previous stages of the recognition process: detection and extraction. In order to implement such a system, various sensors are used: camera (for face detection), GSR, ECG or EEG sensors to obtain data from human physiological manifestations (heart rate, pressure, sweating, brain waves, temperature, etc.) <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>. The problem with such a method of obtaining data is primarily their invasive way of using it, which means that the user from whom we want to obtain data is directly aware of its application, which may affect the result of the classification. At the same time, this way of obtaining data can affect the mobility of the user (problem with a portable device for measuring physiological signals). If a camera is used, the detection capability is limited by the camera\u2019s capabilities (object distance, lighting, etc.).</p><div class=\"section_2\" id=\"sec1a\"><h3>A. Structure of the Paper</h3><p>In this paper we describe the application we designed and programmed, Emotnizer <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1a\">[7]</a>, which obtains the necessary data for classifying the emotional state of the users based on their behavioural characteristics when working with a computer, specifically a keyboard and mouse. This paper is a sequel to the article (The Possibilities of Classification of Emotional States Based on User Behavioural Characteristics) published in the IJIMAI <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1a\">[7]</a>. In the article we published in IJIMAI we aimed on the possibility of creating behavioural databases of the user. For this purpose, we used our self-made and proven application, Emotnizer. The algorithm of detecting and classifying emotions was described in the article completely <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1a\">[7]</a>. For this reason, in this article, we include only short explanation of the algorithm. Therefore, we would like to refer to our previous article to make all the facts clear for any reader and prevent any occurrences of plagiarism. We used a combination of the Ekman and Russell\u2019s models for the classification, because the emotions from the Ekman model can also be classified using Russell\u2019s model. The advantage of Russell\u2019s model is the determination of valence and arousal, which greatly simplifies the subsequent classification (it is also possible to determine the intensity of the emotional state). In the Related work section, we present research that focus on similar issues. Given that at present there is only a small amount of research work focused on behavioural characteristics, specifically in this area, we also present their connection to various other research works related primarily to the method of obtaining and processing data from the field of artificial intelligence. In the Material and Methods section, we present the method of creating a reference database. Based on the reference database, we achieved a classification success of only max. 65% in this phase of the experiment. To identify and remove problematic parameters, which were stored in the form of data in the reference database and reduced the success of the classification, we applied the method of decision trees. In the Results section, we present the results obtained using the decision trees method. By applying this method, we determined the problematic places of the reference database and we have revealed which emotional states we can successfully classify. The goal of the paper is to find out whether it is possible to correctly classify the user\u2019s emotional state on the basis of the examined behavioural characteristics of the user. The classifier is designed to work with the dataset created by us. In addition to the classification, we find out which characteristics correlate the most with the emotional state of the user.</p></div></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Related Work</h2></div><p>From a psychological point of view, our ability to distinguish between simple facial expressions (determining emotional state) develops from infancy to adolescence <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2\">[8]</a>. Recognition of facial expressions and classification of emotional state is still a current area of research in artificial intelligence and computer vision. The goal of the algorithms of a successful recognition process is to detect the relevant face in the image, to extract discriminatory elements of interest from the face and subsequently to classify the emotional state. We used the Ekman classification in this case <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_2\">[2]</a>. According to Ekman\u2019s theory, any expression is the result of a deviation from the neutral state. In the field of computer vision research, different methods are used for each phase of the recognition process. Currently, the most common method for the detection phase is the use of the Viola-Jones algorithm <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\">[14]</a>. This algorithm can also be used in the next phase (extraction of areas of interest). However, it is not always possible to use this algorithm, therefore other methods are used, such as Principal Component Analysis - PCA <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\">[14]</a>, Gabor wavelets <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_2\">[15]</a>, <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2\">[16]</a> or a combination of Gabor wavelets with LBP <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2\">[17]</a>. Kumar and Bhuyan <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2\">[18]</a> also used the PCA method to project an expressive image into a neutral subspace, thus dividing the image into two components, neutral and expressive. The proposed method extracts properties from both components, whereby the extracted characters are divided into several blocks and subsequently the size of each block is reduced by multiple discriminant analysis (MDA). The reduced function is used to train the Support Vector Machine (SVM) classifier.</p><p>According to Li et al. <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2\">[16]</a>, the main problem in classifying the emotional state from the face using a camera arises in the first phase of the recognition process - detection. The detection is mainly influenced by the low resolution of the camera. Lighting and noise are also important factors that affect the performance of the face recognition system. To eliminate these problems the Gabor wavelets are used. Aided with these, it is possible to achieve the required performance even in non-standard lighting conditions and ambient noise. They proposed three methods based on the probabilistic model CF-GW (Copula Function of Gabor Wavelets), LCM-GW (Lightweight Copula Model Gabor Wavelets) and LCM-GW-PSO (Lightweight Copula Model Gabor Wavelets with Particle Swarm Optimization) for face recognition. Realized experiments with face recognition show that the proposed methods are more robust in conditions of low resolution, lighting and noise than the standard Viola-Jones algorithm.</p><p>However, several researchers in the field of computer vision and artificial intelligence point to the current problems of established psychological models. The Ekman classification <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_2\">[2]</a> in particular represents only a limited possibility of expressing diverse and subtle emotions. Russell\u2019s <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_2\">[4]</a> or Plutchik\u2019s model <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_2\">[3]</a> is therefore used to solve the problem of facial expression recognition across multiple data sets and to enrich emotional expression. Yang et al. <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2\">[19]</a> applied the learning TPCA algorithm to Russell\u2019s emotional model of arousal and valence. The facial emotion recognition method, based on TPCA and two-level fusion, combines mass fusion and correlation fusion between arousal and valence to achieve higher success in classifying emotional state.</p><p>Starostenko et al. <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2\">[20]</a> investigated the possibilities of applying local scale invariant feature transform descriptors for the extraction of areas of interest from the human face and then for the encoding of facial deformations according to the Ekman facial coding system (FACS). They used quadratic discriminant analysis and a Bayesian classifier to monitor and recognize faces in real time. With this method (based on the detected reference points) they were able to recognize six prototype expressions of the human face (Ekman\u2019s classification), as well as to determine affective states in real time, as their fuzzy inference system is based on the proposed reasoning model. Experiments show that Ekman\u2019s FACS, which is traditionally used in affective computation, needs to be extended to the interpretation of non-prototype emotions, for example using a psychological model from Plutchik, to achieve a more detailed classification of emotional states.</p><p>In artificial intelligence Russell\u2019s model is justified because it does not only work with strictly given emotional states - expressions that can be observed in the human face but we can use this model, for example, to classify data obtained from ECG sensors <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2\">[21]</a>, GSR <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2\">[22]</a>, EEG <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2\">[23]</a>, <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2\">[24]</a>, temperature sensor and others. Dissanayake et al. <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2\">[21]</a> point out that the standard Ekman model can be extended by another emotional states using the ECG sensor and classified as: anger; sadness; joy; and pleasure. They used Russell\u2019s model for classification, which can be used to record arousal and valence. Individual emotional states were classified using the SVM method.</p><p>According to Lee and Yoo <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2\">[22]</a>, negative emotion is one of the reasons why stress causes negative feedback. Therefore, many studies are being done to recognize negative emotions. However, emotions are difficult to classify because they are subjective and difficult to quantify. In addition, emotions change over time and are influenced by mood. Therefore, Lee conducted an experiment aimed at obtaining data from ECG and GSR sensors to detect objective indicators. Emotion-related functions were compressed using an auto-encoder (SAE) and, along with time information, used in long-term memory (LSTM) training. As a result, the proposed LSTM used with the function compression model showed the highest accuracy (99.4%) for recognizing negative emotions. The results of the proposed model were 11.3% higher than the neuronal model.</p><p>Shams et al. investigated the use of an electroencephalographic system (EEG) for emotion recognition (EEGER), based on capturing the dynamics of human emotions. They used Russell\u2019s model of the two-dimensional structure of valence and excitation to classify the obtained signals. They collected EEG signals from ten respondents and, based on Russell\u2019s model, classified four basic emotions: satisfaction, sadness, neutrality, and fear. They used the Regularized Least Square (RLS) and Multi-Layer Perception (MLP) neural network for classification. The results obtained from the classification showed a high accuracy of about 96% <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2\">[23]</a>.</p><p>According to Sorinas, it is important to develop more accurate and functional affective applications to achieve a balance between psychology and the technique applied to emotions. Therefore, it is appropriate to include other techniques in the recognition process, such as a face camera. Signals from the central and peripheral nervous systems are used to recognize emotions, but their functioning and the relationship between them remain unknown. In this context, Sorinas designed and developed a computational model for recognizing emotions in the valence dimension. He used electroencephalography (EEG), electrocardiography (ECG) and skin temperature signals for 24 tests in the experiment. Each methodology was evaluated individually, and characteristic patterns of positive and negative emotions were found in each of them <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2\">[24]</a>.</p><p>In recent years, a new approach to emotion recognition has been described, based on behavioural characteristics and the use of Russell\u2019s model. The advantages of using this approach are that the data used is not disruptive and can be easily obtained, for example when working with a keyboard <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2\">[25]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2\">[27]</a> a computer mouse <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2\">[28]</a>, a joystick when playing games <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2\">[29]</a> and the like. However, this phenomenon has only been studied to a limited extent in previous studies.</p><p>In 2016, Pentel investigated an unobtrusive way to detect user confusion by monitoring mouse movement. He designed a special computer game that collected the necessary data on the movement of the mouse and the intensity of pressing the buttons on the mouse, as well as the movement of the wheel. Functions extracted from the mouse movement record were used in the training dataset. SVM and Random Forest were used to create classification models. The model created using Random Forest provided classification results with an f-score of 0.938 <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2\">[28]</a>.</p><p>Lee dealt with behavioural characteristics in a similar way. He described a new approach to recognizing emotions, based on the dynamics of strokes. The aim of his study was to investigate, whether the change of variance while typing on a keyboard is caused by emotions. He conducted a controlled experiment to collect data from working with the keyboard (intensity of keystrokes, speed, type of key, etc.), focusing on the valence and arousal of a particular user. The results of the experiment suggest that the effect of arousal is significant in the duration of keystrokes (p &lt; 0.01), i.e. the duration and delay of keystrokes are affected by arousal <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2\">[25]</a>. The fact that behavioural characteristics can be obtained primarily from the behaviour of an individual in a particular situation was confirmed by Sharma\u2019s experiment, in which he focused on the continuous annotation of emotions in real time, as experienced by players playing video games, for example. To obtain relevant data, he developed a special intuitive joystick-based interface that allowed the simultaneous visualization of valence and arousal <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2\">[29]</a>.</p><p>As demonstrated by Pentel <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2\">[26]</a>, Lee et al. <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2\">[25]</a>, Salmeron-Majadas et al. <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2\">[27]</a> and Sharma et al. <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2\">[29]</a>, the processing of behavioural characteristics obtained as input data from a keyboard, mouse, joystick or other device used in a particular life situation is possible to implement using standard methods used in statistics and artificial intelligence. Examples of such methods are the Bayesian classifier, linear / nonlinear or multiple regression, SVM, neural networks or decision trees.</p></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Material and Methods</h2></div><p>Each Automatic Recognition System (ARS) must concurrently meet 3 phases of the recognition process: \n<ol><li><p>Detection,</p></li><li><p>Extraction,</p></li><li><p>Classification.</p></li></ol> These 3 phases are applied to each ARS and are produced by the method of data acquisition in the first phase. It is therefore not essential whether we choose a direct view of the camera or the capture of physiological human signals during detection. In the second phase of ARS, we focus on the extraction of areas of interest, i.e., in the case of a direct view into the camera, the area of interest is the individual parts of the face (mouth, nose, cheekbones, lips, eyes, eyebrows and the like). In the case of physiological signals, we focus on measuring them on individual parts of the body (GSR - skin, ECG - heart, EEG - head / brain, etc.). The third phase of ARS is the use of various classification algorithms, neural networks or other technologies, which take place through a large sample from the reference database and a psychological model subsequently classified by emotional state.</p><p>We designed and programmed the Emotnizer application in Java to determine behavioural characteristics. The application provides automatic recognition of the emotional state of the user based on the input of the keyboard and mouse; it can recognize 4 basic types of emotions. To classify the emotional state, we use Russell\u2019s model for 4 basic types of emotions (Happiness, Anger, Sadness, Relax), which are also found in the Ekman classification, based on the dimensions of arousal and valence <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_3\">[7]</a>. To determine the behavioural characteristics, we recorded the following 15 parameters (<a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>).<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nDescription of Measured Parameters (Behavioral Characteristics)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t1-3227627-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t1-3227627-small.gif\" alt=\"Table 1- &#10;Description of Measured Parameters (Behavioral Characteristics)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><div class=\"section_2\" id=\"sec3a\"><h3>A. Reference Database Preparation Procedure</h3><p>In the case of classification of emotional state based on behavioral characteristics, there is no reference database source that could be used, and in the case of ARS and frontal camera views, such a database had to be created and validated. The entire implementation of the reference database as well as its re-validation was performed under the supervision of a psychologist. We proceeded as follows to obtain data for the reference database. We provided the participants (total number 50) with a psychological questionnaire via the Emotnizer application. In this questionnaire, we present age, gender, number of hours spent on average per day at the computer, finger placement, dominant hand (right or left) and the like. From a psychological point of view, the questions were set by a psychologist to reveal these mental deviations from the standard. The questionnaire also provides an overview of what type of personality the participant is (choleric / sanguine and the like). With this step, we ensured the creation of a user profile. First, we had the participants transcribe the text with the appropriate sentiment using the Emotnizer applications (<a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>). The transcription of the text was not limited to time. Participants were given the opportunity to read it before it was transcribed. Thus, from a psychological point of view, we ruled out the possibility that they would make unnecessary mistakes in terms of transcribing an unknown text. At the same time, from this psychological point of view, we evoked an emotional state (affect) in them.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi1-3227627-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi1-3227627-small.gif\" alt=\"FIGURE 1. - Transcript of the text with the appropriate sentiment.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Transcript of the text with the appropriate sentiment.</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div></p><p>We used the psychological method of a self-assessment manikin (SAM) to classify the user\u2019s emotional state. On a scale of 1-5, participants had the opportunity to assess their emotional state through SAM for valence and arousal. The parameters subsequently entered into the database were SAM1 - rate of valence (from \u22121 to 1) and SAM2 - rate of arousal (from \u22121 to 1). Values marked by participants in the range 1-5 were recalculated to the interval [\u22121; 1] for valence and arousal. In this way, we obtained behavioral characteristics with the respective emotional state evaluated by SAM1 and SAM2. The predicates SAM1 and SAM2 are intended to classify the emotional state of the Russell model through the valence of arousal. According to Russell\u2019s model, a state of happiness occurs when a participant achieves high arousal along with high valence. In the emotional state of anger, on the other hand, there is high arousal but very low valence. Sadness can be described as a state of low arousal and low valence. The state of relaxation is characterized by low arousal and high valence.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Procedure for Verification of Behavioral Characteristics</h3><p>In this article, we focus on finding out whether it is possible to classify a user\u2019s emotional state from behavioral characteristics. For this verification, we will proceed as follows: \n<ol><li><p>Calculation of correlation coefficients, which express the dependencies of the variables SAM1 and SAM2 on the other variables examined,</p></li><li><p>Modification of classes, creation of new categories SAM1 (mod) and SAM2 (mod) from values observed in classes SAM1, SAM2 and calculation of correlation coefficients of new classes,</p></li><li><p>Creating decision trees and finding their basic indicators: accuracy, leaf count, node count,</p></li><li><p>Description of concepts, creation of rules with the greatest support.</p></li></ol></p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Results of the Experiment</h2></div><p>The examined data contained 15 original parameters from <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>, supplemented by the parameters Age, Sex, Hand, Shortcuts, Mouse Wheel, Finger placement, SAM1 and SAM2. Together, we examined 24 quantities. There were 131 of all monitored cases (participants). Descriptive statistics of the investigated quantities are given in the table (<a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>).<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nDescriptive Statistics of the Examined Variables</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t2-3227627-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t2-3227627-small.gif\" alt=\"Table 2- &#10;Descriptive Statistics of the Examined Variables\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>In our analysis, we were interested in the dependencies between the investigated characteristics and the indicators SAM1 and SAM2. For this reason, we calculated Pearson (standard) correlation coefficients for the observed characteristics, which together with the visualization are shown in the figure (<a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2a</a>). Based on the calculated correlation coefficients, we see a low degree of dependence of the variables SAM1 and SAM2 on the individual characteristics. Due to the low dependence, we adjusted the values of the variables SAM1 and SAM2. These variables can have the values \u22121; \u22120.5; 0; 0.5 and 1. This is a self-assessment of the valence and arousal of individual users. From the numerical values of the variables SAM1 and SAM2, we created categorical ordinal variables SAM1 (mod) and SAM2 (mod), which can have the values: low, medium, high (<a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3a</a>, <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">3b</a>, <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">3c</a>, <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">3d</a>). These were used to create decision trees (values: \u22121, 0, 1 were used to calculate correlation coefficients). Even after adjusting the values, reducing the number of categories did not significantly change the degree of dependence of the variables SAM1 (mod) and SAM2 (mod) (<a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2b</a>).\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi2ab-3227627-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi2ab-3227627-small.gif\" alt=\"FIGURE 2. - Pearson correlation coefficients for (a) SAM1 and SAM2 (b) SAM1(mod) and SAM2(mod).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Pearson correlation coefficients for (a) SAM1 and SAM2 (b) SAM1(mod) and SAM2(mod).</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi3abcd-3227627-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi3abcd-3227627-small.gif\" alt=\"FIGURE 3. - Visualization of absolute frequencies of variables (a) SAM1, (b) SAM2, (c) SAM1(mod), (d) SAM(mod).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Visualization of absolute frequencies of variables (a) SAM1, (b) SAM2, (c) SAM1(mod), (d) SAM(mod).</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div></p><p>From the measured data, we tried to create a classifier into the SAM1(mod) and SAM2(mod) categories. A decision tree was chosen as a classifier. Target attribute was SAM1(mod) a SAM2(mod) which can have the values: low, medium, high.</p><p>We used k-fold validation for evaluating our models. We first shuffle our dataset in k-fold cross-validation. We do this step to make sure that our inputs are not biased in any way. The original sample is randomly partitioned into k equal sized subsamples. A single subsample from the k subsamples is retained as the validation data for testing the model, and the remaining k\u20141 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once. We set k <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$=10$\n</tex-math></inline-formula> in our evaluation, so the dataset was randomly split into 10 subsamples.</p><p>Unfortunately, the accuracy values for the created decision trees using k-fold validation reached a maximum value of 65%. In our opinion, this value is insufficient. Most machine learning methods achieve acceptable accuracy for large datasets. Therefore, the main reason is probably the small sample of the control sample data examined (it contained only 131 examples). For this reason, we will describe the concepts of our collected data in the following text. We will make this description by creating a decision tree from all available data (i.e., data from 131 participants).</p><p>Decision trees have been created for both categorical variables SAM1 (mod) and SAM2 (mod). The first step in describing the concepts was to determine the appropriate parameters for creating the tree. For ease of interpretation, we only experimented with the maximum depth of the trees. Obviously, the accuracy of decision trees increases with increasing depth. At the same time, however, there is a risk that the tree will no longer generalize sufficiently and there is a risk of so-called overfitting. For this reason, we created 10 trees for both categorical variables, each with a different depth. The Gini index metric was used as a criterion for selecting a property for the distribution. When creating decision trees, we experimented with the size of the tree. We also found the node count and leaf count. Accuracy was also calculated for each tree created.<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} Accuracy=\\frac {Number~of~Correct~predictions}{Total~number~of~predictions~made}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} Accuracy=\\frac {Number~of~Correct~predictions}{Total~number~of~predictions~made}\\tag{1}\\end{equation*}\n</span></span></disp-formula> It is the ratio of number of correct predictions to the total number of input samples.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. List of Concepts for the Variable SAM1(Mod)</h3><p>From the created decision trees (<a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a>), we selected a tree with a maximum depth of 6 to describe the concepts.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi4-3227627-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi4-3227627-small.gif\" alt=\"FIGURE 4. - Example of parameters of created decision trees for classification into classes given by variable SAM1 (mod).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Example of parameters of created decision trees for classification into classes given by variable SAM1 (mod).</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div></p><p>We transformed the created decision tree (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a>) into rules. We subjected the 21 rules created to a closer analysis.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi5-3227627-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi5-3227627-small.gif\" alt=\"FIGURE 5. - Example of a part of the created decision tree.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Example of a part of the created decision tree.</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div></p><p>The conditions listed in the rules are disjoint. For each classified message, only one rule that classifies the message can be selected into one of three high / medium / low classes using a decision tree. The values of support and accuracy of a specific rule were calculated for the rules. Support or coverage of a rule is the percentage of instances to the condition of a rule. The support value was calculated for all cases from the training set as well as for the predicted class (it expresses the percentage of cases from the training set that were predicted to the given class). In the table (<a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Tab. 3</a>) we list the two rules for each predicted class that had the highest support value. For better clarity, we present the interpretation of rule id 16 (first row of the table). Using this rule, 33.59% of all cases were classified as \u201chigh\u201d with an accuracy of 0.57. This means that the rule was misclassified by quite a number of cases (43%). At the same time, this rule classified 43.86% of all cases that belong to the \u201chigh\u201d class. That rule was one of the least accurate. This rule has the largest share in the overall accuracy of the tree 0.82. Multiple rules created have an accuracy of 1. This means that the rule did not incorrectly classify any case from another class. If we created a complete decision tree, practically all rules would have an accuracy of 1. Inaccuracies are caused by setting the maximum depth of the tree, when the majority was determined as the predicted majority class for nodes in the maximum depth. However, it should be noted that rules with an accuracy of 1 have a very low support value.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nSample of the Rules With the Highest Support Value for Each Variable Class SAM1(mod)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t3-3227627-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t3-3227627-small.gif\" alt=\"Table 3- &#10;Sample of the Rules With the Highest Support Value for Each Variable Class SAM1(mod)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. List of Concepts for the Variable SAM2(Mod)</h3><p>In a similar way, it is possible to make a description of the concepts for the SAM2 variable (mod). From the created decision trees (<a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>), we selected a tree with a maximum depth of 5 to describe the concepts.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi6-3227627-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi6-3227627-small.gif\" alt=\"FIGURE 6. - Example of parameters of created decision trees for classification into classes given by variable SAM2 (mod).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Example of parameters of created decision trees for classification into classes given by variable SAM2 (mod).</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi7-3227627-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi7-3227627-small.gif\" alt=\"FIGURE 7. - Confusion matrices for SAM1(mod) and confusion matrices for SAM2(mod).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Confusion matrices for SAM1(mod) and confusion matrices for SAM2(mod).</p></fig></div><p class=\"links\"><a href=\"/document/9975322/all-figures\" class=\"all\">Show All</a></p></div></p><p>As in the case of SAM1 (mod), we transformed the selected decision tree into rules (total of 19). For each predicted class, we list the two rules that had the highest support value (<a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>).<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nSample of the Rules With the Highest Support Value for Each Variable Class SAM2(mod)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t4-3227627-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975322/magdi.t4-3227627-small.gif\" alt=\"Table 4- &#10;Sample of the Rules With the Highest Support Value for Each Variable Class SAM2(mod)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Discussion</h2></div><p>Classification using decision trees is one of the basic and practically simplest methods of machine learning. In case of using other (more suitable) methods, e.g., neural networks, on the created datasets it is assumed that the accuracy of the classification will increase. However, the advantage of decision trees is their easier interpretability of results. They are created mainly to understand the researched issues. For this reason, we implemented the description of concepts using the rules of created decision trees.</p><p>On the basis of standard procedure of creating a classifier for a set of training examples and its verification using a set of test examples, we achieved a max accuracy of 65% from the decision tree classifier model (in k-fold validation). For this reason, we used the collected data to describe concepts that point to important properties for classifying a user\u2019s emotional state.</p><p>We tried to relate all the parameters listed in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> to items SAM1 (valence) and SAM2 (arousal). These two items are very important for the classification of the emotional state using Russell\u2019s model. Therefore, if we want to classify the emotional state, all other data from <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> serve us to get the answer on which parameters changed the most when changing SAM1 and SAM2. The answers are the rules defined in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a> and <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>.It is also interesting to look at the confusion matrix of created rules for classification (high, medium, low) of classes SAM1 (mod) and SAM2 (mod).</p><p>From both confusion matrices (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5a</a>, <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5b</a>) it can be seen that the created rules have very good classification accuracy for the value \u201chigh\u201d. Of the 57 cases of SAM1 (mod) that contained the value \u201chigh\u201d, the rules classified 56 correctly. This is represented by the value of the so-called recall <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$=0.9824$\n</tex-math></inline-formula>. A similarly high value is recall <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$=0.7105$\n</tex-math></inline-formula> for \u201cmedium\u201d values. It is practically the same in the case of SAM2. This means that with the created model of the decision tree (as well as the rules extracted from it) we can correctly classify the values of SAM1 and SAM2 for the emotional states of high and medium. A value of \u201clow\u201d significantly complicates the classification of the emotional state. The result of our analyzes is the finding that using the investigated behavioral characteristics of the user, it is possible with high success to correctly classify emotional states for SAM1 and SAM2 falling into the category of \u201chigh\u201d, or \u201cmedium\u201d. The emotional state based on the predicates SAM1 and SAM2 falling into the category \u201clow\u201d cannot be classified in this way with the same success as in the previous case.</p><p>If we look in detail at the original Russell\u2019s model <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_5\">[5]</a> and the model that is currently used (supplemented by other emotional states) <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_5\">[30]</a>, we find that the emotional state of happiness applies to the high value of high arousal and positive valence. According to Russell\u2019s model, Angry is an emotional state represents a high value of high arousal and negative valence. This condition can therefore also be successfully classified by behavioral characteristics. The problem, however, is the classification of the emotional state of Relax and Sadness, these represent low arousal on the y-axis of Russell\u2019s model. This follows from the above that behavioral characteristics can be used to successfully classify especially emotional states with a high value arousal, whether they be states with positive or negative valence.</p><p>The classification of the user\u2019s emotional state on the basis of his behavioral characteristics is a new area of research not only from the point of view of psychology, but also of pedagogy and artificial intelligence. Each person has their own style of acquiring and processing knowledge. In connection with the so-called Adaptive hypermedia systems that have evolved in the past, this make this area of research very important, as it provides answers to why it was not possible to apply a single learning style to a particular user throughout the learning process.</p><p>In our research, we have shown that emotional states with the highest values of SAM1 and SAM2 can be classified with high success. If we compare the results of this research with the results already published, similar results were achieved by Lee and Yoo <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_5\">[22]</a> and Pentel <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_5\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_5\">[28]</a>. Lee and Pentel dealt with anger as well as the state of confusion in their classification (classification with an accuracy of 84.47%). However, according to Rosenberg and Ekman <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_5\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_5\">[31]</a>, confusion, frustration or shame cannot be considered an emotion, but rather a manifestation (effect) of a particular emotional state. The results from Epp et al. <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_5\">[31]</a> are also in agreement with the results of our experiment. Epp, Lippold, and Mandryk used two-stage classifiers to determine self-confidence, hesitation, nervousness, relaxation, sadness, and fatigue. In these low-emotional emotional states, SAM1 and SAM2 achieved a maximum classification success of 77.4%. However, for emotional states with a high value of SAM1 and SAM2 - anger and arousal, they achieved a classification success of up to 84%.</p></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VI.</div><h2>Conclusion</h2></div><p>In this paper we tried to find out whether it is possible to classify a user\u2019s emotional state using his or her behavioral characteristics.</p><p>We obtained the first cleaned data of behavioral characteristics from the number of 50 participants. At the same time, these 50 participants determined the value of SAM1 and SAM2 using a self-assessment manikin. In this way, we obtained not only behavioral characteristics, but also their subjective view of how they felt in the given situation. Since everything was carried out under the supervision of a psychologist, the processed data are valid. In this way, we obtained a database of behavioral characteristics, which we can now use for other experiments at any time. We did the current experiment with 131 participants in order to compare these data with each other, determine the success of the classifier and find any problematic parts that should be eliminated.The first finding was that using the data measured by us, it is possible to create a classifier of emotional state, but the overall success of the classification is max. 65%. The measured data is mainly influenced by the size of the research sample (number of participants), which is used to create behavioral characteristics in the database. For this reason, we focused on the description of the values we measured. To describe the concepts, we created a decision tree with an acceptable depth and rewrote the created tree into rules. To illustrate, we have listed the rules with the highest support value.</p><p>Upon closer examination of the classification of measured values using the confusion matrices, we found that the biggest problems in the classification are caused by the value \u201clow\u201d for the category SAM1 (mod) as well as SAM2 (mod). We have shown that the created rules (or the decision tree from which they were derived) can correctly classify the value \u201chigh\u201d, in the case of accepted error rate also the value \u201cmedium\u201d.</p><p>At the same time, it is necessary to realize that the method of classifying emotional states from behavioral characteristics is one of the most complicated, as it is a classification of often hidden emotions (unlike when using, for example, with a webcam). In further research, we will focus on improving the database of behavioral characteristics. Although there is currently very little expert work on this issue that could serve as a model for us, development and research in this area are still advancing. An example of how to design a public database of user behavioral characteristics can be the work of Ko\u0142akowska et al. <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_6\">[32]</a>. The problem (as also in our case), however, is the lack of validity of the data, which should be the subject of further investigation.</p><p>At the same time, however, we are working on further expanding the application and the classifier itself so that it is possible to classify other non-standard emotions from Russell\u2019s Circumplex model.</p><p>Currently, our goal is to implement Emotnizer into the educational process, as a part (module) for the most widespread Moodle e-learning system. Through such a module, we could reveal problematic parts of the educational process, especially when transcribing long texts on a computer and the subsequent connection with the overall attention of students.</p></div>\n<div class=\"section\" id=\"sec7\"><div class=\"header article-hdr\"><h2>Declaration of Interest</h2></div><p>All the authors substantially contributed to conducting the underlying research and drafting this manuscript. In addition, the authors have no conflict of interest, financial or otherwise.</p></div>\n</div></div></response>\n"
}