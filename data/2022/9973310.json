{
    "abstract": "Clustered storage systems are dominant solutions for the era of data-intensive computing. Ceph represents a sustainable clustered storage solution, supporting object, block, and file storage capabilities with no single point of failure. Despite the strong management abilities, security remains a serious concern in the Ceph storage system. To date, authentication and access control are the only sup...",
    "articleNumber": "9973310",
    "articleTitle": "CephArmor: A Lightweight Cryptographic Interface for Secure High-Performance Ceph Storage Systems",
    "authors": [
        {
            "preferredName": "Fatemeh Khoda Parast",
            "normalizedName": "F. Khoda Parast",
            "firstName": "Fatemeh",
            "lastName": "Khoda Parast",
            "searchablePreferredName": "Fatemeh Khoda Parast"
        },
        {
            "preferredName": "Brett Kelly",
            "normalizedName": "B. Kelly",
            "firstName": "Brett",
            "lastName": "Kelly",
            "searchablePreferredName": "Brett Kelly"
        },
        {
            "preferredName": "Saqib Hakak",
            "normalizedName": "S. Hakak",
            "firstName": "Saqib",
            "lastName": "Hakak",
            "searchablePreferredName": "Saqib Hakak"
        },
        {
            "preferredName": "Yang Wang",
            "normalizedName": "Y. Wang",
            "firstName": "Yang",
            "lastName": "Wang",
            "searchablePreferredName": "Yang Wang"
        },
        {
            "preferredName": "Kenneth B. Kent",
            "normalizedName": "K. B. Kent",
            "firstName": "Kenneth B.",
            "lastName": "Kent",
            "searchablePreferredName": "Kenneth B. Kent"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227384",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9973310/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Clustered storage systems define well-adapted solutions in data-intensive domains <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>. These systems can be directly adopted by an organization or a cloud service provider. Cloud storage represents a straightforward but expensive method to manage a large volume of data. In addition, governmental organizations are reluctant to share clients\u2019 information with any third-party service provider. Data-intensive applications such as multimedia, meteorology, simulation, and space science demand a high-performance environment, and inefficient I/O can become the system bottleneck. Ceph portrays one of the open-source distributed storage management solutions initially designed to address the scalability, single-point-of-failure, and performance issue in the storage systems <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>, <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>. High-Performance Computing (HPC) <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, data centers, and cloud environments <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a> denote part of Ceph\u2019s applications. Ceph offers different forms of storage through a Reliable, Autonomous, Distributed Object Store (Rados) layer. Block and file storage functionalities are enabled through Rados Block Device (RBD) and CephFS <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>.</p><p>Improvement of storage and file system capabilities, i.e., the metadata management <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a>, <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>, <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a>, <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\">[18]</a>, <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\">[19]</a>, and file system load-balancing <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a>, have been the main focus of the community from Ceph\u2019s inception <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\">[21]</a>; security, on the other hand, has been neglected. Data confidentiality in transit and storage (at rest) depicts two critical security aspects in storage systems <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\">[22]</a>. Ceph included authentication and access control protocols from the initial versions.</p><p>Despite the authentication protocols, Ceph still has no support for data confidentiality at the storage devices. The only methods for protecting data within the storage devices are employing the underlying infrastructure, such as LUKS or client-side encryption. Needless to say, these methods are not only associated with complexities but involve security vulnerabilities <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1\">[23]</a>.</p><div class=\"section_2\" id=\"sec1a\"><h3>A. Problem Statement</h3><p>The National Institute of Standards and Technology (NIST) has defined three pillar security objectives, <i>confidentiality</i>, <i>integrity</i>, and <i>availability</i> (CIA), for any network-connected system. Later, <i>authenticity</i> and <i>accountability</i> were added to the current list (CIA++) to address further security concerns. A system should provide data confidentiality, integrity, availability, access control, authentication, and non-repudiation to fulfill the security objectives <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1a\">[24]</a>. While CIA++ is recommended, authentication and access control are the only security services in Ceph.</p><p>Despite the NIST recommendation, some services have a higher priority in a system based on the application. Ceph, as a data storage, should have at least robust authentication, access control, and confidentiality to protect data within the storage medium. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> illustrates an attack tree for a Ceph storage server with the most common security threats and countermeasures. Security is not a concrete concept; one extra security protocol or layer enhances the security surface of a target resource. Although the confidentiality of data at the storage layer is studied in this research, we evaluate other security services presented in the system. Authentication verifies the identity of an entity and establishes a secure client-server connection for communication. There are numerous security threats for attacking authentication methods by stealing or breaking secrets. In <i>social engineering</i>, a user would unintentionally reveal the secrets to an attacker, while in a <i>brute force attack</i> or <i>dictionary attack</i>, an adversary attempts to break a pass phrase. Secrets also can be compromised in a <i>session hijack</i>, <i>keylogger attack</i>, or a <i>SQL injection</i>. In these threats, the attacker attempts to steal the secrets by interleaving the connection, attacking the key storage, or installing software for recording critical patterns <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1a\">[25]</a>.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda1-3227384-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda1-3227384-small.gif\" alt=\"FIGURE 1. - Attack tree on ceph storage servers.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Attack tree on ceph storage servers.</p></fig></div><p class=\"links\"><a href=\"/document/9973310/all-figures\" class=\"all\">Show All</a></p></div></p><p>Breaking authentication compromises data confidentiality when the only security protection is authentication. Intruders can steal plain data or modify the original message breaking this layer. An insider attack indicates a threat toward both security goals. Unauthorized data access can be easier for the employees of a company by stealing secrets through a <i>shoulder attack</i>, a subclass of social engineering, or inappropriate data access <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_1a\">[26]</a>. Data encryption at the storage level represents a countermeasure to mitigate information disclosure in case of unauthorized access. Ceph has no embedded data encryption at the storage layer. The Rados gateway (RGW) layer supports client-side storage encryption. As discussed, Ceph supports file, block, and object storage. The RGW interface is introduced for block storage. Typically, a third-party application, such as OpenStack, will come over the RGW interface to provide the storage functionalities. This model represents the client-side encryption associated with numerous complexities and challenges, such as key management systems. The other solution benefits from the underlying infrastructure security approaches such as Linux LUKS <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_1a\">[27]</a>, <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_1a\">[28]</a>. LUKS introduces a hardware-based security approach for encrypting volumes with a user-driven password key. The encryption key is created from the user key combined with the masking data, <i>salt</i>, and stored in a metadata file on the same device. Accessing physical drives, an attacker can retrieve metadata and break the key by brute force or a dictionary attack <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1a\">[23]</a>. To this end, we propose an embedded encryption approach to address the data confidentiality at the storage layer.</p></div><div class=\"section_2\" id=\"sec1b\"><h3>B. Contribution</h3><p>We propose a lightweight security API, named CephArmor, over the Rados layer. CephArmor represents a countermeasure to address data confidentiality on the Ceph storage system. All data are stored encrypted through the proposed API without any interference in performance as it has been built over the socket-based native protocol, Rados. In summary our contributions are:\n<ul style=\"list-style-type:disc\"><li><p>Integrating storage encryption with the Ceph source code.</p></li><li><p>Proposing an all-in-one security API for professional and non-professional users.</p></li><li><p>Deploying the proposed method over a real-world environment, conducting an extensive set of experiments, and analyzing the performance of the proposed approach through various benchmarks.</p></li></ul></p><p><a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> represents the employed abbreviations in this study. The rest of this paper is arranged as follows. The status of the literature and system overview are discussed in <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Sections II</a> and <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">III</a>. System design and implementation are presented in <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a>, a description of the experimental environment in <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a>, and results in <a ref-type=\"sec\" anchor=\"sec6\" class=\"fulltext-link\">Section VI</a>. A discussion of the security analysis is provided in <a ref-type=\"sec\" anchor=\"sec7\" class=\"fulltext-link\">Section VII</a>, and future work and conclusion in <a ref-type=\"sec\" anchor=\"sec8\" class=\"fulltext-link\">Sections VIII</a> and <a ref-type=\"sec\" anchor=\"sec9\" class=\"fulltext-link\">IX</a>.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nAbbreviations Used in This Study (Alphabetically Ordered)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t1-3227384-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t1-3227384-small.gif\" alt=\"Table 1- &#10;Abbreviations Used in This Study (Alphabetically Ordered)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Status of Security in Distributed Storage Systems</h2></div><p>Unlike proprietary storage systems, open-source solutions provide higher flexibility in underlying hardware commodities. Users can select the desired open-source system based on their requirements and capabilities. Following a general overview of the most popular CFSs, we elaborate on each one from a technical viewpoint. Afterward, we analyze the storage encryption methods proposed in the literature.</p><div class=\"section_2\" id=\"sec2a\"><h3>A. Technical Comparison of Open Source CFSs</h3><p>Hadoop Distributed File System (HDFS) defines a scalable open-source storage solution to handle large volumes of data. The namespace in HDFS includes a folder/file hierarchy managed by <i>inodes</i>. The <i>namenode</i> is a single metadata server tracking data storage information within numerous <i>datanodes</i> as a data storage architecture. The single metadata server is a performance bottleneck in the HDFS architecture. HDFS supports C, Python, Java, and REST APIs as communication modalities. The load balancing is applied on a server based on a fixed threshold value. If the balance reaches the threshold, data copies move to other servers. The namenode plays an important role in detecting any fault in the datanodes. The namenode receives a signal from the datanode, indicating the availability of a node. If the namenode receives no heartbeat from a datanode, that node would be eliminated from available storage nodes, and data of that node would propagate among other datanodes <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2a\">[29]</a>.</p><p>Integrated Rule-Oriented Data System (iRODS) benefits from Unix naming conventions and follows a similar hierarchy structure. iRODS includes one metadata server, <i>iCat</i>, and numerous data servers. As a centralized data solution, the iCat is a bottleneck of the system. Replication and load balancing are not fully automated and are customized through user-defined rules. The placement policy is also the user\u2019s responsibility, which might be noted as a disadvantage of the solution. Fault detection, however, operates automatically through message passing among servers <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2a\">[30]</a>.</p><p>Lustre represents an open-source parallel, distributed file system. Two metadata structures are employed in this system, a single Metadata Target (MDT) and two Meta Data Servers (MDSs) for handling metadata requests. Similarly, data storage utilizes two data structures, Object Storage Target (OST) and Object Storage Server (OSS), the former for object storage and the latter for response to object requests. The object names are mapped into the related OST via a single namespace. Interactions with Lustre proceed through the file system in the userspace (FUSE) interface. The load balancing is supported by sending new write requests into OSTs with a lower data population. The fault detection procedure only initiates with an I/O request. If an MDS sends no response to an I/O request, the system assumes the MDS is out-of-service <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2a\">[31]</a>.</p><p>Gluster File System (GlusterFS) defines an open-source distributed file system primarily funded by Gluster Inc. and later acquired by Red Hat. The Gluster architectural design is quite different from other distributed file systems. That means no metadata server is defined to manage the location or additional fundamental storage information; instead, storage operations are performed in a global namespace. A hash function mechanism in this system, Elastic Hashing Algorithm (EHA), maps object names in the global namespace. A data <i>brick</i> is the smallest storage element, mapping an Extended File System (XFS) directory to a user namespace. According to the Red Hat recommendation, defining more than two bricks mitigates the SPoF issue in the system. REST and FUSE APIs are standard tools for communication purposes with the file system. The EHA equalizes storage load among existing disks; however, adding or removing a disk requires manual configuration for bypassing I/O to the new or existing storage traffic due to a current disk change. In addition, a fault is detected when no response is received from a storage volume <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2a\">[32]</a>.</p><p>Ceph provides three forms of storage interfaces: objects, blocks, and files, each with built-in daemons and corresponding components. Employing daemons, <i>OSD</i>, <i>MON</i>, <i>MDS</i>, and <i>MGR</i>, the SPoF is eliminated and scalability, failure detection, and fault tolerance are improved. OSD defines a single daemon per disk entity, handling the object storage process. Then an OSD daemon communicates with a MON to report its own and other OSD states. The MON retains the map of OSDs, PGs, and CRUSH. The MGR operates as a bookkeeping module, and MDS manages the file metadata operations <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2a\">[33]</a>, <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2a\">[34]</a>.</p><p>HDFS, iRODS, Lustre, GlusterFS, and Ceph represent the five top most adapted open-source CFSs. HDFS was not initially released with internal security mechanisms. Eventually, multiple projects were added to the HDFS project, such as Apache Knox, to support various security aspects.</p><p>Kerberos controls the authentication in the Hadoop file system as a third-party security provider. In contrast, Apache Knox controls user access control. The data encryption at rest was not initially included in HDFS, and users can benefit from the pluggable interfaces to have this service in storage medium <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2a\">[35]</a>, <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2a\">[36]</a>.</p><p>iRODS have no embedded policy for data encryption at storage or authentication. As a middleware, iRODS is highly dependent on the security mechanism of the underlying infrastructure/operating system. For authentication, iRODS users can employ plugins, such as OpenID, or a third-party protocol. <i>Tickets</i>, <i>permissions</i>, and <i>federation</i> are access control mechanisms. Tickets are one-time-use tokens granting read/write access to a user. Permissions behave similarly to Unix file permissions, which grant read/write authority to a group of users in a zone. The federation model, on the other hand, grants read/write access to users beyond a zone <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2a\">[30]</a>.</p><p>Lustre internally has no storage cryptographic mechanism for file/object encryption. Access control is not directly provided in Luster. Users can benefit from the underlying mechanisms such as the Unix Discretionary Access Control (DAC) method or Mandatory Access Control (MAC) in Security-Enhanced Linux (SELinux). Kerberos server can be instantiated for authentication in Luster. This model has no built-in authentication mechanism and depends on a third party for authentication <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2a\">[31]</a>. Client-side encryption, however, has been supported for data protection. In this model, users manage keys, and encryption/decryption is performed on the user side <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2a\">[37]</a>.</p><p>GlusterFS internally provides no mechanism for storage encryption. However, constructing data bricks over encrypted block devices is applicable in this system. GlusterFS benefits from SSL/TLS authentication. These methods employ a public key set for verifying the identity of an entity and establishing a connection. Access control is possible through the underlying infrastructure, POSIX Access Control Lists (ACLs) <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2a\">[32]</a>.</p><p><a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> summarizes a technical comparison of the discussed CFSs in this section. From the user and developer points of view, Ceph and GlusterFS have achieved more adoption than comparable solutions due to the rich documentation and user guides. Compared to other CFSs, Ceph has properly addressed centralization and SPoF issues through numerous daemons without undermining the distribution. Self-balancing represents the other important feature of Ceph operated periodically to retain the balance of data within a storage cluster <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2a\">[33]</a>, <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2a\">[34]</a>. Ceph detects and refines failures in any granularity. All mentioned advantages, in addition to high availability and scalability, have made Ceph superior to other open-source Clustered File Storage (CFS) solutions <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2a\">[21]</a>. Comparing the primary security protocols, Ceph has built-in internal authentication and access control mechanisms. <i>CephX</i> represents the embedded authentication model with two different modes, <i>crc</i>, and <i>secure</i>, as discussed earlier. In addition to the built-in authentication mechanism, other third-party mechanisms, such as Kerberos, can be adapted to control the identity of entities.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nTechnical Comparison and Security Services of Open-Source CFSs</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t2-3227384-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t2-3227384-small.gif\" alt=\"Table 2- &#10;Technical Comparison and Security Services of Open-Source CFSs\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Shah and So <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2a\">[22]</a> proposed <i>Lamassu</i>, a host-based file encryption approach. Lamassu applies two rounds of encryption, one on the file and the other round on the keys. In the file encryption process, one key is utilized to encrypt the file, and the other key is employed to protect the encryption key. The first key is stored on a metadata segment at the beginning of the file. Chen et al. <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2a\">[38]</a> proposed a heuristic storage encryption approach, named <i>Carp</i>, in preserving frequently accessed data unencrypted in memory for a short period to reduce the encryption overhead. In addition to a path to objects, multiple flags, i.e., encryption bit and relaxed counter, are defined to trace the status of an object. Receiving a read request, the encryption flag is checked; in case of 0, the object retrieves from disk; otherwise, the flag resets to 1, the relaxed counter decreases by 1, and the object is retrieved from memory. A function periodically checks the relaxed time of objects for the re-encryption process. The variable <i>Time-To-Live (TTL)</i> verifies the threshold an object can live in memory unencrypted, which is defined based on the access frequency. Although the proposed approach improves the performance, preserving data unencrypted in memory for any period of time exposes the system to various security vulnerabilities. Storer et al. <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2a\">[39]</a> employed a splitting and spreading technique to address security in large-scale archival storage systems without encryption. Feng et al. <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_2a\">[40]</a> presented an efficient object-based encryption method named <i>BLESS</i> (oBject Level Encryption for Secured object-based Storage System), in which an object is partially encrypted. A user defines the sensitive area of an object for encryption in BLESS, integrated with Lustre. The Metadata Server (MDS) includes an encryption module for cryptography purposes. Issuing a write, the MDS receives the request, a key exchange is performed between client and MDS, the object is encrypted, and is stored on the Object Storage Daemon (OSD). Similarly, the read request is issued to the MDS, a key exchange procedure between client and MDS, the object returns to the client, and the decryption module is called to receive the plain text.</p><p>Methods storing keys in the same location as data, such as Lamassu <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2a\">[22]</a>, undermine the confidentiality of data. Carp <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2a\">[38]</a> stores data in plain format within the storage before a threshold is reached to reduce the encryption overhead. Keeping plain text in memory exposes it to attackers and undermines data confidentiality. Bless <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_2a\">[40]</a> requires users to define critical parts of the data to perform encryption on those sections. There are numerous issues with this approach. Defining the sensitive Section of a file/object is a complicated task for a client; any fault could lead to data leakage. Moreover, the method might be less helpful for files with highly sensitive portions. Based on our knowledge, no secure method has been integrated with an available storage solution in a real-world scenario. In this study, we implemented the proposed security API within the Ceph version 16.2.7 and operated exhaustive experiments to evaluate the performance with different scenarios.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>System Overview</h2></div><p>We analyze Ceph\u2019s underlying infrastructure, design, and other required knowledge for this study. Later we discuss the block encryption security models and define the selected algorithm in the proposed interface.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. CEPH Background</h3><p>On the surface, Ceph has an overwhelming architecture, including abstract concepts and entities such as <i>OSD</i>, <i>Monitor (MON)</i>, <i>MDS</i>, and <i>Manager Data Server (MGR)</i>. Rados characterizes Ceph\u2019s lowest level storage system, comprising the principal Ceph architecture. OSDs and MONs are the primary daemons in a Ceph cluster, enabling Rados to provide the promised storage services. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a> depicts a high-level abstract view of the Ceph architecture. Ceph provides three primary interfaces for communication purposes. All Ceph interaction methods are built over the Rados interface. The librados library provides socket-based native protocols for high-speed communication through Rados. RGW has been built over the librados API for the REST programming technique. The RGW API, as an object storage proxy, supports S3 and swift standards for Amazon and OpenStack web application models. RGW translates the REST requests to the native Rados commands. RBD provides the Ceph block storage interface favoured in a cloud storage environment. RBD could be utilized through a Virtual Machine (VM) or a host model. RBD provides a unit disk entity from the smaller spread storage volumes within a cluster to a virtualization container in the VM model. Although an RBD module is mapped to a Linux device in the host model, it still includes all underlying object-based features such as parallel reads. CephFS facilitates a POSIX-compliant hierarchy file access storage format. In this application, metadata servers assist the system in simulating and managing the file system hierarchy <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_3a\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_3a\">[34]</a>, <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_3a\">[41]</a>. As displayed in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a>, the Rados API directly communicates with OSDs and MONs, while other APIs should employ Rados for issuing any storage procedures over a cluster.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda2-3227384-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda2-3227384-small.gif\" alt=\"FIGURE 2. - Ceph architecture.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Ceph architecture.</p></fig></div><p class=\"links\"><a href=\"/document/9973310/all-figures\" class=\"all\">Show All</a></p></div></p><div class=\"section_2\" id=\"sec3a1\"><h4>1) Object Storage Daemons and Monitors</h4><p>Object Storage Daemons (OSD) defines a daemon providing a software layer over a disk equipped with a file system for uniforming disks as a unified storage space. Technically, an OSD maps a file system path as part of a cluster. OSDs are smart elements communicating with other OSDs for data replication, replacement, and migration. Monitors (MON) represent the other essential component of a storage cluster, maintaining the status of a cluster\u2019s members, such as OSD and host. MONs support OSD coordination for correct functionality and assure consistency. In a typical Ceph deployment, one OSD per disk and an odd number of MONs have been recommended to fulfill the voting policy <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_3a1\">[4]</a>.</p></div><div class=\"section_2\" id=\"sec3a2\"><h4>2) Pools</h4><p>Pools are abstract entities grouping objects, as the atomic unit of data, through Placement Groups (PGs). <i>Replicated</i> and <i>Erasure Code (EC)</i> pools denote two supported types. Data protection is fulfilled by replication in the <i>replicated</i> pools. According to the replication size, the Controlled Replication Under Scalable Hashing (CRUSH) algorithm mirrors data in the failure domains. This type of pool behaves similarly to the Redundant Array of Independent Disks 1 (RAID) storage technology. In RAID 1, the storage capacity splits into two equal parts, leading to a 50% redundancy level. In a Ceph replicated pool, <i>Replica2</i> provides 50%, and <i>Replica3</i> a 33% storage efficiency.</p><p>In storage systems with large volumes of data, performance is equally vital as data resiliency and storage overhead. An EC pool deploys a striping technique to address storage efficiency and overhead simultaneously. It splits data into <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> chunks and adds <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> parity chunks. Compared to the RAID system, EC behaves somewhat similarly to RAID 5 and 6. With <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M=1$\n</tex-math></inline-formula>, the system behaves analogously to RAID 5 and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M=2$\n</tex-math></inline-formula> to RAID 6. In <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M=1$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M=2$\n</tex-math></inline-formula>, the system can tolerate losing one and two failure domains, respectively <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_3a2\">[42]</a>, <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_3a2\">[43]</a>. Storage overhead in the EC pool can be formalized as <a ref-type=\"disp-formula\" anchor=\"deqn1-deqn3\" href=\"#deqn1-deqn3\" class=\"fulltext-link\">(1)</a> and the efficiency can be characterized as <a ref-type=\"disp-formula\" anchor=\"deqn1-deqn3\" href=\"#deqn1-deqn3\" class=\"fulltext-link\">(2)</a>. Assuming OSDs as the failure domain (FD), the minimum required OSDs is illustrated in <a ref-type=\"disp-formula\" anchor=\"deqn1-deqn3\" href=\"#deqn1-deqn3\" class=\"fulltext-link\">(3)</a>. In an EC pool with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> chunks of data and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> parity, the original data can be restored from the remaining fragments if we lose <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> OSDs.<disp-formula id=\"deqn1-deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} E\\mathcal {C}_{overhead}=&amp;M / (K+M) \\tag{1}\\\\ E\\mathcal {C}_{efficiency}=&amp;1-EC_{overhead} \\tag{2}\\\\ E\\mathcal {C}_{FD}=&amp;{K+M} \\tag{3}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} E\\mathcal {C}_{overhead}=&amp;M / (K+M) \\tag{1}\\\\ E\\mathcal {C}_{efficiency}=&amp;1-EC_{overhead} \\tag{2}\\\\ E\\mathcal {C}_{FD}=&amp;{K+M} \\tag{3}\\end{align*}\n</span></span></disp-formula></p><p>The EC pool performs more efficiently than a replicated pool in terms of storage overhead. However, the EC algorithm consumes more CPU and Memory as it requires computations. According to the application, the replicated pool would be more desired once a high-speed read is essential. In contrast, the EC pool provides higher efficiency in archival and cold storage purposes <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_3a2\">[5]</a>, <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_3a2\">[42]</a>, <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_3a2\">[43]</a>.</p></div><div class=\"section_2\" id=\"sec3a3\"><h4>3) Placement Groups</h4><p>Placement Groups (PGs) form logical entities for object location management. Tracking the location of objects in the storage device is a computationally expensive procedure. Instead, Ceph splits a pool into placement groups and maps an object to a PG <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_3a3\">[5]</a>. PGs aggregate pool objects to enhance the storage system\u2019s performance by providing a higher granularity compared to the object-based placement tracking approach. PGs store objects into OSDs according to the hash value of objects\u2019 identification. There is a trade-off between the number of PGs\u2019 and data durability, distribution, and resource usage. The recommended number of total PGs per pool is formalized as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$(OSDs \\times 100) / (pool\\_{}size)$\n</tex-math></inline-formula>.</p></div></div><div class=\"section_2\" id=\"sec3b\"><h3>B. A Review on Block Encryption Techniques</h3><p>Symmetric and asymmetric encryption algorithms represent single- and multiple-key cryptographic approaches. In a symmetric algorithm, a single key performs encryption and decryption. The Advanced Encryption Standard (AES) and Data Encryption Standard (DES) algorithms represent examples of symmetric, while Rivest-Shamir-Adle (RSA) represents an asymmetric approach. Symmetric models perform well in systems without key communication between users. In contrast, asymmetric, known as public-key (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$PK$\n</tex-math></inline-formula>), methods perform securely in systems with more users and key exchanges, such as cloud environments. One drawback of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$PK$\n</tex-math></inline-formula> systems is that they are typically slower than the symmetric-key methods with an identical key length <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_3b\">[44]</a>, <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_3b\">[45]</a>. Comparing the symmetric-key cryptosystems, AES stands out due to its time efficiency and security <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_3b\">[46]</a>, <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_3b\">[47]</a>. Despite innovative approaches for breaking AES, such as prime and probe side-channel attacks, the method is strong enough to prevent currently known types of vulnerabilities <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_3b\">[48]</a>.</p><p>To date, the National Institute of Standards and Technology (NIST) has approved fourteen modes of block cipher encryption, including eight confidentiality modes (ECB, CBC, OFB, CFB, CTR, XTS, FF1, and FF3), one authentication mode (CMAC), and five confidentiality and authentication modes (CCM, GCM, KW, KWP, and TKW) <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_3b\">[49]</a>. Electronic Codebook (ECB) and Cipher Block Chaining (CBC) represent the well-known block cipher models. ECB encrypts a block of data separately, increasing the cryptanalysis implications and reducing the security. The CBC mode addresses ECB vulnerabilities by chaining blocks of encryption. The output feedback (OFB) and Counter (CTR) models benefit stream encryption procedures. Disk encryption modes, such as XEX Tweakable Block cipher text Stealing (XTS), are introduced to protect data at the hardware levels <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_3b\">[50]</a>. Hybrid models, i.e., CCM, typically apply two rounds of encryption, and a higher time overhead. Despite the implementation complexity of hybrid modes, these approaches guarantee data confidentiality and authentication simultaneously. This study selected the CBC mode as the most compatible model with the Ceph infrastructures and layers. The performance and efficiency of this model are critical selection factors. This model can deal with object storage and provides high performance, an essential element in distributed storage systems.</p><p>AES represents a block cipher encryption scheme with a fixed block size of 128 and three possible key sizes of 128, 192, and 256, with 10, 12, and 14 rounds of transform operations, respectively. AES with 192 and 256 key length have 20% and 40% increased overhead compared to the AES 128. <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a> represents the AES algorithm specifications. The AES-128 is strong enough to tolerate attacks on the key with the processing power of this era. Although AES-128 sufficiently tolerates any common known key attack, it can be broken with post-quantum computing power. The AES-256, however, can withstand the attacks with the post-quantum computing systems <a ref-type=\"bibr\" anchor=\"ref51\" id=\"context_ref_51_3b\">[51]</a>.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nAES Different Key Length Comparison</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t3-3227384-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t3-3227384-small.gif\" alt=\"Table 3- &#10;AES Different Key Length Comparison\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>AES includes four coarse-grained stages in a high-abstract description, each associated with multiple fine-grained mathematical operations. The algorithm primarily deploys a sequence of substitution-permutation operations on four-by-four arrays of data to transform a plain text into the counterpart cipher text. The process commences with XORing the plain text with the assigned key. According to a lookup table, the XORed bits are replaced with new values in the substitution step. The permutation phase operates a cyclic shift on rows of data.</p><p>The AES primarily operates on a single block of 128 bits. However, in larger volumes, the data can be divided into blocks of 128 bits and encrypted with the same secret key. The issue with this approach is that the associated cipher text comprises critical information that might reveal the driven plain text. The CBC mode has been introduced to address the cryptoanalysis implications of ECB. Assume <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$E_{k}(\\cdot)$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathcal {D}_{k}(\\cdot)$\n</tex-math></inline-formula> represent encryption and decryption functions with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> indicating the symmetric key. In the AES CBC encryption <a ref-type=\"disp-formula\" anchor=\"deqn4-deqn5\" href=\"#deqn4-deqn5\" class=\"fulltext-link\">(5)</a>, each block of plain text XORes with the cipher resulting from the previous encryption block <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$cipher_{i-1}$\n</tex-math></inline-formula> and encrypted by the secret key <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>. An initialization vector denoted by IV is generated at the beginning of encryption to fulfill the algorithm\u2019s primary requirements as there is no previous plain text block <a ref-type=\"disp-formula\" anchor=\"deqn4-deqn5\" href=\"#deqn4-deqn5\" class=\"fulltext-link\">(4)</a> <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_3b\">[50]</a>.<disp-formula id=\"deqn4-deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} E_{k}(plain_{i})=&amp;E_{k}(IV \\oplus plain_{i}), i = 0;\\tag{4}\\\\ E_{k}(plain_{i})=&amp;E_{k}(cipher_{i-1} \\oplus plain_{i}), i &gt;0;\\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} E_{k}(plain_{i})=&amp;E_{k}(IV \\oplus plain_{i}), i = 0;\\tag{4}\\\\ E_{k}(plain_{i})=&amp;E_{k}(cipher_{i-1} \\oplus plain_{i}), i &gt;0;\\tag{5}\\end{align*}\n</span></span></disp-formula></p><p>The decryption, however, has no dependency on any process from other blocks. In decryption <a ref-type=\"disp-formula\" anchor=\"deqn6-deqn7\" href=\"#deqn6-deqn7\" class=\"fulltext-link\">(7)</a>, the plain text block, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$plain_{i}$\n</tex-math></inline-formula>, resulted from XORes of the current decrypted block <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$cipher_{i}$\n</tex-math></inline-formula> by secret key <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> and previous cipher block <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$cipher_{i-1}$\n</tex-math></inline-formula>, which are prepared in advance. Similarly, the IV at the beginning of the process accomplishes the algorithm requirement <a ref-type=\"disp-formula\" anchor=\"deqn6-deqn7\" href=\"#deqn6-deqn7\" class=\"fulltext-link\">(6)</a> <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_3b\">[48]</a>.<disp-formula id=\"deqn6-deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\mathcal {D}_{k}(cipher_{i})=&amp;\\mathcal {D}_{k}(cipher_{i}) \\oplus IV,\\quad \\qquad i = 0;\\tag{6}\\\\ \\mathcal {D}_{k}(cipher_{i})=&amp;\\mathcal {D}_{k}(cipher_{i-1}) \\oplus cipher_{i},\\quad i &gt;0;\\tag{7}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\mathcal {D}_{k}(cipher_{i})=&amp;\\mathcal {D}_{k}(cipher_{i}) \\oplus IV,\\quad \\qquad i = 0;\\tag{6}\\\\ \\mathcal {D}_{k}(cipher_{i})=&amp;\\mathcal {D}_{k}(cipher_{i-1}) \\oplus cipher_{i},\\quad i &gt;0;\\tag{7}\\end{align*}\n</span></span></disp-formula></p><p>The key and IV are two required values in the AES CBC encryption/decryption procedures. The IV should be random or pseudorandom data to provide semantic security, preventing adversaries from obtaining extra knowledge about the plain message.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>System Design and Implementation</h2></div><p>In this section, we first elaborate on the assumptions of the proposed method, discuss the threat model, and proceed with the design and implementation details.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Threat Model</h3><p><a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> illustrates possible attacks on the Ceph storage system. Without confidentiality, authentication is the only layer safeguarding the whole system. Breaking authentication can compromise the stored data within the storage medium. In this study, we added one extra layer to protect data confidentiality at the storage layer.</p><p>Malicious entities in this system can be either employees of an organization without access rights or any outside attacker. An outside attacker can eavesdrop on the channel to obtain the message, break the authentication, or directly access physical storage. We customize the underlying security layer to protect data confidentiality regardless of the threat entity. The activation of authentication is optional in Ceph. In the proposed design, however, the authentication must be activated to access certain functionalities. In <i>crc</i> mode, the wire is not encrypted. A user sends plain text to the server, and CephArmor encrypts data and propagates it through OSD nodes. In <i>secure</i> mode, the protocol encrypts data, forwards it through the wire, and decrypts it at the destination. The <i>secure</i> mode guarantees the end-to-end data protection.</p><p>Ceph internally protects data availability by replication or erasure coding. In a malicious activity, the system automatically detects and replaces the missed replication if an attacker removes one copy of the data. The self-healing specification of Ceph reduces the risk of data lost by malicious activities. Data modification, however, defines the other security issue. In a read request, Ceph returns the stored object or replicated versions to the user. If an attacker modifies data, there will be no additional security protection to detect the message modification. We assume an <i>honest-but-curious</i> storage system in an untrusted environment. In this model, opponents have access to the stored data but do not intentionally manipulate or destroy the contents <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_4a\">[22]</a>.</p><p>Further, we assume the AES encryption algorithm is strong enough to withstand any form of known attacks on the algorithm, such as side-channel attacks. We also assume the attacker knows the algorithm we select and how it works. We assume the KMS is secure enough to protect encryption keys.</p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Design</h3><p>The design of our API includes three main components, CephArmor, Crypt, and KMS. CephArmor is the intermediary interface between a user and other elements, mapping user commands to method and class calls. Crypt defines the sub-module performing cryptographic operations. KMS manages all aspects of the encryption key lifecycles. <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a> illustrates the main communication sequence in the proposed design.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda3-3227384-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda3-3227384-small.gif\" alt=\"FIGURE 3. - Secure write request communication sequence on a replicated pool.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Secure write request communication sequence on a replicated pool.</p></fig></div><p class=\"links\"><a href=\"/document/9973310/all-figures\" class=\"all\">Show All</a></p></div></p><p>When issuing a write, if the user is authenticated, the command proceeds to other components; otherwise, a message is displayed. A user-key negotiation between MON, MDS, and OSD daemons performs the authentication procedure. Suppose the user issues a write request to the CephArmor interface. If authorized, the API forwards a key request to the KMS module. The user-based encryption key is generated and sent back to the API. CephArmor forwards plain text, key, and IV to the Crypt module and receives cipher text. Then, it dispatches cipher text and the required cluster information for storage to the Rados module. Rados issues a write to the primary OSD, and the primary OSD directs the object into the allocated OSDs for replication. The CRUSH algorithm manages replacement locations and object mapping into the OSDs. Once the primary OSD receives the acknowledgment from the replica OSDs, it forwards an acknowledgment message to Rados. The write confirmation message moves backward to the upper layers until the user receives a notice approving the completion of the write request. A read request has a similar communication sequence model with minor differences. Once authenticated, a user can issue a read from CephArmor. The API requests the object from Rados and sends a decryption key request to the KMS module. KMS calculates the decryption key and transfers the key back to the API. Rados reviews the primary OSD, returning the object to the CephArmor if the object exists; otherwise, alternative OSDs will be searched. The object is then sent to the Crypt module for decryption, forwarded back to CephArmor, and then to the user.</p><p>In Ceph, the replication method is determined through pool abstraction. To have a more detailed description, the read/write operations on replicated and EC pools have been depicted over the CephArmor design model in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>. In a replicated write mode, the object becomes encrypted and forwarded to the initial OSD and then copied to the alternative OSDs. In EC write, after encryption, the object splits into <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> chunks, the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> parity is added, and all fragments are dispatched to the OSDs. The object is retrieved from the primary OSD in a replicated read mode, passes the decryption process, and returns to the user. In EC read, on the other hand, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> chunks should be retrieved from OSDs to create the primary object, decrypted, and returned to the user. Regardless of pool type, size, or configuration, there is only one encryption/decryption per write/read operation.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda4-3227384-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda4-3227384-small.gif\" alt=\"FIGURE 4. - Erasure vs. replicated pools data encryption (read/write).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Erasure vs. replicated pools data encryption (read/write).</p></fig></div><p class=\"links\"><a href=\"/document/9973310/all-figures\" class=\"all\">Show All</a></p></div></p><p>The CephArmor API is designed over the Rados layer through the librados library. Typically, two approaches exist, Command Line Interface (CLI) and Graphical User Interface (GUI), for direct communication to a Ceph cluster. In CLI mode, a user directly sends commands to a cluster through a terminal, while in GUI mode, a dashboard is designed for interaction purposes. All supported cryptography procedures are applicable via the CLI method in this study. The proposed API includes three main components, CephArmor, KMS, and Crypt. All three members are implemented in C++, and integrated into the Ceph build system.</p><p><b>CephArmor</b> represents the main component directly interacting with users, and other layers to validate constraints, such as input files and cluster variables. Then according to a request, encryption/decryption are instantiated, and calls are processed. In addition to the main storage functionalities, the performance evaluation functions are located within this module.</p><p><b>Crypt</b> represents the cryptographic component implemented with the OpenSSL library in C++. OpenSSL provides initial security services and functionalities applicable in various cryptographic algorithms. AES encryption/decryption are the primary operations provided by the Crypt component. Ceph internally utilizes customized APIs and structures the I/O purposes, i.e., IoCtx, and <i>bufferlist</i>. The former defines a Rados structure for managing I/Os over pools, such as constructing or inspecting the existence of a pool. The <i>bufferlist</i> structure represents a singly linked list for storing data in memory until the I/O operation is finalized on disk. The encryption function from the Crypt class is called in a write request, which in turn outputs a cipher text. Then, the cipher text is prepared for storage on disk through the IoCtx class. Similarly, in a read request, the cipher text retrieved from the OSD, becomes decrypted by the <i>Crypt</i> module, and the plain text is returned as an output. From the engineering point of view, we tried to maintain the added components\u2019 independence where possible from the Ceph core source code. This fact facilitates future development, maintenance, and source control processes.</p><p><b>KMS</b> manages the cryptographic key production and lifecycle. In all cryptographic methods, managing the lifecycle of the key plays an essential role in preserving the security of data and as a result preventing data leakage. Cryptographic keys are generated based on the user passphrase in the proposed design. The phrase is employed as a seed in the secure hash function for the key generation; keys are encrypted by a master key, and stored in Hardware Security Modules (HSMs) on the KMS machine. HSMs represent a Federal Information Processing Standards (FIPS) 140\u20132 Level 2 designed to protect cryptographic keys <a ref-type=\"bibr\" anchor=\"ref52\" id=\"context_ref_52_4b\">[52]</a>. A similar KMS design and architecture has been employed by Amazon Web Services (AWS) <a ref-type=\"bibr\" anchor=\"ref53\" id=\"context_ref_53_4b\">[53]</a>, Microsoft Azure <a ref-type=\"bibr\" anchor=\"ref54\" id=\"context_ref_54_4b\">[54]</a>, and Google Cloud <a ref-type=\"bibr\" anchor=\"ref55\" id=\"context_ref_55_4b\">[55]</a>. Issuing a secure operation, CephArmor sends the user phrase to the KMS component employing a Transport Layer Security (TLS) connection. KMS generates the cryptographic key employing the phrase as a seed, comparing the key with the user record information, and if validated, sends the key to CephArmor. Otherwise, CephArmor requests passphrase re-submission. The user can enter the passphrase up to three times; after that, the user will be guided to the reset password page. KMS is located in a virtual private network, and all communications between CephArmor and KMS are encrypted through the TLS protocol ensuring confidentiality and message integrity <a ref-type=\"bibr\" anchor=\"ref56\" id=\"context_ref_56_4b\">[56]</a>.</p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Experiments</h2></div><p>All experiments of this study are conducted through three server nodes. Commands can be directly issued to any servers connected with a screen and an input device as Ceph provides a uniform storage namespace; however, in our design, a separate system forwards commands through an SSH connection to a server, indicating a more real-world scenario. A single KMS controls all keys\u2019 lifecycle procedures and provides the encryption/decryption key and IV. Storage overhead, execution time, bandwidth, average number of operations per seconds (IOPS), and average latency are evaluated metrics.</p><div class=\"section_2\" id=\"sec5a\"><h3>A. Evaluation Tool</h3><p>Ceph provides internal tools for performance evaluation through APIs, such as Rados and RBD, facilitating the performance analysis procedures. The Rados API issues read/write operations with various parameters, i.e., the number of threads, message size, and objects. A similar benchmark component was added to the CephArmor API to evaluate the encryption/decryption performance. Rados has no support for the number of object parameters in the read evaluation; however, a constraint was added to the code for this purpose in the CephArmor API. A buffer of strings with the requested size is generated in a write request and the encryption function is called. The write operation is executed on the OSDs, and a metadata file is stored, including the current benchmark information for future tracking. Technically, a write operation should be performed on the cluster in advance to perform a read benchmark operation. To clarify, for evaluating the read performance, we should have stored enough objects within a cluster so that we employ them in a read evaluation operation. In a write benchmark, we can pass a command line parameter to Rados preventing object removal after the write evaluation. The read benchmark fetches the metadata file, including all required information for a read operation, and conducts the benchmarking procedures.</p></div><div class=\"section_2\" id=\"sec5b\"><h3>B. Experimental Environment</h3><p>All experiments are evaluated on three nodes of 15-drive Storinator servers from 45Drives <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\mathrm{ TM}}$\n</tex-math></inline-formula>. Each includes, an 8X Intel \u24c7 Xeon CPU E5-2620 v4 @ 2.10GHz CPU, an X10DRL-i Supermicro base board, a 128GB DDR4 2667 MT/s RAM, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$15 6$\n</tex-math></inline-formula>TB Western Digital HDD drives, a 10-Gigabit X540-AT2 Intel \u24c7 Ethernet PCIe Card Controller, and four-level cache as L1d 32K, L1i 32K, L2 256K, L3 20480K.</p></div><div class=\"section_2\" id=\"sec5c\"><h3>C. Cluster Environment</h3><p>Experiments are designed in two groups, replicated and EC pools. There is a positive correlation between data chunks and performance as data are striped and storage procedures are performed in parallel. In the EC pool titles, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> denotes the number of data chunks and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> the parity. Similarly, replication size denotes an indicator of fault tolerance in the former approach. <i>Replica2</i> and <i>Replica3</i> mirror two and three copies, including the object itself. Any pool type includes 64 placement groups with OSDs defined as the crush rule scope. Three MONs in quorum oversee and coordinate storage operations.</p></div><div class=\"section_2\" id=\"sec5d\"><h3>D. Experimental Design</h3><p>In the first set of experiments, single thread read/write is evaluated to reflect the effect of the encryption operation directly on the storage procedure. The multi-thread configuration includes two, four, and sixteen threads. Unless otherwise noted, we have two constant variables in all experiments, number of objects and object size, 1000 and 4MB, respectively. Each experiment runs ten times, the standard deviation is calculated, and the value is reported as collected in case of random error. Otherwise, the experiment is repeated, and the error calculation is analyzed. In this study, we employed AES CBC 256 to display the maximum possible overhead from the added API. According to the discussion on the power of computing, users can decide to apply a smaller key size.</p></div></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VI.</div><h2>Results</h2></div><p>The read/write operation performance is evaluated through the CephArmor API. In addition to the four metrics, storage overhead is reported as a critical factor in the storage systems. We assessed the AES performance on our servers over 4MB messages. The encryption and decryption elapsed time were collected as 12.79 and 1.94 milliseconds, respectively. In the first series of experiments, the EC and replicated pools with equal resiliency levels are compared. The microbenchmark outcome (<a ref-type=\"sec\" anchor=\"sec6c\" class=\"fulltext-link\">Section VI-C</a>) provides a detailed performance analysis against further pool configurations.</p><div class=\"section_2\" id=\"sec6a\"><h3>A. Write Evaluation</h3><p>The level of parallelism is adjustable in a storage operation through the multi-thread read/write. The single-thread mode read/write was analyzed to scrutinize the effect of the encryption/decryption on the storage operations. Despite running each experiment on ten seeds and reporting the average value, time defines a less accurate metric in the Ceph storage system. Typically, unpredictable in-progress tasks, such as load balancing and daemon communications, in addition to hardware limitations, affect the execution time. In a replicated pool, all versions of the entire object are copied to the OSDs. In contrast, the EC pool replicates a chunk of data on an OSD. The waiting time for a complete write process in the EC is shorter than the replicated pool (as shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4</a>), affecting the performance. <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a> illustrates the results of the single- and multi-thread write experiments.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda5-3227384-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda5-3227384-small.gif\" alt=\"FIGURE 5. - Write benchmark.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Write benchmark.</p></fig></div><p class=\"links\"><a href=\"/document/9973310/all-figures\" class=\"all\">Show All</a></p></div></p><p>The ECK8M2 has the lowest elapsed time compared to other pools in the single-thread write, encrypted, and plain mode (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5a</a>). Increasing the number of threads up to four amortizes the overhead and reduces the execution time remarkably; afterwards, the improvement is negligible. Greater throughput is desirable in storage systems. The ECK8M2 displayed the best bandwidth usage in single-thread mode. Adding the number of threads, the ECK4M2 illustrates improvement similar to ECK8M2. In all configurations, the replicated format has a lower throughput (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5c</a>). The highest IOPS belongs to the sixteen-threads mode on ECK8M2 pool, with 32 and 31 operations per second in plain and encrypted formats, respectively (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5d</a>).</p><p>The IOPS illustrates the number of operations per second, here objects, while the throughput specifies the transferred bytes of data per second. Although IOPS and throughput measure different metrics, they typically follow a similar pattern. Increasing or decreasing the object size affects the IOPS and throughput directly.</p><p>The average latency in the single-thread method has the best performance in all configurations. Increasing the number of threads to sixteen boosts the idle time, leading to higher latency (<a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5b</a>).</p><div class=\"section_2\" id=\"sec6a1\"><h4>1) Storage Overhead</h4><p>AES encryption increases data size by the nearest multiple of sixteen. In encryption with a 16 byte (or 128 bit) block size, the plain text is divided into equal partitions of 128 bits that would lead to a last incomplete block of data. The block is filled with the values according to a padding scheme. Therefore, the maximum added size resulting from padding is 15 bytes, regardless of object size. To this end, the maximum achievable storage overhead in a replicated pool is equal to (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$objects \\times padding \\times replication\\_{}size$\n</tex-math></inline-formula>), where <i>objects</i> indicates the number of objects, <i>padding</i>, the added overhead, and <i>replication_size</i>, the number of copies. Suppose we store one million objects, with any size and three replicas, the maximum overhead resulting from AES encryption would be <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1M \\times 15 \\times 3$\n</tex-math></inline-formula> bytes. The storage overhead in the EC pool correlates with the selected <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K\\text{s}$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula>. In addition to padding, higher values of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> lead to a lower size of parity and, as a result, lower storage overhead. The storage overhead of different pool schemes is illustrated in <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>. In this experiment, 50,000 objects of 4MB are stored in a pool. In ECK2M1, one object transforms into two data and one parity chunks. As displayed, the number of copies is three times more than the primary number of objects. The ECK12M2 demonstrates the most efficient format among all pool configurations.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nStorage Overhead on Write Operation: Replicated vs. Erasure Code Pools</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t4-3227384-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t4-3227384-small.gif\" alt=\"Table 4- &#10;Storage Overhead on Write Operation: Replicated vs. Erasure Code Pools\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div></div><div class=\"section_2\" id=\"sec6b\"><h3>B. Read Evaluation</h3><p>The sequential and random read operations are typically evaluated in a read benchmark process, illustrated in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6</a>. Random data access has lower efficiency than the sequential model due to the extra disk seek operations. While object storage outperforms the EC pool, the replicated pool performs better on object retrieval. Issuing a read on a replicated pool produces only one access to the primary OSD versus the EC pool creating <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> accesses to retrieve data chunks and rebuilding the primary object, requiring more processing time and overhead. Due to the lower impact of decryption time, 1.94 milliseconds, compared to the encryption, a nuanced difference between plain and encrypted reads has been achieved in both sequential and random reads elapsed time (<a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6a</a>). Similar to the write operation, the random and sequential read bandwidth (<a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6c</a>), average IOPS (<a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6d</a>), and average latency (<a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6b</a>), negatively correlate with execution time. We can determine thread four as the breaking point of the current storage system. From that point, the performance improvement is negligible due to the underlying communications or hardware limitations, such as disk bandwidth. In all metrics, the replicated pool has better performance. The latency of the EC pool is remarkably high in the sixteen threads operation. Based on the experimental results, increasing the concurrency level by more than four has a low impact on the execution time enhancement in both sequential and random reads.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda6-3227384-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda6-3227384-small.gif\" alt=\"FIGURE 6. - Sequential and random read benchmark.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Sequential and random read benchmark.</p></fig></div><p class=\"links\"><a href=\"/document/9973310/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec6c\"><h3>C. Microbenchmarks</h3><p><a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a> illustrates the microbenchmark outcome representing the difference between encrypted and normal operations on different pool types and configurations. Positive numbers represent a higher normal value; conversely, negative content indicates a greater encrypted figure. The eight most adapted pool types in the storage domain are selected for evaluation. If we categorize pools into groups based on resiliency level, <i>Replica2</i> and <i>ECK2M1</i> represent level one, <i>Replica3</i>, <i>ECK4M2</i>, <i>ECK8M2</i>, and <i>ECK12M2</i> demonstrate level two, and <i>ECK8M4</i> and <i>ECK12M4</i> constitute the level three group. Comparing write requests, in the first group, ECK2M1 outperforms Replica2 on elapsed time, throughput, IOPS, and average latency. Despite the constant number of encryptions/decryptions per read/write, ECK12M2 indicates the best performance in the second group and ECK12M4 in the third. We observe a low overhead on the collected IOPS and latency, while the throughput illustrated a higher difference. For read requests, on the other hand, the cryptographic overhead is negligible in all configurations. The average elapsed time difference between normal and encrypted format stays under <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\approx 3.5$\n</tex-math></inline-formula> seconds.<div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nMicrobenchmark Evaluation: Encrypted vs. Normal Read/Write Difference Over EC and Replicated Pools</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t5-3227384-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973310/khoda.t5-3227384-small.gif\" alt=\"Table 5- &#10;Microbenchmark Evaluation: Encrypted vs. Normal Read/Write Difference Over EC and Replicated Pools\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec6d\"><h3>D. Performance Analysis</h3><p>Cryptographic processes and Ceph underlying algorithms affect the storage performance. In an encryption process, each sub-block depends on a value from the previous block. As a result, the encryption performs sequentially, influencing the storage elapsed time. The decryption of blocks, on the other hand, proceeds independently, providing the parallel decryption on sub-modules, as a result improving the retrieval process time. The CRUSH algorithm determines object placement according to the defined replication domain. The algorithm dispatches objects to different OSDs if the OSD is specified as the failure domain. Suppose the algorithm stores all chunks or copies on one server, resulting in no network communication, lower execution time, and higher performance. Despite various configuration options for cluster tuning, the underlying complexities and operations are not user configurable, resulting in different performance behavior. The other influential factor in the experimental results is mapping the placement groups. Despite the replication domain, if the assigned OSDs to a PG are located in different network servers, the storage operation demands network communication, affecting the performance.</p></div></div>\n<div class=\"section\" id=\"sec7\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VII.</div><h2>Security Analysis</h2></div><p>Opponents in this system can be attackers from outside of the system or users inside the cluster storage with malicious objectives. For the sake of simplicity we name these entities <i>outsiders</i> and <i>insiders</i>, respectively. The threat model in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> revealed the system vulnerabilities without any confidentiality protocol in place. We elaborate on the addressed security threats by adding the proposed security interface.</p><div class=\"section_2\" id=\"sec7a\"><h3>A. Attack Shared Resources</h3><p>Pools are shared resources with manageable access control. Multiple users can coexist within a storage cluster and access the same pool contents. All users can manipulate all objects within a pool without any access control policy. In this scenario, as CephArmor has a user-based design, users can only decrypt their own data. A read request on a shared pool will not operate unless the user is the object\u2019s owner. During the execution time, each user has its memory, and others can not access the runtime content within the memory and registers. The context belonging to a user will be discarded by the end of execution.</p></div><div class=\"section_2\" id=\"sec7b\"><h3>B. Data Theft and Physical Access</h3><p>Outsiders can break the authentication, attack the storage, or interleave the connection and eavesdrop on the channel to steal data. Data are transferred in an encrypted format between the user and the storage medium. The outsider perceives no meaningful information by eavesdropping the network connection similarly to insiders. Data are stored in an encrypted format within the storage medium. Access to physical drives reveals no meaningful information.</p></div><div class=\"section_2\" id=\"sec7c\"><h3>C. Data Modification</h3><p>Adding the CephArmor layer to the Ceph storage system protects data confidentiality. Outsiders or insiders cannot obtain meaningful information by any form of attack. Data manipulation, however, demands other security techniques. Adding message authentication code helps us to detect data manipulation, but still, we need another techniques to prevent this type of threat.</p></div><div class=\"section_2\" id=\"sec7d\"><h3>D. Insider Attack</h3><p>A series of assaults applied by employees of an organization or individuals who directly access systems are considered malicious insider attacks\u2013one of the most challenging security issues <a ref-type=\"bibr\" anchor=\"ref57\" id=\"context_ref_57_7d\">[57]</a>. Discretionary Access Control (DAC) and Mandatory Access Control (MAC) are two approaches for preventing malicious insider vulnerabilities <a ref-type=\"bibr\" anchor=\"ref57\" id=\"context_ref_57_7d\">[57]</a>, <a ref-type=\"bibr\" anchor=\"ref58\" id=\"context_ref_58_7d\">[58]</a>. The authorization is granted through capabilities in Ceph. Administrators of the Ceph system can define rules and access policies. With MAC, the administrator of a Ceph cluster defines access levels through management items such as MONs or data storage daemons and pools. With DAC, the permitted operations, such as read, write, and execute, are granted to a user or group of users. Preventing malicious insider attacks depends on the proper access grants and limitations. The proposed design is compliant with Ceph authorization mechanisms. CephArmor communicates with MONs and MGRs to grant access to an operation or a storage zone.</p></div></div>\n<div class=\"section\" id=\"sec8\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VIII.</div><h2>Future Work</h2></div><p>This study integrated a secure API within the Ceph source code, examining the primary behavior. This paper presents the first iteration of an evolutionary project.</p><p>In this study, we assumed that the AES algorithm is strong enough to tolerate all types of known attacks on the implementation with the power of the current system. In this category, intruders abuse a system\u2019s behavior or physical parameters, such as timing information, electromagnetic signals, memory sharing, and power consumption, to collect information about secrets rather than breaking an algorithm. Timing attacks, Electromagnetic (EM) attacks, Simple Power Analysis (SPA), Differential Power Analysis (DPA), and Template attacks define some known side-channel attacks <a ref-type=\"bibr\" anchor=\"ref59\" id=\"context_ref_59_8\">[59]</a>, <a ref-type=\"bibr\" anchor=\"ref60\" id=\"context_ref_60_8\">[60]</a>, <a ref-type=\"bibr\" anchor=\"ref61\" id=\"context_ref_61_8\">[61]</a>, <a ref-type=\"bibr\" anchor=\"ref62\" id=\"context_ref_62_8\">[62]</a>. A timing attack tracks the performance time of a certain cryptographic operation to obtain critical information. SPA measures the power consumption of the cryptosystem operations to profile the sequence of instructions and exploit the hidden information. DPA employs the SPA method with many data and statistical analyses to reveal the secret. Various hardware and software approaches are proposed to prevent or mitigate side-channel attacks, such as dynamic binary rewriting <a ref-type=\"bibr\" anchor=\"ref63\" id=\"context_ref_63_8\">[63]</a>, adaptive compiler approach <a ref-type=\"bibr\" anchor=\"ref64\" id=\"context_ref_64_8\">[64]</a>, obscuring alternatives in code <a ref-type=\"bibr\" anchor=\"ref65\" id=\"context_ref_65_8\">[65]</a> and masking <a ref-type=\"bibr\" anchor=\"ref66\" id=\"context_ref_66_8\">[66]</a>, <a ref-type=\"bibr\" anchor=\"ref67\" id=\"context_ref_67_8\">[67]</a>, <a ref-type=\"bibr\" anchor=\"ref68\" id=\"context_ref_68_8\">[68]</a>. <i>Raccoon</i> defines an obfuscated execution approach proposed by Rane et al. <a ref-type=\"bibr\" anchor=\"ref69\" id=\"context_ref_69_8\">[69]</a> to address various types of side-channel attacks. A profiled code for Raccoon has obfuscated execution direction and memory access. The next iteration of the AES implementation will be integrated with Raccoon and masking methods to address the side-channel attack vector.</p><p>Malicious activities, such as data tampering, can undermine data confidentiality. In data tampering, the attacker manipulates or destroys the original data. In the cryptographic system, decryption provides incorrect plain text if an attacker tampers with encrypted values. In some cases, the cipher text is not reversible to plain text due to this activity. In this study we assumed an <i>honest-but-curious</i> model. To release this assumption, other methods, such as Message Authentication Codes (MACs), are required to prevent data tampering and protect data integrity <a ref-type=\"bibr\" anchor=\"ref70\" id=\"context_ref_70_8\">[70]</a>. The next generation of the CephArmor API will include additional security algorithms, such as AES CTR, CCM, and GCM. The AES GCM adds an extra security layer by providing message authentication. A user can then decide what operation mode to select based on the environment and other requirements. In the experiments, the number of objects and data size are two constant parameters, and the number of threads vary from one to sixteen to evaluate the system\u2019s behaviour. However, we are interested in assessing the system in the presence of different object sizes, specifically more than 4MB. More what-if scenarios would reveal interesting information in future research. Comparing CephArmor with client-side and server-side encryption APIs, RGW and HDFS would provide a comparative perspective on overhead and performance.</p></div>\n<div class=\"section\" id=\"sec9\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IX.</div><h2>Conclusion</h2></div><p>Ceph has been well adopted in data-intensive domains due to its storage management capabilities. Security, on the other hand, is not thoroughly addressed in the system. Maintained data in a plain format on the storage devices raises security issues if an adversary accesses the storage by any means. This study proposed a secure, lightweight API over the Rados layer to mitigate the data vulnerability at the repository. CephArmor addresses data confidentiality by preserving data encrypted on OSDs, reducing the probability of data leakage. The API facilitates object encryption functionality, algorithm selection, and encryption configuration, such as key strength to the end-users. A user applies the secure API without dealing with underlying complexities as all required functionalities have been encapsulated, and the final product is an off-the-shelf API. The proposed method has been integrated with the Ceph stable version 16.2.7 and evaluated through a commercial environment, 45Drives <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$^{\\mathrm{ TM}}$\n</tex-math></inline-formula> Storinators, including three server nodes, forty five hard drives, and numerous microbenchmarks. Experiments were performed over EC and replicated pools with different configurations. The results display an execution time overhead on the write operation; however, increasing the concurrency level by four amortizes the cost over the multiple threads and, as a result, mitigates the impact of the extra elapsed time. Due to the small decryption time, the read operation displayed a nuanced execution time overhead. In future research, we have scheduled different strategies to overcome encryption time in write operations while preserving security strength.</p></div>\n<h3>ACKNOWLEDGMENT</h3><p>The authors would like to express special appreciation to Stephen MacKay for his professional guidance in improving the quality of the current research document.</p></div></div>\n"
}