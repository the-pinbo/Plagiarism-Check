{
    "abstract": "Automatic target recognition (ATR) is of increasing importance for the modern radar system, where the high-resolution range profile (HRRP) is essential. However, the recognition accuracy and sensitivity to dataset should be optimized for practical applications. Herein, we proposed a novel algorithm for HRRP target recognition based on a convolutional capsule network rather than neurons in conventi...",
    "articleNumber": "9973295",
    "articleTitle": "Radar Target Recognition by Convolutional Capsule Networks Based on High-Resolution Range Profile",
    "authors": [
        {
            "preferredName": "Xianwen Zhang",
            "normalizedName": "X. Zhang",
            "firstName": "Xianwen",
            "lastName": "Zhang",
            "searchablePreferredName": "Xianwen Zhang"
        },
        {
            "preferredName": "Wenying Wang",
            "normalizedName": "W. Wang",
            "firstName": "Wenying",
            "lastName": "Wang",
            "searchablePreferredName": "Wenying Wang"
        },
        {
            "preferredName": "Xuanxuan Zheng",
            "normalizedName": "X. Zheng",
            "firstName": "Xuanxuan",
            "lastName": "Zheng",
            "searchablePreferredName": "Xuanxuan Zheng"
        },
        {
            "preferredName": "Yao Wei",
            "normalizedName": "Y. Wei",
            "firstName": "Yao",
            "lastName": "Wei",
            "searchablePreferredName": "Yao Wei"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227404",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9973295/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Modern radar system requires not only detecting and tracking the target but also recognizing the identity of the target accurately. Thus, Automatic Target Recognition (ATR) which utilizes radar echo to recognize the unknown targets is of vital importance for the development of modern radar systems. Synthetic aperture radar (SAR) <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, Inverse SAR (ISAR) <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, high-resolution range profile (HRRP) <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, etc. carry the identification information of targets and thus become the main tendency in the field of ATR. Among them, HRRP represents the scatter distribution along the radar line-of-sight (LOS) as shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>, containing the structure and electromagnetic properties of the target, and is suitable for ATR, especially for the ground-based radar system. Particularly, HRRP is a one-dimensional signal and has the superiority of convenient measurement and efficient processing over other signals, which attracts intensive attention from researchers.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang1-3227404-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang1-3227404-small.gif\" alt=\"FIGURE 1. - Illustration of an HRRP from a plane target, where the circles on the plane represent scatters.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Illustration of an HRRP from a plane target, where the circles on the plane represent scatters.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>Early HRRP recognition studies focused on extracting manually designed features including scatter distribution <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>, spectra <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>, bispectra <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, power spectra <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, cepstrum <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, etc. However, these methods were of less robustness and poor generalizability due to dependence on personal experience and skill. With the development of machine learning (ML) in recent years, the ATR method based on ML became the mainstream. Dictionary learning <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a>, Principal components analysis (PCA) <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a>, Factor analysis (FA) <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>, etc. were proposed to extract abstract features by learning from data and recognized HRRP in feature space. However, they had poor ability to learn a complicated and non-linear representation of HRRP due to the shallow architecture and linear operation. Deep learning (DL) based methods showed great potential recently. Auto-encoders <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a>, convolutional neural networks (CNN) <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>, recurrent neural network (RNN) <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a> were studied and achieved state-of-art performance. The convolutional layer played a fundamental role among all the deep learning methods, and various novel neural network architectures with convolutional layers were proposed for HRRP recognition. For instance, the one-dimensional residual-inception network composed of the convolutional kernel and pooling layers <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\">[18]</a>, the combination of CNN and bi-directional RNN (Bi-RNN) with attention mechanism <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a>, and the transfer learning architecture with CNN layers for incomplete targets <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\">[19]</a>. However, convolutional layers were commonly connected by max-pooling or average pooling layers and easily discarded the information which might be valuable for the recognition purpose. CNNs were also proved to be less efficient in capturing the part-whole relationship between the entities in the image <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a>.</p><p>To address the drawbacks of CNN, Capsule Network (CapsNet) was introduced by Sabour et al. <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a> with the ability to encode the part-whole relationship. Capsules were vectors containing several neurons to capture latent features and the dynamic routing mechanism determined information transferred between two capsule layers <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a>, <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\">[21]</a>, <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\">[22]</a>. CapsNet achieved state-of-the-art performance in various tasks and fields, such as the Fast CapsNet <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1\">[23]</a> for lung cancer screening task, the convolutional-deconvolutional CapsNet <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\">[24]</a> to segment pathological lungs from CT scans, and the convolutional CapsNet <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1\">[25]</a> for apoptosis classification. The capsules network architecture has advantages in target recognition compared with CNN architecture, due to its stronger sensitivity to the overall features by learning part-whole relationships between various parts. Robust features against range translation, viewpoint variation and noise could be learned by the capsules network, indicating promising potentials for the HRRP recognition task.</p><p>In this study, we proposed a novel method for HRRP target recognition based on the combination of convolutional layers and capsules, which was referred to as the convolution capsule network (CCNet). The CCNet was composed of a convolutional encoder, capsules, and a de-convolutional decoder, where the convolutional dynamic routing mechanism was applied between capsule layers which shared parameters in different positions and tremendously reduced the model complexity. Our method was validated via measured HRRP data from airplanes and demonstrated its high performance in HRRP target recognition.</p><p>The main contributions of this paper could be concluded as follows:\n<ol><li><p>We proposed a novel method based on convolutional capsule networks to solve the HRRP recognition problem in the field of RATR, with improved recognition accuracy.</p></li><li><p>A encoder-decoder architecture was designed to extract adequate information from the HRRP and promote the robustness against noise.</p></li><li><p>Experiments on measured data was conducted and the results proved the excellent performance and promising potentials of our method.</p></li></ol></p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Methode</h2></div><div class=\"section_2\" id=\"sec2a\"><h3>A. Capsule Network</h3><p>The capsule was a vector containing several neurons that represented various pose parameters and the length of the vector was defined as the probability. In Capsule networks, capsules in lower and higher layers were connected by a dynamic routing mechanism. Considering <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$u_{i} $\n</tex-math></inline-formula> as the output of capsule <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> in the lower layer, it was first encoded by a transformation matrix <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W_{ij} $\n</tex-math></inline-formula> and the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$u_{i} $\n</tex-math></inline-formula> was converted to:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\hat {u}_{j\\vert i}=W_{ij}u_{i}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\hat {u}_{j\\vert i}=W_{ij}u_{i}\\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {u}_{j\\vert i}$\n</tex-math></inline-formula> was the prediction vector representing the belief of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> capsule in a lower layer about <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j^{\\mathrm {th}}$\n</tex-math></inline-formula> capsule in the higher layer, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W_{ij}$\n</tex-math></inline-formula> was learned by backward propagation.</p><p>The dynamic routing mechanism guaranteed that the outputs of the lower capsules were sent to the proper higher capsules through iteratively updating the routing coefficient. Let <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{ij}$\n</tex-math></inline-formula> denoted the routing coefficient from <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> capsule in the lower layer to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j^{\\mathrm {th}}$\n</tex-math></inline-formula> capsule in the higher layer. The capsule activation of higher layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v_{j}$\n</tex-math></inline-formula> was computed as the sum of all predictions (denotes as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j}$\n</tex-math></inline-formula>) from all lower capsules with weights of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{ij}$\n</tex-math></inline-formula>, and then normalized through a squashing transformation, as shown in <a ref-type=\"disp-formula\" anchor=\"deqn2-deqn3\" href=\"#deqn2-deqn3\" class=\"fulltext-link\">equation (2) and (3)</a>.<disp-formula id=\"deqn2-deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} s_{j}=&amp;\\sum \\nolimits _{i} {c_{ij}\\hat {u}_{j\\vert i}} \\tag{2}\\\\ v_{j}=&amp;\\frac {\\left \\|{ s_{j} }\\right \\|^{2}}{1+\\left \\|{ s_{j} }\\right \\|^{2}}\\cdot \\frac {s_{j}}{\\left \\|{ s_{j} }\\right \\|}\\tag{3}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} s_{j}=&amp;\\sum \\nolimits _{i} {c_{ij}\\hat {u}_{j\\vert i}} \\tag{2}\\\\ v_{j}=&amp;\\frac {\\left \\|{ s_{j} }\\right \\|^{2}}{1+\\left \\|{ s_{j} }\\right \\|^{2}}\\cdot \\frac {s_{j}}{\\left \\|{ s_{j} }\\right \\|}\\tag{3}\\end{align*}\n</span></span></disp-formula> Since the length of the capsule vector was interpreted as the probability of a given feature being detected, squashing was necessary to ensure the length less than 1 without changing its direction.</p><p>The routing coefficient was iteratively updated by <a ref-type=\"disp-formula\" anchor=\"deqn4-deqn5\" href=\"#deqn4-deqn5\" class=\"fulltext-link\">equation (4) and (5)</a>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b_{ij} $\n</tex-math></inline-formula> was initialized to zero and tuned by a factor of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {u}_{j\\vert i}\\cdot v_{j}$\n</tex-math></inline-formula>. Thus, a lower capsule would send more information to the higher capsule whose output <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v_{j}$\n</tex-math></inline-formula> was more similar to its prediction <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {u}_{j\\vert i}$\n</tex-math></inline-formula>.<disp-formula id=\"deqn4-deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} b_{ij}=&amp;b_{ij}+\\hat {u}_{j\\vert i}\\mathrm {\\cdot }v_{j} \\tag{4}\\\\ c_{ij}=&amp;\\frac {e^{b_{ij}}}{\\sum \\nolimits _{k} e^{b_{ik}}}\\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} b_{ij}=&amp;b_{ij}+\\hat {u}_{j\\vert i}\\mathrm {\\cdot }v_{j} \\tag{4}\\\\ c_{ij}=&amp;\\frac {e^{b_{ij}}}{\\sum \\nolimits _{k} e^{b_{ik}}}\\tag{5}\\end{align*}\n</span></span></disp-formula></p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Convolutional Capsule Network</h3><p>In the fully connected CapsNet described above, each lower-higher capsule pair required a unique transformation matrix <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W_{ij}$\n</tex-math></inline-formula> learned by back-propagation and routing coefficient <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{ij}$\n</tex-math></inline-formula> updated iteratively. This made CapsNet had a large amount of trainable parameters and computationally expensive, and not suitable for the efficient ATR based on HRRP. Therefore, Convolutional CapsNet was proposed to address this problem with locally constrained dynamic routing and sharing trainable transformation matrix across different capsules.</p><p>Let the output of a lower capsule layer had the shape of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W^{l}\\times d^{l}\\times T^{l}$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W^{l}$\n</tex-math></inline-formula> was corresponding to HRRP length, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d^{l}$\n</tex-math></inline-formula> was the dimension of capsules, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$T^{l}$\n</tex-math></inline-formula> was the number of capsule types. The lower capsules were fed forward to higher capsule layers by convolutional dynamic routing. A higher capsule <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v_{j}^{(w)}$\n</tex-math></inline-formula> at location <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$w$\n</tex-math></inline-formula> of type <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> received information from lower capsules <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$u_{i}^{(x)}$\n</tex-math></inline-formula> in a corresponding region confined by a kernel of size <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k_{w}^{l}$\n</tex-math></inline-formula> as follows, <disp-formula id=\"deqn6-deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\hat {u}_{j\\vert i}^{\\left ({x }\\right)}=&amp;W_{ij}u_{i}^{\\left ({x }\\right)} \\tag{6}\\\\ s_{j}^{\\left ({w }\\right)}=&amp;\\sum \\nolimits _{x\\in \\left \\{{ w-k_{w}^{l}\\mathrm {,\\cdots,}w+k_{w}^{l} }\\right \\}} {c_{j}^{\\left ({x }\\right)}\\hat {u}_{\\left ({j\\thinspace \\vert \\thinspace i}\\right)}^{\\left ({x }\\right)}} \\tag{7}\\\\ v_{j}^{\\left ({w }\\right)}=&amp;\\frac {\\left \\|{ s_{j}^{\\left ({w }\\right)} }\\right \\|^{2}}{1+\\left \\|{ s_{j}^{\\left ({w }\\right)} }\\right \\|^{2}}\\cdot \\frac {s_{j}^{\\left ({w }\\right)}}{\\left \\|{ s_{j}^{\\left ({w }\\right)} }\\right \\|}\\tag{8}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\hat {u}_{j\\vert i}^{\\left ({x }\\right)}=&amp;W_{ij}u_{i}^{\\left ({x }\\right)} \\tag{6}\\\\ s_{j}^{\\left ({w }\\right)}=&amp;\\sum \\nolimits _{x\\in \\left \\{{ w-k_{w}^{l}\\mathrm {,\\cdots,}w+k_{w}^{l} }\\right \\}} {c_{j}^{\\left ({x }\\right)}\\hat {u}_{\\left ({j\\thinspace \\vert \\thinspace i}\\right)}^{\\left ({x }\\right)}} \\tag{7}\\\\ v_{j}^{\\left ({w }\\right)}=&amp;\\frac {\\left \\|{ s_{j}^{\\left ({w }\\right)} }\\right \\|^{2}}{1+\\left \\|{ s_{j}^{\\left ({w }\\right)} }\\right \\|^{2}}\\cdot \\frac {s_{j}^{\\left ({w }\\right)}}{\\left \\|{ s_{j}^{\\left ({w }\\right)} }\\right \\|}\\tag{8}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W_{ij}$\n</tex-math></inline-formula> was the transformation matrix for type <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> capsules in the lower layer to type <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> capsules in the higher layer and it was shared across all spatial locations. The input of higher capsule <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j}^{\\left ({w }\\right)}$\n</tex-math></inline-formula> was only computed by lower capsules <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {u}_{j\\vert i}^{\\left ({x }\\right)}$\n</tex-math></inline-formula> from the corresponding region:<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} x\\in \\left \\{{w-k_{w}^{l}\\mathrm {,\\cdots,}w+k_{w}^{l} }\\right \\}\\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} x\\in \\left \\{{w-k_{w}^{l}\\mathrm {,\\cdots,}w+k_{w}^{l} }\\right \\}\\tag{9}\\end{equation*}\n</span></span></disp-formula> Since capsules in the same location detected the same entity of the input HRRP though they were from different capsule types, the routing coefficient <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{ij}$\n</tex-math></inline-formula> was constrained to be equal and made an identical contribution to the higher layer accordingly as follows:<disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} c_{ij}^{\\left ({x }\\right)}=c_{kj}^{\\left ({x }\\right)}\\tag{10}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} c_{ij}^{\\left ({x }\\right)}=c_{kj}^{\\left ({x }\\right)}\\tag{10}\\end{equation*}\n</span></span></disp-formula> Thus, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{j}$\n</tex-math></inline-formula> was updated by <a ref-type=\"disp-formula\" anchor=\"deqn11-deqn12\" href=\"#deqn11-deqn12\" class=\"fulltext-link\">equation (11) and (12)</a>.<disp-formula id=\"deqn11-deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} b_{j}^{\\left ({x }\\right)}\\text {mean}_{i}=&amp;\\left ({b_{ij}^{\\left ({x }\\right)}+\\hat {u}_{j\\vert i}^{\\left ({x }\\right)}\\cdot v_{j}^{\\left ({w }\\right)} }\\right) \\tag{11}\\\\ c_{j}=&amp;\\frac {e^{b_{j}^{\\left ({x }\\right)}}}{\\sum \\nolimits _{k} e^{b_{k}^{\\left ({x }\\right)}}}\\tag{12}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} b_{j}^{\\left ({x }\\right)}\\text {mean}_{i}=&amp;\\left ({b_{ij}^{\\left ({x }\\right)}+\\hat {u}_{j\\vert i}^{\\left ({x }\\right)}\\cdot v_{j}^{\\left ({w }\\right)} }\\right) \\tag{11}\\\\ c_{j}=&amp;\\frac {e^{b_{j}^{\\left ({x }\\right)}}}{\\sum \\nolimits _{k} e^{b_{k}^{\\left ({x }\\right)}}}\\tag{12}\\end{align*}\n</span></span></disp-formula></p></div><div class=\"section_2\" id=\"sec2c\"><h3>C. Architecture of Proposed Network</h3><p>The architecture of CapsNet for HRRP target recognition is illustrated in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a>. Given the HRRP signal <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathrm {s\\in }R^{1\\times N}$\n</tex-math></inline-formula> A standard one-dimensional convolution layer with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$f$\n</tex-math></inline-formula> filters with the length of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula> and ReLU activation were first applied to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s$\n</tex-math></inline-formula> and converted the HRRP signal to feature maps denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathrm {X\\in }R^{f\\times N}$\n</tex-math></inline-formula>.<disp-formula id=\"deqn13\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} X_{ij}\\mathrm {=max}\\left ({\\mathrm {0,}\\sum \\nolimits _{k=j}^{j+l} {s_{k}\\cdot f_{ki}} }\\right)\\tag{13}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} X_{ij}\\mathrm {=max}\\left ({\\mathrm {0,}\\sum \\nolimits _{k=j}^{j+l} {s_{k}\\cdot f_{ki}} }\\right)\\tag{13}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$f_{ki}$\n</tex-math></inline-formula> represents the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k^{\\mathrm {th}}$\n</tex-math></inline-formula> element of the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i^{\\mathrm {th}}$\n</tex-math></inline-formula> filter and the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$l$\n</tex-math></inline-formula> is the length of the filter.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang2-3227404-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang2-3227404-small.gif\" alt=\"FIGURE 2. - Framework of capsule network for HRRP target recognition.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Framework of capsule network for HRRP target recognition.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>Then, the feature map <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula> was further fed into the primary capsule layer composed of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$T$\n</tex-math></inline-formula> paralleled convolutional layer with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d$\n</tex-math></inline-formula> filters. The feature map was transformed to <i>T-</i> type and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d$\n</tex-math></inline-formula>-dimensional capsules.</p><p>The third layer was the convolutional capsule layer with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$T$\n</tex-math></inline-formula>-type capsules where each of its capsules received information from only a certain grid of primary capsules. The last layer was referred as to the HRRP capsule (HrrpCaps) layer assigning one capsule per class. Convolutional dynamic routing was applied between capsule layers, encoding the input to the HRRP capsule whose length represents the possibility of the predicted class.</p><p>The decoder contained three de-convolution layers to reconstruct the HRRP input from the output capsule vector, encouraging the final HRRP capsules to carry adequate information from the input HRRP and alleviate the sensitivity to noise. We first masked the HRRP capsule vectors of the wrong prediction, and then fed them into the de-convolutional layers. The output of the decoder was resized to the identical size of input HRRP by setting proper filter size and stride.</p><p>The margin loss regulated by the reconstruction loss was adopted to train the proposed CCNet, which was computed as:<disp-formula id=\"deqn14-deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} L_{margin}=&amp;\\frac {1}{C}\\sum \\nolimits _{k=1}^{C} \\left [{ {\\begin{array}{l} y_{k}\\max \\left ({\\mathrm {0,}m^{+}-\\left \\|{ v_{k} }\\right \\| }\\right)^{2}+ \\\\ \\lambda \\left ({1-y_{k} }\\right)\\max \\left ({\\mathrm {0,}\\left \\|{ v_{k} }\\right \\|-m^{-} }\\right)^{2} \\\\ \\end{array}} }\\right] \\tag{14}\\\\ L_{rec}=&amp;\\left \\|{ x-x_{rec} }\\right \\|^{2} \\tag{15}\\\\ L=&amp;L_{margin}+\\alpha L_{rec}\\tag{16}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} L_{margin}=&amp;\\frac {1}{C}\\sum \\nolimits _{k=1}^{C} \\left [{ {\\begin{array}{l} y_{k}\\max \\left ({\\mathrm {0,}m^{+}-\\left \\|{ v_{k} }\\right \\| }\\right)^{2}+ \\\\ \\lambda \\left ({1-y_{k} }\\right)\\max \\left ({\\mathrm {0,}\\left \\|{ v_{k} }\\right \\|-m^{-} }\\right)^{2} \\\\ \\end{array}} }\\right] \\tag{14}\\\\ L_{rec}=&amp;\\left \\|{ x-x_{rec} }\\right \\|^{2} \\tag{15}\\\\ L=&amp;L_{margin}+\\alpha L_{rec}\\tag{16}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{k}\\in \\left \\{{0,1 }\\right \\}$\n</tex-math></inline-formula> is the label of class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$v_{k}$\n</tex-math></inline-formula> is the HRRP capsule vector of class <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C$\n</tex-math></inline-formula> is the number of class, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m^{+}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m^{-}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> are hyper-parameters.</p></div><div class=\"section_2\" id=\"sec2d\"><h3>D. Experiment Setup</h3><p>The proposed method was evaluated on measured HPPP data from airplane targets. The dataset information is summarized in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. The dataset contains six categories and 31k HRRP samples with imbalanced numbers. It should be noted that target A and B have a similar appearance.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nInformation of the HRRP Data Sets</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang.t1-3227404-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang.t1-3227404-small.gif\" alt=\"Table 1- &#10;Information of the HRRP Data Sets\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>Training set and test set came from different target flight batches. 70% of the flights were the training set and the remained samples were the testing set. Samples of each class were showed in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a>.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang3-3227404-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang3-3227404-small.gif\" alt=\"FIGURE 3. - HRRP samples of targets A to F.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>HRRP samples of targets A to F.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>Our method was compared with some recent advanced HRRP target recognition methods, including residual CNN (Res-CNN) <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2d\">[26]</a>, inception-based VGG (IVGG) <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2d\">[27]</a>, point-wise discriminative auto-encoder (PDAE) <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2d\">[28]</a> and some baseline neural networks including CNN and Multi-Layer Perceptron (MLP).</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Results</h2></div><p>The recognition accuracy of each target was first evaluated and the results were shown in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>. Comparing to other methods, the CCNet reached the highest overall accuracy and outperformed others in terms of recognition accuracy of targets B, C, D, E, and F.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nRecognition Accuracy Compared With Other Methods (%)</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang.t2-3227404-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang.t2-3227404-small.gif\" alt=\"Table 2- &#10;Recognition Accuracy Compared With Other Methods (%)\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>For a better understanding of the representative ability of CCNet, The HRRP capsule vectors of CCNet and the latent features of other competing methods were visualized by t-SNE, as shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a>. All the six targets were separately distributed except some samples in class A, B, D and F overlapped. Class C and E were relatively isolated. The distribution of the capsule vector was more concentrated than other methods, which was consistent with the recognition accuracy shown in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang4abcdef-3227404-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang4abcdef-3227404-small.gif\" alt=\"FIGURE 4. - t-SNE visualization of capsule vector of different classes. (a) CCNet; (b) CNN; (c) MLP; (d) IVGG; (e) PDAE; (f) Res-CNN.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>t-SNE visualization of capsule vector of different classes. (a) CCNet; (b) CNN; (c) MLP; (d) IVGG; (e) PDAE; (f) Res-CNN.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>Since CCNet improved the recognition rate on targets with fewer samples (D, E, and F), the sensitivity to the training size was further investigated. As depicted in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a>, CCNet maintained higher accuracy on the training dataset than CNN-based and Muti-Layer Perceptron (MLP) based methods, when the size of the training dataset was small. The recognition performance also showed less sensitivity to the size of training data as the change of accuracy is of stability.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang5-3227404-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang5-3227404-small.gif\" alt=\"FIGURE 5. - Sensitivity to training size of different deep learning methods.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Sensitivity to training size of different deep learning methods.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>The decoder of the networks was adopted to improve the robustness against noise and the recognition accuracy was compared with and without the decoder, as shown in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>. The results showed that the recognition accuracy was increased dramatically when the input SNR was lower than 15 dB, comparing to the network without the decoder.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang6-3227404-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang6-3227404-small.gif\" alt=\"FIGURE 6. - The recognition accuracy with and without the decoder.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>The recognition accuracy with and without the decoder.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>The HRRP capsule vectors learned sufficient information from the input HRRPs and were utilized to reconstruct the HRRP by the decoder. <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7</a> compared the input HRRP and the reconstructed HRRP. The main scatters of the reconstructed profile are close to the origins and the two profiles were of good similarity, with the RMS error of 0.025.\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang7-3227404-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang7-3227404-small.gif\" alt=\"FIGURE 7. - The input HRRP and reconstructed HRRP.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>The input HRRP and reconstructed HRRP.</p></fig></div><p class=\"links\"><a href=\"/document/9973295/all-figures\" class=\"all\">Show All</a></p></div></p><p>The CCNet had fewer parameters and was less computationally expensive than the conventional capsule network. <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a> compared the fully connected (FC) CapsNet and the CCNet in terms of trainable parameter size and recognition performance. The number of parameters of convolutional CpasNet reduced 2 orders of magnitude while the recognition accuracy was higher than FC-CapsNet, as shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nComparison Between Fully-Connected and Convolutional Capsule Networks</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang.t3-3227404-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9973295/zhang.t3-3227404-small.gif\" alt=\"Table 3- &#10;Comparison Between Fully-Connected and Convolutional Capsule Networks\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Discussion</h2></div><p>The convolutional capsule network for HRRP target recognition shows promising performance and reaches higher accuracy than other competing methods. The classification accuracy on most classes is improved while it decreases on class A whose representation might not be discriminatively learned due to its large data size. The CCNet shows less sensitivity to the training size than other deep learning methods, which implies its potential in application of few-shot learning problem. The reconstructed HRRP by the decoder is in accord with the original HRRP and maintained the character of dominant scatters, proving that the capsule vectors in the last layer are encoded with sufficient information of input data.</p><p>The distribution of capsule vector shown by <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a> approves the recognition performance. The vector of class A, B and D partially overlapped and class E and C are relatively isolated, which is in accordance with the classification accuracy in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>. As the HRRP was the projection of target\u2019s scatters into the lines of Radar sight, the angle between the direction of the airplane and the radar sight influenced the envelope of the HRRP. Thus, A HRRP with few peaks and short length might be measured from a large target, especially from the side view of the plane, inducing similar input HRRP and overlapped capsule vectors among different targets.</p><p>In the CCNet, with transformation matrix shared among different spatial location and the dynamic routing constrained in a small region, the trainable parameter number reduces tremendously and the routing becomes less computationally expensive. The convolution mechanism in capsule network is approved to be effective by the result in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>, as the recognition accuracy is slightly improved. The decoder of the CCNet improved the recognition performance for low SNR input signal. With the encoder and the reconstruction loss, the network was trained to learn sufficient information from the HRRP and to alleviate the sensitivity to the noise.</p><p>The real-time performance of the CCNet is improved compared to traditional capsule networks, with less parameters and computations. The processing speed is comparable to conventional neural networks such as CNN and MLP, showing its great potential in real-time application. Despite the lower recognition accuracy of class A, the overall performance is improved, especially for classes with less training samples. Therefore, the CCNet provides a novel approach for the practical application of RATR, with improved recognition performance and high computational efficiency.</p><p>There are some limitations in this study. the recognition accuracy on class with large sample size was not comparable with other classes, and the method is not interpretable, which might limit the scope of application of RART system. CapsNet has potentials in open set recognition and few-show learning, which should be studied in the future work.</p></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion</h2></div><p>In this paper, we proposed a novel HRRP target recognition method based on a convolutional capsule network. A capsule was a vector containing a group of neurons and its length is defined as the possibility of features been detected. CCNet shared the transformation matrix in different spatial locations and constrained the dynamic routing between layers in a small region. Our method outperformed other competing methods in terms of recognition accuracy and robustness to training size, and also reduced the size of trainable parameters and computational expense compared to the conventional capsule network. The results demonstrated that CCNet was a promising method for HRRP target recognition.</p></div>\n</div></div>\n"
}