{
    "abstract": "In practical process industries, the measurements coming from different sources are collected at different sampling rates, thereby soft sensors developed using uniformly sampled measurements may result in poor prediction performance. Besides, industrial processes are inherently stochastic and most of them present dynamic characteristic. To cope with these issues, a multi-rate probabilistic slow fe...",
    "articleNumber": "9978612",
    "articleTitle": "A Multi-Rate Probabilistic Slow Feature Regression Model for Dynamic Feature Learning and Industrial Soft Sensor Development",
    "authors": [
        {
            "preferredName": "Miao Zhang",
            "normalizedName": "M. Zhang",
            "firstName": "Miao",
            "lastName": "Zhang",
            "searchablePreferredName": "Miao Zhang"
        },
        {
            "preferredName": "Zhiwei Wen",
            "normalizedName": "Z. Wen",
            "firstName": "Zhiwei",
            "lastName": "Wen",
            "searchablePreferredName": "Zhiwei Wen"
        },
        {
            "preferredName": "Le Zhou",
            "normalizedName": "L. Zhou",
            "firstName": "Le",
            "lastName": "Zhou",
            "searchablePreferredName": "Le Zhou"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3228048",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9978612/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>In industrial processes, accurate measurement of quality variables is of great significance for process control, monitoring and optimization. At present, quality variables are mainly measured by online sampling and off-line analysis in the laboratory. The measurement process is time-consuming and the related detection equipment is expensive, so it is not conducive to realize the real-time monitoring and control of the industrial process <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. As a supplement of traditional measurement methods, soft sensor technology can solve this problem. Data-driven soft sensors can estimate quality variables accurately and economically by establishing mathematical models between key quality variables and easily measurable auxiliary variables <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>, <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>. With the wide application of distributed control system (DCS), a large amount of process data reflecting the real process state has been collected and stored. On this basis, various data-driven soft sensors have been developed and successfully applied in practical industrial processes <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>.</p><p>However, most traditional soft sensor methods assume a consistent sampling rate for all the observations. In most chemical processes, the sampling rates of the process and quality variables may vary among a large range. Those important quality variables are tested at the laboratory with a much lower sampling rate, such as the melt index of polypropylene, the content of butane, the endpoint of crude oil. They may be collected among a few hours or days. On the other hand, basic process variables such as pressure, flow rate and temperature, can be measured online using high-rate sensors, resulting in the multi-rate characteristic of the industrial process <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a>. The data imbalance between the quality variables and the process variables makes it challenging to build an accurate estimation model for quality variables.</p><p>For quality prediction, the traditional method is to convert the multi-rate sampling data into single-rate sampling data. Usually, the data preprocessing techniques include up-sampling and down-sampling. The up-sampling method uses high sampling rate data to estimate the uncollected data with low sampling rate by establishing a regression model. In the down-sampling method, all variables are recorded at the lowest sampling rate by subsampling. Lu et al. <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a> proposed a multi-rate dynamic modeling method for quality prediction at a faster rate based on the multiway partial least squares (PLS), in which the original dual sampled data is transformed into a three-dimensional matrix. Marjanovic et al. <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a> presented a real-time monitoring scheme of an industrial batch process using the down-sampling methods. However, there are still some limitations for these methods. The prediction accuracy of the up-sampling methods relies on the regression model and the down-sampling methods result in loss of significant data information and distortion of process dynamics.</p><p>Therefore, it is more appropriate to directly use the multi-sampling-rate measurements without down-sampling or up-sampling. Data fusion techniques have provided an effective way for directly using multi-sample rate data <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>. Under the conditions of multi-rate sampling, the data fusion technology based on Kalman filter is introduced into soft sensor maintenance, and the fusion of soft sensor model estimation and process measurement is realized <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a>. Huang et al. <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\">[18]</a> have used deep learning (DL) methods to fuse signals with different sampling rates and proposed a multi-rate sampling data fusion method for fault diagnosis. As an alternative, the semi-supervised methods have been used for quality prediction in the dual-rate process <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\">[19]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a>. Zhu et al. <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\">[21]</a> proposed a semi-supervised learning approach based on quantum statistic for industrial soft sensor development. Shao et al. <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\">[22]</a> developed a semi-supervised Dirichlet mixture of Gaussians models, in which a fully Bayesian model structure is designed to implement semi-supervised tasks. Jin et al. <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1\">[23]</a> presented a semi-supervised soft sensor using evolutionary optimization-based pseudo labeling. Lima et al. <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\">[24]</a> developed an industrial semi-supervised dynamic soft-sensor modeling approach based on deep relevant representation learning. Unfortunately, these semi-supervised methods only consider sampling rate inconsistencies between quality variables and process variables, and none of them can handle process data with three or more sampling rates. Recently, some of the semi-supervised probabilistic models have been extended to the multi-rate form. The multi-rate probabilistic principal component analysis (PPCA) and multi-rate factor analysis (MRFA) models have been developed for fault detection in multi-rate processes <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1\">[25]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_1\">[26]</a>. Moreover, Zhou et al. proposed a multi-rate principal component regression model (MRPCR) for soft sensor applications in chemical process <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_1\">[27]</a>.</p><p>Besides, in order to improve the prediction performance of the model for multi-rate process, it is necessary to consider the process dynamics. However, the traditional dynamic process modeling methods focus on uniformly sampled data sets <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_1\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_1\">[29]</a>. To address this issue, the state of each measurement type is estimated using two Kalman filters, and the estimates are fused considering the correlation between them in the next step <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_1\">[30]</a>. Aiming at the dual-rate characteristic of the system, Cao et al. <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_1\">[31]</a> used the separation of multi-dynamic and static characteristics to predict the quality variable. But these models based on dual-rate system cannot be extended to three or more sampling rate systems. Furthermore, as a dynamic extension of the MRFA model, Cong et al. <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_1\">[32]</a> proposed a multi-rate linear Gaussian state space model (MLGSS) for dynamic process monitoring. Although MLGSS can effectively extract dynamic latent variables in the multi-rate process, the model does not consider any output information. In the above literature, multi-rate dynamic modeling has found applications in fault detection and diagnosis. Its application to soft sensors has only gained recent attention with few industrial applications. Moreover, there are still some aspects which need to be improved. Firstly, we intend to extract the latent features of multi-rate processes that change slowly and reflect the internal trend of the process, because noise is usually included in the rapidly changing features. Secondly, in order to make full use of process information for quality prediction, the output information of the process is taken into account during the latent variable extraction. Finally, a more common model structure, model training procedure and the corresponding quality prediction strategy should be derived for any multi-rate dynamic processes.</p><p>To address these issues, the traditional probabilistic slow feature analysis is utilized and extended to its multi-rate form. And a multi-rate probabilistic slow feature regression (MR-PSFR) model is proposed for dynamic feature learning and industrial soft sensor development in this paper. Slow feature analysis (SFA) and probabilistic SFA (PSFA) are effective tools for dynamic modeling of industrial processes. By extracting latent features that vary slowly in time (i.e., slow features), they are able to capture all the dynamic information contained in the observations during modeling. Thus, a dimensionality reduction model with less noise can be obtained <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_1\">[33]</a>, <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_1\">[34]</a>, <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_1\">[35]</a>. In the proposed MR-PSFR model, the whole input and output observations with different sampling rates are used to extract slow features (SFs), where both the internal correlations between the individual sampling rate and the internal correlations between different sampling rates are all considered. Moreover, the SFs extracted from MR-PSFR can separate slowly and fast changing latent features, and they can have a better interpretation of the outputs. Next, the Expectation-Maximization (EM) algorithm is modified to derive the model parameters of MR-PSFR. Finally, based on the established model, the quality prediction strategy for multi-rate processes is constructed.</p><p>The rest of the paper is organized as follows. <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section 2</a> gives a brief introduction of SFA and PSFA. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section 3</a> presents a detailed explanation of the MR-PSFR model, followed by the model parameter estimation using EM method and the corresponding quality prediction strategy. <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section 4</a> shows two case studies on a numerical example and an industrial process application. Finally, conclusion is made in <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section 5</a>.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Related Work</h2></div><div class=\"section_2\" id=\"sec2a\"><h3>A. Slow Feature Analysis</h3><p>SFA is an unsupervised learning method proposed by Wiskott and Sejnowski <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2a\">[36]</a>. The core idea of SFA is to extract some of the most slowly changing components from the time series as essential features. It aims to find a set of nonlinear functions <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left \\{{{g_{j} (\\cdot),1\\le j\\le q\\;} }\\right \\}$\n</tex-math></inline-formula> to map a <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d$\n</tex-math></inline-formula>-dimensional input signal <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {x}}(t)$\n</tex-math></inline-formula> to a <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$q$\n</tex-math></inline-formula>-dimensional feature space. The outputs of these functions are called SFs, which are denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j} (t):=g_{j} ({\\mathbf {x}}(t))\\;(1\\le j\\le q)$\n</tex-math></inline-formula>. The SFA algorithm is to solve the following optimization problem:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\min \\limits _{g_{j} (\\cdot)} \\Delta (\\cdot):=\\min \\limits _{g_{j} (\\cdot)} \\left \\langle{ {\\dot {s}_{j}^{2} (t)} }\\right \\rangle _{t}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\min \\limits _{g_{j} (\\cdot)} \\Delta (\\cdot):=\\min \\limits _{g_{j} (\\cdot)} \\left \\langle{ {\\dot {s}_{j}^{2} (t)} }\\right \\rangle _{t}\\tag{1}\\end{equation*}\n</span></span></disp-formula> subject to <disp-formula id=\"deqn2-deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\left \\langle{ {s_{j} (t)} }\\right \\rangle _{t}=&amp;0,\\;\\;(\\textrm {zero}\\;\\textrm {mean}) \\tag{2}\\\\ \\left \\langle{ {s_{j}^{2} (t)} }\\right \\rangle _{t}=&amp;1,\\;\\;(\\textrm {unit}\\;\\textrm {variance}) \\tag{3}\\\\ \\forall i\\ne j,\\left \\langle{ {s_{i} (t)s_{j} (t)} }\\right \\rangle _{t}=&amp;0,\\;\\;(\\textrm {decorrelation}\\;\\textrm {and}\\;\\textrm {order})\\tag{4}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\left \\langle{ {s_{j} (t)} }\\right \\rangle _{t}=&amp;0,\\;\\;(\\textrm {zero}\\;\\textrm {mean}) \\tag{2}\\\\ \\left \\langle{ {s_{j}^{2} (t)} }\\right \\rangle _{t}=&amp;1,\\;\\;(\\textrm {unit}\\;\\textrm {variance}) \\tag{3}\\\\ \\forall i\\ne j,\\left \\langle{ {s_{i} (t)s_{j} (t)} }\\right \\rangle _{t}=&amp;0,\\;\\;(\\textrm {decorrelation}\\;\\textrm {and}\\;\\textrm {order})\\tag{4}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Delta (\\cdot)$\n</tex-math></inline-formula> represents an indicator of changing rate of features, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\dot {s}_{j} (t)=s_{j} (t)-s_{j} (t-1)$\n</tex-math></inline-formula> means the first-order time difference of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s(t)$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left \\langle{ \\cdot }\\right \\rangle _{t} $\n</tex-math></inline-formula> denotes the expectation over time. Constraints <a ref-type=\"disp-formula\" anchor=\"deqn2-deqn4\" href=\"#deqn2-deqn4\" class=\"fulltext-link\">(2) and (3)</a> ensure the normalization of the output signal and avoid the appearance of the constant value solution, whereas constraint <a ref-type=\"disp-formula\" anchor=\"deqn2-deqn4\" href=\"#deqn2-deqn4\" class=\"fulltext-link\">(4)</a> ensures that each component of the output signal is uncorrelated and avoids redundant signals.</p><p>When the mapping functions <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left \\{{{g_{j} (\\cdot),1\\le j\\le q\\;} }\\right \\}$\n</tex-math></inline-formula> are linear, the SFs can be derived in linear form:<disp-formula id=\"deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} {\\mathbf {s}}(t)={\\mathbf {W}}^{\\textrm {T}}{\\mathbf {x}}(t)\\tag{5}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} {\\mathbf {s}}(t)={\\mathbf {W}}^{\\textrm {T}}{\\mathbf {x}}(t)\\tag{5}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {W}}=[{\\mathbf {w}}_{1} \\;{\\mathbf {w}}_{2} \\;\\cdots {\\mathbf {w}}_{q}]\\in {\\rm \\mathbb {R}}^{d\\times q}$\n</tex-math></inline-formula> is the mapping matrix. When the number of SFs is the same as that of inputs, i.e. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d=m$\n</tex-math></inline-formula>, the above optimization solution problem is equivalent to the following generalized eigenvalue decomposition problem:<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\left \\langle{ {{{\\dot {\\mathbf {x}}\\dot {\\mathbf {x}}}}^{T}} }\\right \\rangle _{t} {\\mathbf {W}}=\\left \\langle{ {{\\mathbf { xx}}^{T}} }\\right \\rangle _{t} {\\mathbf {W\\Omega }}\\tag{6}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\left \\langle{ {{{\\dot {\\mathbf {x}}\\dot {\\mathbf {x}}}}^{T}} }\\right \\rangle _{t} {\\mathbf {W}}=\\left \\langle{ {{\\mathbf { xx}}^{T}} }\\right \\rangle _{t} {\\mathbf {W\\Omega }}\\tag{6}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Omega $\n</tex-math></inline-formula> is a diagonal matrix that contains the generalized eigenvalues <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\{\\omega _{j} \\}$\n</tex-math></inline-formula> on its diagonal, which are exactly the optimal values of objectives in <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">Eq. (1)</a>, that is, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Delta (\\cdot)=\\omega _{j} $\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {W}}=[{\\mathbf {w}}_{1} \\;{\\mathbf {w}}_{2} \\;\\cdots {\\mathbf {w}}_{q}]$\n</tex-math></inline-formula> is the corresponding generalized eigenvector matrix <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2a\">[37]</a>.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Probabilistic SFA</h3><p>SFA has been extended in a probabilistic framework to PSFA by Turner and Sahani <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2b\">[38]</a>. The model structure of PSFA is depicted in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> and the model formula is given as follows:<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\begin{cases} \\displaystyle {\\mathbf {s}(t)=\\mathbf {Fs}(t-1)+\\mathbf {e}_{{\\mathbf {s}}} (t)} \\\\ \\displaystyle {\\mathbf {x}(t)=\\mathbf {Hs}(t)+\\mathbf {e}_{{\\mathbf {x}}} (t)}  \\end{cases}\\tag{7}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\begin{cases} \\displaystyle {\\mathbf {s}(t)=\\mathbf {Fs}(t-1)+\\mathbf {e}_{{\\mathbf {s}}} (t)} \\\\ \\displaystyle {\\mathbf {x}(t)=\\mathbf {Hs}(t)+\\mathbf {e}_{{\\mathbf {x}}} (t)}  \\end{cases}\\tag{7}\\end{align*}\n</span></span></disp-formula> where the state transition matrix <b>F</b> is defined as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {F}=\\textrm {diag}\\{\\lambda _{1},\\cdots,\\lambda _{q} \\}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {H}\\in R^{d\\times q}$\n</tex-math></inline-formula> is the emission matrix, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {s}}} (t)$\n</tex-math></inline-formula> is the state noise and follows a Gaussian distribution <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{s} (t)\\sim N(\\mathbf {0}, \\boldsymbol {\\Lambda })$\n</tex-math></inline-formula>, where the covariance matrix <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\Lambda }={\\textrm {diag}}\\{1-\\lambda _{1}^{2},\\cdots,1-\\lambda _{q}^{2} \\}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {x}}} (t)$\n</tex-math></inline-formula> is the observation noise and follows a Gaussian distribution <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {x}}} (t)\\sim N(\\mathbf {0}, \\boldsymbol {\\Sigma })$\n</tex-math></inline-formula>, where the covariance matrix <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\Sigma }=\\text {diag}\\{\\sigma _{1}^{2},\\cdots,\\sigma _{d}^{2} \\}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sigma _{j}^{2} $\n</tex-math></inline-formula> is the variance of the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>th dimension SF.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang1-3228048-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang1-3228048-small.gif\" alt=\"FIGURE 1. - The model structure of PSFA.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>The model structure of PSFA.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div></p><p>The independence assumption of SFs reflects the decorrelation nature of constraint <a ref-type=\"disp-formula\" anchor=\"deqn2-deqn4\" href=\"#deqn2-deqn4\" class=\"fulltext-link\">(4)</a>. It can also be verified that the SFs derived from PSFA still satisfy the constraints <a ref-type=\"disp-formula\" anchor=\"deqn2-deqn4\" href=\"#deqn2-deqn4\" class=\"fulltext-link\">(2) and (3)</a>, which are <disp-formula id=\"deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} E[s_{j} (t)]=0,\\;Var\\{s_{j} (t)\\}=1,\\;1\\le j\\le q\\tag{8}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} E[s_{j} (t)]=0,\\;Var\\{s_{j} (t)\\}=1,\\;1\\le j\\le q\\tag{8}\\end{equation*}\n</span></span></disp-formula></p><p>The transition parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{j} $\n</tex-math></inline-formula>, which satisfies <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$0\\le \\lambda _{j} &lt; 1$\n</tex-math></inline-formula>, controls the correlation level between adjacent data points <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j} (t)$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j} (t-1)$\n</tex-math></inline-formula>. In fact, the indicator <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Delta (\\cdot)$\n</tex-math></inline-formula> is calculated as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Delta (s_{j})=2(1-\\lambda _{j})$\n</tex-math></inline-formula>, which means that the larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{j} $\n</tex-math></inline-formula> is, the stronger the correlation between <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j} (t)$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j} (t-1)$\n</tex-math></inline-formula> is. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{j} (t)$\n</tex-math></inline-formula> tends to have slower variation with a smaller <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Delta (\\cdot)$\n</tex-math></inline-formula>, and vice versa.</p><p>As can be seen from <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a>, the temporally correlated latent variables <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {s}(t)$\n</tex-math></inline-formula> are derived from input variables <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {x}(t)$\n</tex-math></inline-formula>, which can only capture the dynamics in <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {x}(t)$\n</tex-math></inline-formula> <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2b\">[33]</a>, <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2b\">[39]</a>. It is noticed that the output observations could also contain dynamics which are beneficial to the prediction of future outputs. Moreover, the PSFA is trained using the uniformly sampling data. When faced with the problem of multiple sampling rates, quality prediction based on the PSFA model is not satisfactory. Based on the above considerations, the traditional PSFA is extended to its multi-sampling-rate form with consideration of output information in the next section.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Mr-PSFR Based Quality Prediction Method</h2></div><div class=\"section_2\" id=\"sec3a\"><h3>A. Multi-Rate Probabilistic Slow Feature Regression Model</h3><p>Suppose a multi-rate process contains <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula> kinds of sampling rates for the process variables <b>X</b> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> kinds of sampling rates for the quality variables <b>Y</b>. Given a multi-rate data set <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\{{\\mathbf {X}},{\\mathbf {Y}}\\}=\\{{\\mathbf {x}}_{t} \\in R^{t},{\\mathbf {y}}_{t} \\in R^{t}\\}_{t=1}^{T} $\n</tex-math></inline-formula>, the multi-rate probabilistic slow feature regression model (MR-PSFR) is given as <disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\begin{cases} \\displaystyle {\\mathbf {s}_{t} =\\mathbf {Fs}_{t-1} +\\mathbf {e}_{{\\mathbf {s}}}} \\\\ \\displaystyle {\\mathbf {x}_{t}^{(m)} =\\mathbf {H}^{(m)}\\mathbf {s}_{t} +\\mathbf {e}_{{\\mathbf {x}}}^{(m)},\\;m=1,2,\\cdots,M} \\\\ \\displaystyle {\\mathbf {y}_{t}^{(n)} =\\mathbf {U}^{(n)}\\mathbf {s}_{t} +\\mathbf {e}_{{\\mathbf {y}}}^{(n)},\\;n=1,2,\\cdots,N}  \\end{cases}\\tag{9}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\begin{cases} \\displaystyle {\\mathbf {s}_{t} =\\mathbf {Fs}_{t-1} +\\mathbf {e}_{{\\mathbf {s}}}} \\\\ \\displaystyle {\\mathbf {x}_{t}^{(m)} =\\mathbf {H}^{(m)}\\mathbf {s}_{t} +\\mathbf {e}_{{\\mathbf {x}}}^{(m)},\\;m=1,2,\\cdots,M} \\\\ \\displaystyle {\\mathbf {y}_{t}^{(n)} =\\mathbf {U}^{(n)}\\mathbf {s}_{t} +\\mathbf {e}_{{\\mathbf {y}}}^{(n)},\\;n=1,2,\\cdots,N}  \\end{cases}\\tag{9}\\end{align*}\n</span></span></disp-formula> in which <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {x}_{t}^{(m)} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {y}_{t}^{(n)} $\n</tex-math></inline-formula> denote observations at sample time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> from different sampling rates. The observations of process variables at sampling time are denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {x}}_{t} =\\left [{ {{\\mathbf { x}}\\left ({{o_{t}^{1}} }\\right)^{T}\\;{\\mathbf {x}}\\left ({{o_{t}^{2}} }\\right)^{T}\\;\\cdots \\;{\\mathbf {x}}\\left ({{o_{t}^{k_{t}}} }\\right)^{T}} }\\right]^{T}$\n</tex-math></inline-formula>, in which <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$o_{t}^{1} $\n</tex-math></inline-formula> to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$o_{t}^{k_{t}} $\n</tex-math></inline-formula> represent different sampling rates and it is readily to obtain that <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1\\le o_{t}^{1} &lt; o_{t}^{2} &lt; \\cdots &lt; o_{t}^{k_{t}} \\le M$\n</tex-math></inline-formula>. Similarly, the measurements of quality variables are denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {y}}_{t} =\\left [{ {{\\mathbf {y}}\\left ({{b_{t}^{1}} }\\right)^{T}\\;{\\mathbf {y}}\\left ({{b_{t}^{2}} }\\right)^{T}\\;\\cdots \\;{\\mathbf {y}}\\left ({{b_{t}^{J_{t}}} }\\right)^{T}} }\\right]^{T}$\n</tex-math></inline-formula>, in which <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b_{t}^{1} $\n</tex-math></inline-formula> to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$b_{t}^{J_{t}} $\n</tex-math></inline-formula> represents different sampling rates of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {y}}_{t}$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1\\le b_{t}^{1} &lt; b_{t}^{2} &lt; \\cdots &lt; b_{t}^{J_{t}} \\le N$\n</tex-math></inline-formula>. The key factor of the MR-PSFR model is the slow feature <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {s}_{t} \\in R^{q}$\n</tex-math></inline-formula>, which follows Gaussian distribution. The slow feature <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {s}_{t} $\n</tex-math></inline-formula> is determined and shared by all the multi-rate measurements. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {F}=\\textrm {diag}\\{\\lambda _{1},\\cdots,\\lambda _{q} \\}$\n</tex-math></inline-formula> is the state transition matrix, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {H}^{(m)}\\in R^{d(m)\\times q}\\;(m=1,2,\\cdots,M)$\n</tex-math></inline-formula> are the emission matrix of the process variables under each sampling rate, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {U}^{(n)}\\in R^{d(n)\\times q}\\;(n=1,2,\\cdots,N)$\n</tex-math></inline-formula> are the emission matrix of the quality variables under each sampling rate, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d(m)$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d(n)$\n</tex-math></inline-formula> represent the variable dimensions of process variables and quality variables at different sampling rates, respectively. The state noise <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {s}}} $\n</tex-math></inline-formula> follows a Gaussian distribution <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {s}}} \\sim N(\\mathbf {0}, \\boldsymbol {\\Lambda })$\n</tex-math></inline-formula>, where the covariance matrix <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\Lambda }=\\textrm {diag}\\{1-\\lambda _{1}^{2},\\cdots,1-\\lambda _{q}^{2} \\}$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {x}}}^{(m)} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {y}}}^{(n)} $\n</tex-math></inline-formula> are Gaussian noises of the process variables and quality variables, which are <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {x}}}^{(m)} \\sim N({\\mathbf {0}},{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)}),\\;m=1,2,\\cdots,M$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {y}}}^{(n)} \\sim N({\\mathbf {0}},{\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(n)}),\\;n=1,2,\\cdots,N$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(n)} $\n</tex-math></inline-formula> are the noise variances.</p><p>The model structure diagram of MR-PSFR is depicted in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M=2$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N=2$\n</tex-math></inline-formula> are specified as an example. The whole observations with different sampling rates are used to extract SFs, where both the internal correlations between the individual sampling rate and the internal correlations between different sampling rates are all considered. Moreover, the extracted SFs take into account the output information of the process and can have a better interpretation of the output. The unknown model parameter set is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Theta }}=\\left \\{{{\\lambda _{j},1\\le j\\le q,\\;\\mathbf {H}^{(m)},{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)},\\mathbf {U}^{(n)},{\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(n)}} }\\right \\}$\n</tex-math></inline-formula>, (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m=1,2,\\cdots,M$\n</tex-math></inline-formula>; <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n=1,2,\\cdots,N$\n</tex-math></inline-formula>). In the next section, the EM algorithm is modified to derive the model parameters of MR-PSFR.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang2-3228048-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang2-3228048-small.gif\" alt=\"FIGURE 2. - MR-PSFR model structure diagram.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>MR-PSFR model structure diagram.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Model Parameter Estimation Using EM</h3><p>The model parameters <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Theta }}=\\left \\{{{\\lambda _{j},1\\le j\\le q,\\;\\mathbf {H}^{(m)},{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)},\\mathbf {U}^{(n)},{\\boldsymbol{\\Gamma }}_{{\\mathbf { y}}}^{(n)}} }\\right \\}$\n</tex-math></inline-formula>, (<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m=1,2,\\cdots,M$\n</tex-math></inline-formula>; <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n=1,2,\\cdots,N$\n</tex-math></inline-formula>) in MR-PSFR are derived using the EM algorithm. Given observations <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$D_{o} =\\{{\\mathbf {X}},{\\mathbf {Y}}\\}=\\{{\\mathbf { x}}_{1},\\cdots,{\\mathbf {x}}_{T},{\\mathbf {y}}_{1},\\cdots,{\\mathbf {y}}_{T} \\}$\n</tex-math></inline-formula> and latent variables <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$D_{hid} =\\{{\\mathbf {s}}_{1},\\cdots,{\\mathbf {s}}_{T} \\}$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {x}}_{t} =\\left [{ {{\\mathbf {x}}\\left ({{o_{t}^{1}} }\\right)^{T}\\;{\\mathbf {x}}\\left ({{o_{t}^{2}} }\\right)^{T}\\;\\cdots \\;{\\mathbf {x}}\\left ({{o_{t}^{k_{t}}} }\\right)^{T}} }\\right]^{T}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {y}}_{t} =\\left [{ {{\\mathbf {y}}\\left ({{b_{t}^{1}} }\\right)^{T}\\;{\\mathbf {y}}\\left ({{b_{t}^{2}} }\\right)^{T}\\;\\cdots \\;{\\mathbf {y}}\\left ({{b_{t}^{J_{t}}} }\\right)^{T}} }\\right]^{T}$\n</tex-math></inline-formula>, the global log-likelihood of the complete data is derived as <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">(10)</a>, shown at the bottom of the page. <disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-1pc}\\ln p\\left ({{{\\mathbf {X}},{\\mathbf {Y}},{\\mathbf {s}}\\vert {\\boldsymbol{\\Theta }}} }\\right) \\\\=&amp;\\ln \\begin{cases} \\displaystyle {p\\left ({{{\\mathbf {s}}_{1}} }\\right)p\\left ({{{\\mathbf {x}}_{1}^{(o_{1}^{1})},\\cdots,{\\mathbf {x}}_{1}^{(o_{1}^{k_{1}})} \\vert {\\mathbf {s}}_{1}} }\\right)p\\left ({{{\\mathbf {y}}_{1}^{(b_{1}^{1})},\\cdots,{\\mathbf {y}}_{1}^{(b_{1}^{J_{1}})} \\vert {\\mathbf { s}}_{1}} }\\right)} \\\\ \\displaystyle {\\mathop \\prod \\nolimits _{t=2}^{T} \\;p\\left ({{\\mathbf {s}_{t} \\vert \\mathbf {s}_{t-1}} }\\right)p\\left ({{{\\mathbf {x}}_{t}^{(o_{t}^{1})},\\cdots,{\\mathbf {x}}_{t}^{(o_{t}^{k_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)p\\left ({{{\\mathbf {y}}_{t}^{(b_{t}^{1})},\\cdots,{\\mathbf {y}}_{t}^{(b_{t}^{J_{t}})} \\vert {\\mathbf { s}}_{t}} }\\right)}  \\end{cases} \\\\=&amp;\\ln p\\left ({{{\\mathbf {s}}_{1}} }\\right)+\\sum \\limits _{t=2}^{T} {\\ln p\\left ({{\\mathbf {s}_{t} \\vert \\mathbf {s}_{t-1}} }\\right)} +\\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {x}}_{t}^{(o_{t}^{1})},\\cdots,{\\mathbf {x}}_{t}^{(o_{t}^{k_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)} \\\\&amp;+\\,\\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {y}}_{t}^{(b_{t}^{1})},\\cdots,{\\mathbf { y}}_{t}^{(b_{t}^{J_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)}\\tag{10}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-1pc}\\ln p\\left ({{{\\mathbf {X}},{\\mathbf {Y}},{\\mathbf {s}}\\vert {\\boldsymbol{\\Theta }}} }\\right) \\\\=&amp;\\ln \\begin{cases} \\displaystyle {p\\left ({{{\\mathbf {s}}_{1}} }\\right)p\\left ({{{\\mathbf {x}}_{1}^{(o_{1}^{1})},\\cdots,{\\mathbf {x}}_{1}^{(o_{1}^{k_{1}})} \\vert {\\mathbf {s}}_{1}} }\\right)p\\left ({{{\\mathbf {y}}_{1}^{(b_{1}^{1})},\\cdots,{\\mathbf {y}}_{1}^{(b_{1}^{J_{1}})} \\vert {\\mathbf { s}}_{1}} }\\right)} \\\\ \\displaystyle {\\mathop \\prod \\nolimits _{t=2}^{T} \\;p\\left ({{\\mathbf {s}_{t} \\vert \\mathbf {s}_{t-1}} }\\right)p\\left ({{{\\mathbf {x}}_{t}^{(o_{t}^{1})},\\cdots,{\\mathbf {x}}_{t}^{(o_{t}^{k_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)p\\left ({{{\\mathbf {y}}_{t}^{(b_{t}^{1})},\\cdots,{\\mathbf {y}}_{t}^{(b_{t}^{J_{t}})} \\vert {\\mathbf { s}}_{t}} }\\right)}  \\end{cases} \\\\=&amp;\\ln p\\left ({{{\\mathbf {s}}_{1}} }\\right)+\\sum \\limits _{t=2}^{T} {\\ln p\\left ({{\\mathbf {s}_{t} \\vert \\mathbf {s}_{t-1}} }\\right)} +\\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {x}}_{t}^{(o_{t}^{1})},\\cdots,{\\mathbf {x}}_{t}^{(o_{t}^{k_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)} \\\\&amp;+\\,\\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {y}}_{t}^{(b_{t}^{1})},\\cdots,{\\mathbf { y}}_{t}^{(b_{t}^{J_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)}\\tag{10}\\end{align*}\n</span></span></disp-formula></p><p>Assuming that the prior distribution of SFs is standard Gaussian distribution: <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p({\\mathbf {s}}_{1})=N({\\mathbf {0}},{\\mathbf {I}}_{q})$\n</tex-math></inline-formula>, the first term in <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">Eq. (10)</a> is derived as <disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\ln p\\left ({{{\\mathbf {s}}_{1}} }\\right)=\\textrm {const}-\\frac {1}{2}{\\mathbf {s}}_{1}^{T} {\\mathbf {s}}_{1}\\tag{11}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\ln p\\left ({{{\\mathbf {s}}_{1}} }\\right)=\\textrm {const}-\\frac {1}{2}{\\mathbf {s}}_{1}^{T} {\\mathbf {s}}_{1}\\tag{11}\\end{equation*}\n</span></span></disp-formula></p><p>Then, the second term in <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">Eq. (10)</a> is calculated as <disp-formula id=\"deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\sum \\limits _{t=2}^{T} {\\ln p\\left ({{\\mathbf {s}_{t} \\vert \\mathbf {s}_{t-1}} }\\right)}=&amp;\\textrm {const}-\\frac {T-1}{2}\\sum \\limits _{j=1}^{q} {\\log \\left ({{1-\\lambda { }_{j}^{2}} }\\right)} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=2}^{T} \\sum \\limits _{j=1}^{q} \\frac {1}{1-\\lambda {}_{j}^{2}}\\Bigg ({s_{j} (t)}\\\\&amp;{-\\,\\lambda _{j} s_{j} (t-1) }\\Bigg){}^{2} \\tag{12}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\sum \\limits _{t=2}^{T} {\\ln p\\left ({{\\mathbf {s}_{t} \\vert \\mathbf {s}_{t-1}} }\\right)}=&amp;\\textrm {const}-\\frac {T-1}{2}\\sum \\limits _{j=1}^{q} {\\log \\left ({{1-\\lambda { }_{j}^{2}} }\\right)} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=2}^{T} \\sum \\limits _{j=1}^{q} \\frac {1}{1-\\lambda {}_{j}^{2}}\\Bigg ({s_{j} (t)}\\\\&amp;{-\\,\\lambda _{j} s_{j} (t-1) }\\Bigg){}^{2} \\tag{12}\\end{align*}\n</span></span></disp-formula></p><p>For the process variables, it can be proved that the covariance of the conditional distribution of the input observed samples with respect to the latent vector can be expressed as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}^{(o_{t})}=\\textrm {diag}\\left ({{{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(o_{t}^{1})};\\;{\\boldsymbol{\\Sigma }}_{{\\mathbf { x}}}^{(o_{t}^{2})} \\;;\\;\\cdots \\;;\\;{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(o_{t}^{k_{t}})}} }\\right)$\n</tex-math></inline-formula>. Similarly, for the quality variables, the covariance of the conditional distribution of the output observed samples with respect to the latent vector is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Gamma }}^{(b_{t})}=\\textrm {diag}\\left ({{{\\boldsymbol{\\Gamma }}_{{\\mathbf { y}}}^{(b_{t}^{1})};\\;{\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(b_{t}^{2})} \\;;\\;\\cdots \\;;\\;{\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(b_{t}^{J_{t}})}} }\\right)$\n</tex-math></inline-formula>. Then, the third term and fourth term in <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">Eq. (10)</a> are derived as<disp-formula id=\"deqn13-deqn14\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-1pc}\\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {x}}_{t}^{(o_{t}^{1})},\\cdots,{\\mathbf {x}}_{t}^{(o_{t}^{k_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)} \\\\=&amp;\\textrm {const}-\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Sigma }}^{(o_{t})}} }\\right |} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\left [{ {\\left ({{{\\mathbf {x}}_{t} -{\\mathbf {H}}^{(o_{t})}\\mathbf {s}_{t}} }\\right)^{T}\\left ({{{\\boldsymbol{\\Sigma }}^{(o_{t})}} }\\right)^{-1}\\left ({{{\\mathbf {x}}_{t} -{\\mathbf {H}}^{(o_{t})}\\mathbf {s}_{t}} }\\right)} }\\right]} \\tag{13}\\\\&amp;\\hspace {-1pc} \\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {y}}_{t}^{(b_{t}^{1})},\\cdots,{\\mathbf {y}}_{t}^{(b_{t}^{J_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)} \\\\=&amp;\\textrm {const}-\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Gamma }}^{(b_{t})}} }\\right |} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\left [{ {\\left ({{{\\mathbf {y}}_{t} -\\mathbf {U}^{(b_{t})}\\mathbf {s}_{t}} }\\right)^{T}\\left ({{{\\boldsymbol{\\Gamma }}^{(b_{t})}} }\\right)^{-1}\\left ({{{\\mathbf {y}}_{t} -\\mathbf {U}^{(b_{t})}\\mathbf {s}_{t}} }\\right)} }\\right]} \\tag{14}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-1pc}\\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {x}}_{t}^{(o_{t}^{1})},\\cdots,{\\mathbf {x}}_{t}^{(o_{t}^{k_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)} \\\\=&amp;\\textrm {const}-\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Sigma }}^{(o_{t})}} }\\right |} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\left [{ {\\left ({{{\\mathbf {x}}_{t} -{\\mathbf {H}}^{(o_{t})}\\mathbf {s}_{t}} }\\right)^{T}\\left ({{{\\boldsymbol{\\Sigma }}^{(o_{t})}} }\\right)^{-1}\\left ({{{\\mathbf {x}}_{t} -{\\mathbf {H}}^{(o_{t})}\\mathbf {s}_{t}} }\\right)} }\\right]} \\tag{13}\\\\&amp;\\hspace {-1pc} \\sum \\limits _{t=1}^{T} {\\ln p\\left ({{{\\mathbf {y}}_{t}^{(b_{t}^{1})},\\cdots,{\\mathbf {y}}_{t}^{(b_{t}^{J_{t}})} \\vert {\\mathbf {s}}_{t}} }\\right)} \\\\=&amp;\\textrm {const}-\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Gamma }}^{(b_{t})}} }\\right |} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} {\\left [{ {\\left ({{{\\mathbf {y}}_{t} -\\mathbf {U}^{(b_{t})}\\mathbf {s}_{t}} }\\right)^{T}\\left ({{{\\boldsymbol{\\Gamma }}^{(b_{t})}} }\\right)^{-1}\\left ({{{\\mathbf {y}}_{t} -\\mathbf {U}^{(b_{t})}\\mathbf {s}_{t}} }\\right)} }\\right]} \\tag{14}\\end{align*}\n</span></span></disp-formula> in which the emission matrix of the process variables and the quality variables are defined as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {H}}^{(o_{t})}=[\\mathbf {H}^{(o_{t}^{1})};\\;\\mathbf {H}^{(o_{t}^{2})};\\;\\cdots \\;;\\mathbf {H}^{(o_{t}^{k_{t}})}]$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {U}^{(b_{t})}=[\\mathbf {U}^{(b_{t}^{1})};\\;\\mathbf {U}^{(b_{t}^{2})};\\;\\cdots \\;;\\mathbf {U}^{(b_{t}^{J_{t}})}]$\n</tex-math></inline-formula>, respectively.</p><p>Substituting <a ref-type=\"disp-formula\" anchor=\"deqn11\" href=\"#deqn11\" class=\"fulltext-link\">Eqs. (11)</a><a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\"/>\u2013<a ref-type=\"disp-formula\" anchor=\"deqn13-deqn14\" href=\"#deqn13-deqn14\" class=\"fulltext-link\">(14)</a> into <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">Eq. (10)</a>, the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q$\n</tex-math></inline-formula>-function can be formally derived by considering the conditional expectation of <a ref-type=\"disp-formula\" anchor=\"deqn10\" href=\"#deqn10\" class=\"fulltext-link\">(10)</a> as <a ref-type=\"disp-formula\" anchor=\"deqn15\" href=\"#deqn15\" class=\"fulltext-link\">(15)</a>, shown at the bottom of the page, <disp-formula id=\"deqn15\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Q\\left ({{{\\boldsymbol{\\Theta }},{\\boldsymbol{\\Theta }}^{\\textrm {old}}} }\\right)=&amp;E_{{\\mathbf {s}}\\vert {\\mathbf {X}},{\\mathbf {Y}},{\\boldsymbol{\\Theta }}^{\\textrm {old}}} \\left \\{{{\\ln p\\left ({{{\\mathbf {X}},{\\mathbf {Y}},{\\mathbf {s}}\\vert {\\boldsymbol{\\Theta }}} }\\right)} }\\right \\} \\\\=&amp;\\textrm {const}-\\frac {1}{2}\\left \\{{{\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Sigma }}^{(o_{t})}} }\\right |} +\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Gamma }}^{(b_{t})}} }\\right |}} }\\right \\} \\\\&amp;-\\,E\\left \\{{{\\frac {T-1}{2}\\sum \\limits _{j=1}^{q} {\\log \\left ({{1-\\lambda { }_{j}^{2}} }\\right)} +\\frac {1}{2}\\sum \\limits _{t=2}^{T} {\\sum \\limits _{j=1}^{q} {\\frac {1}{1-\\lambda {}_{j}^{2}}\\left ({{s_{j} (t)-\\lambda _{j} s_{j} (t-1)} }\\right){}^{2}}}} }\\right \\} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} \\begin{cases} \\displaystyle {\\mathbf {x}}_{t}^{T} ({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {x}}_{t} -2({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {x}}_{t} E[\\mathbf {s}_{t}] \\\\ \\displaystyle +\\textrm {tr}\\left ({{({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}E[\\mathbf {s}_{t} \\mathbf {s}_{t} ^{T}]} }\\right)  \\end{cases} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} \\begin{cases} \\displaystyle {\\mathbf {y}}_{t}^{T} ({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}{\\mathbf {y}}_{t} -2(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}{\\mathbf {y}}_{t} E[\\mathbf {s}_{t}] \\\\ \\displaystyle +\\textrm {tr}\\left ({{(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}\\mathbf {U}^{(b_{t})}E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]} }\\right)  \\end{cases}\\tag{15}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Q\\left ({{{\\boldsymbol{\\Theta }},{\\boldsymbol{\\Theta }}^{\\textrm {old}}} }\\right)=&amp;E_{{\\mathbf {s}}\\vert {\\mathbf {X}},{\\mathbf {Y}},{\\boldsymbol{\\Theta }}^{\\textrm {old}}} \\left \\{{{\\ln p\\left ({{{\\mathbf {X}},{\\mathbf {Y}},{\\mathbf {s}}\\vert {\\boldsymbol{\\Theta }}} }\\right)} }\\right \\} \\\\=&amp;\\textrm {const}-\\frac {1}{2}\\left \\{{{\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Sigma }}^{(o_{t})}} }\\right |} +\\sum \\limits _{t=1}^{T} {\\ln \\left |{ {{\\boldsymbol{\\Gamma }}^{(b_{t})}} }\\right |}} }\\right \\} \\\\&amp;-\\,E\\left \\{{{\\frac {T-1}{2}\\sum \\limits _{j=1}^{q} {\\log \\left ({{1-\\lambda { }_{j}^{2}} }\\right)} +\\frac {1}{2}\\sum \\limits _{t=2}^{T} {\\sum \\limits _{j=1}^{q} {\\frac {1}{1-\\lambda {}_{j}^{2}}\\left ({{s_{j} (t)-\\lambda _{j} s_{j} (t-1)} }\\right){}^{2}}}} }\\right \\} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} \\begin{cases} \\displaystyle {\\mathbf {x}}_{t}^{T} ({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {x}}_{t} -2({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {x}}_{t} E[\\mathbf {s}_{t}] \\\\ \\displaystyle +\\textrm {tr}\\left ({{({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}E[\\mathbf {s}_{t} \\mathbf {s}_{t} ^{T}]} }\\right)  \\end{cases} \\\\&amp;-\\,\\frac {1}{2}\\sum \\limits _{t=1}^{T} \\begin{cases} \\displaystyle {\\mathbf {y}}_{t}^{T} ({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}{\\mathbf {y}}_{t} -2(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}{\\mathbf {y}}_{t} E[\\mathbf {s}_{t}] \\\\ \\displaystyle +\\textrm {tr}\\left ({{(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}\\mathbf {U}^{(b_{t})}E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]} }\\right)  \\end{cases}\\tag{15}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\Theta }^{\\textrm {old}}$\n</tex-math></inline-formula> represents the model parameters obtained in the previous iteration.</p><div class=\"section_2\" id=\"sec3b1\"><h4>1) M-Step</h4><p>In the M-step, the new parameters <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\Theta }^{\\textrm {new}}$\n</tex-math></inline-formula> can be calculated by maximizing the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q$\n</tex-math></inline-formula>-function:<disp-formula id=\"deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} {\\boldsymbol{\\Theta }}^{\\textrm {new}}=\\arg \\;\\max \\limits _{\\theta } \\;E_{{\\mathbf {s}}\\vert {\\mathbf { X}},{\\mathbf {Y}},{\\boldsymbol{\\Theta }}^{\\textrm {old}}} \\left \\{{{\\ln p\\left ({{{\\mathbf {X}},{\\mathbf {Y}},{\\mathbf {s}}\\vert {\\boldsymbol{\\Theta }}} }\\right)} }\\right \\}\\tag{16}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} {\\boldsymbol{\\Theta }}^{\\textrm {new}}=\\arg \\;\\max \\limits _{\\theta } \\;E_{{\\mathbf {s}}\\vert {\\mathbf { X}},{\\mathbf {Y}},{\\boldsymbol{\\Theta }}^{\\textrm {old}}} \\left \\{{{\\ln p\\left ({{{\\mathbf {X}},{\\mathbf {Y}},{\\mathbf {s}}\\vert {\\boldsymbol{\\Theta }}} }\\right)} }\\right \\}\\tag{16}\\end{equation*}\n</span></span></disp-formula></p><p>Taking the derivative of the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q$\n</tex-math></inline-formula>-function with respect to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{j}$\n</tex-math></inline-formula>, we derive <disp-formula id=\"deqn17\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\frac {\\partial Q}{\\partial \\lambda _{j}}=&amp;\\frac {(T-1)\\lambda _{j} }{1-\\lambda _{j}^{2}}-\\frac {\\lambda _{j}}{1-\\lambda _{j}^{2}}\\Bigg \\{{ \\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j}^{2} (t-1)} }\\right]}} \\\\&amp; {-\\,\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j} (t)s_{j} (t-1)} }\\right]} }\\Bigg \\} \\\\&amp;-\\,\\frac {\\lambda _{j}}{\\left ({{1-\\lambda _{j}^{2}} }\\right)^{2}}\\sum \\limits _{t=2}^{T} {E\\left [{ {\\left ({{s_{j} (t)-\\lambda _{j} s_{j} (t-1)} }\\right)^{2}} }\\right]} \\;=\\;0 \\tag{17}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\frac {\\partial Q}{\\partial \\lambda _{j}}=&amp;\\frac {(T-1)\\lambda _{j} }{1-\\lambda _{j}^{2}}-\\frac {\\lambda _{j}}{1-\\lambda _{j}^{2}}\\Bigg \\{{ \\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j}^{2} (t-1)} }\\right]}} \\\\&amp; {-\\,\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j} (t)s_{j} (t-1)} }\\right]} }\\Bigg \\} \\\\&amp;-\\,\\frac {\\lambda _{j}}{\\left ({{1-\\lambda _{j}^{2}} }\\right)^{2}}\\sum \\limits _{t=2}^{T} {E\\left [{ {\\left ({{s_{j} (t)-\\lambda _{j} s_{j} (t-1)} }\\right)^{2}} }\\right]} \\;=\\;0 \\tag{17}\\end{align*}\n</span></span></disp-formula></p><p>Further, the updating <a ref-type=\"disp-formula\" anchor=\"deqn17\" href=\"#deqn17\" class=\"fulltext-link\">equation (17)</a> can be simplified as:<disp-formula id=\"deqn18\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-2pc} (T-1)\\lambda _{j}^{3} -\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j} (t)s_{j} (t-1)} }\\right]} \\cdot \\lambda _{j}^{2} +\\bigg(\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j}^{2} (t)} }\\right]} \\\\&amp;+\\,\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j}^{2} (t-1)} }\\right]} -T+1\\bigg)\\lambda _{j} \\\\&amp;-\\,\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j} (t)s_{j} (t-1)} }\\right]} =0\\tag{18}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-2pc} (T-1)\\lambda _{j}^{3} -\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j} (t)s_{j} (t-1)} }\\right]} \\cdot \\lambda _{j}^{2} +\\bigg(\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j}^{2} (t)} }\\right]} \\\\&amp;+\\,\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j}^{2} (t-1)} }\\right]} -T+1\\bigg)\\lambda _{j} \\\\&amp;-\\,\\sum \\limits _{t=2}^{T} {E\\left [{ {s_{j} (t)s_{j} (t-1)} }\\right]} =0\\tag{18}\\end{align*}\n</span></span></disp-formula> By solving <a ref-type=\"disp-formula\" anchor=\"deqn18\" href=\"#deqn18\" class=\"fulltext-link\">Eq. (18)</a> and constraining the roots in the range [0, 1], the updated <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{j} $\n</tex-math></inline-formula> can be obtained.</p><p>Parameters <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {H}^{(m)},{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)},\\mathbf {U}^{(n)},{\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(n)} $\n</tex-math></inline-formula> are updated by taking derivative of the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q$\n</tex-math></inline-formula>-function with respect to each parameter and equating them to zero, which is given as <disp-formula id=\"deqn19-deqn22\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\mathbf {H}^{(m)\\textrm {new}}=&amp;\\left ({{\\sum \\limits _{(m)} {{\\mathbf {x}}_{t}^{(m)} E[\\mathbf {s}_{t}]}} }\\right)\\left ({{\\sum \\limits _{(m)} {E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]}} }\\right)^{-1} \\tag{19}\\\\ {\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)\\textrm {new}}=&amp;\\frac {1}{N^{(m)}}\\sum \\limits _{(m)} {\\left ({{\\begin{array}{l} {\\mathbf {x}}_{t}^{(m)} ({\\mathbf {x}}_{t}^{(m)})^{T}-2\\mathbf {H}^{(m)\\textrm {new}}E[\\mathbf {s}_{t}]({\\mathbf {x}}_{t}^{(m)})^{T} \\\\ +\\mathbf {H}^{(m)\\textrm {new}}E[\\mathbf {s}_{t} \\mathbf {s}_{t} ^{T}](\\mathbf {H}^{(m)\\textrm {new}})^{T} \\\\ \\end{array}} }\\right)} \\tag{20}\\\\ \\mathbf {U}^{(n)\\textrm {new}}=&amp;\\left ({{\\sum \\limits _{(n)} {{\\mathbf {y}}_{t}^{(n)} E[\\mathbf {s}_{t}]}} }\\right)\\left ({{\\sum \\limits _{(n)} {E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]}} }\\right)^{-1} \\tag{21}\\\\ {\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(n)\\textrm {new}}=&amp;\\frac {1}{N^{(n)}}\\sum \\limits _{(n)} {\\left ({{\\begin{array}{l} {\\mathbf {y}}_{t}^{(n)} ({\\mathbf {y}}_{t}^{(n)})^{T}-2\\mathbf {U}^{(n)\\textrm {new}}E[\\mathbf {s}_{t}]({\\mathbf {y}}_{t}^{(n)})^{T} \\\\ +\\mathbf {U}^{(n)\\textrm {new}}E[\\mathbf {s}_{t} \\mathbf {s}_{t} ^{T}](\\mathbf {U}^{(n)\\textrm {new}})^{T} \\\\ \\end{array}} }\\right)} \\tag{22}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\mathbf {H}^{(m)\\textrm {new}}=&amp;\\left ({{\\sum \\limits _{(m)} {{\\mathbf {x}}_{t}^{(m)} E[\\mathbf {s}_{t}]}} }\\right)\\left ({{\\sum \\limits _{(m)} {E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]}} }\\right)^{-1} \\tag{19}\\\\ {\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(m)\\textrm {new}}=&amp;\\frac {1}{N^{(m)}}\\sum \\limits _{(m)} {\\left ({{\\begin{array}{l} {\\mathbf {x}}_{t}^{(m)} ({\\mathbf {x}}_{t}^{(m)})^{T}-2\\mathbf {H}^{(m)\\textrm {new}}E[\\mathbf {s}_{t}]({\\mathbf {x}}_{t}^{(m)})^{T} \\\\ +\\mathbf {H}^{(m)\\textrm {new}}E[\\mathbf {s}_{t} \\mathbf {s}_{t} ^{T}](\\mathbf {H}^{(m)\\textrm {new}})^{T} \\\\ \\end{array}} }\\right)} \\tag{20}\\\\ \\mathbf {U}^{(n)\\textrm {new}}=&amp;\\left ({{\\sum \\limits _{(n)} {{\\mathbf {y}}_{t}^{(n)} E[\\mathbf {s}_{t}]}} }\\right)\\left ({{\\sum \\limits _{(n)} {E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]}} }\\right)^{-1} \\tag{21}\\\\ {\\boldsymbol{\\Gamma }}_{{\\mathbf {y}}}^{(n)\\textrm {new}}=&amp;\\frac {1}{N^{(n)}}\\sum \\limits _{(n)} {\\left ({{\\begin{array}{l} {\\mathbf {y}}_{t}^{(n)} ({\\mathbf {y}}_{t}^{(n)})^{T}-2\\mathbf {U}^{(n)\\textrm {new}}E[\\mathbf {s}_{t}]({\\mathbf {y}}_{t}^{(n)})^{T} \\\\ +\\mathbf {U}^{(n)\\textrm {new}}E[\\mathbf {s}_{t} \\mathbf {s}_{t} ^{T}](\\mathbf {U}^{(n)\\textrm {new}})^{T} \\\\ \\end{array}} }\\right)} \\tag{22}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m=1,2,\\cdots,M;\\;n=1,2,\\cdots,N$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N^{(m)}$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N^{(n)}$\n</tex-math></inline-formula> represent the sample numbers for the process variables and the quality variables at sampling rates <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula>, respectively.</p></div><div class=\"section_2\" id=\"sec3b2\"><h4>2) E-Step</h4><p>The parameter update formulas in M-step require three expectation terms: <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$E\\left [{ {{\\mathbf {s}}_{t}} }\\right]$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$E[\\mathbf {s}_{t} \\mathbf {s}_{t-1}^{T}]$\n</tex-math></inline-formula>. The forward filtering and backward smoothing algorithm are used to estimate the expectations of SFs. For the multi-rate data set, some modifications to the forward filtering algorithm are required. Given the posterior distribution of the SF at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t-1$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p({\\mathbf {s}}_{t-1} \\vert {\\mathbf {x}}_{1:t-1},{\\mathbf {y}}_{1:t-1})=N({\\mathbf {g}}_{t-1},{\\mathbf {G}}_{t-1})$\n</tex-math></inline-formula>, in which <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {g}}_{t-1} $\n</tex-math></inline-formula> is the mean value and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {G}}_{t-1} $\n</tex-math></inline-formula> is the covariance matrix, we need to calculate the posterior distribution of the SF at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p({\\mathbf {s}}_{t} \\vert {\\mathbf {x}}_{t},{\\mathbf {y}}_{t},{\\mathbf {x}}_{1:t-1},{\\mathbf {y}}_{1:t-1})$\n</tex-math></inline-formula>. According to the model structure of MR-PSFR, the joint probability distribution of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {s}}_{t} $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {x}}_{t} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {y}}_{t} $\n</tex-math></inline-formula> can be obtained:<disp-formula id=\"deqn23\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;\\hspace {-3pc}p({\\mathbf {s}}_{t},{\\mathbf {x}}_{t},{\\mathbf {y}}_{t} \\vert {\\mathbf {x}}_{1:t-1},{\\mathbf {y}}_{1:t-1}) \\\\=&amp;N\\left ({{\\left [{ {{\\begin{array}{cccccccccccccccccccc} { \\boldsymbol {\\mu }_{s}} \\\\ { \\boldsymbol {\\mu }_{x}} \\\\ { \\boldsymbol {\\mu }_{y}} \\\\ \\end{array}}} }\\right],\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\boldsymbol{\\Sigma }}_{ss}} &amp; {{\\boldsymbol{\\Sigma }}_{sx}} &amp; {{\\boldsymbol{\\Sigma }}_{sy}} \\\\ {{\\boldsymbol{\\Sigma }}_{xs}} &amp; {{\\boldsymbol{\\Sigma }}_{xx}} &amp; {{\\boldsymbol{\\Sigma }}_{xy}} \\\\ {{\\boldsymbol{\\Sigma }}_{ys}} &amp; {{\\boldsymbol{\\Sigma }}_{yx}} &amp; {{\\boldsymbol{\\Sigma }}_{yy}} \\\\ \\end{array}}} }\\right]} }\\right)\\tag{23}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;\\hspace {-3pc}p({\\mathbf {s}}_{t},{\\mathbf {x}}_{t},{\\mathbf {y}}_{t} \\vert {\\mathbf {x}}_{1:t-1},{\\mathbf {y}}_{1:t-1}) \\\\=&amp;N\\left ({{\\left [{ {{\\begin{array}{cccccccccccccccccccc} { \\boldsymbol {\\mu }_{s}} \\\\ { \\boldsymbol {\\mu }_{x}} \\\\ { \\boldsymbol {\\mu }_{y}} \\\\ \\end{array}}} }\\right],\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\boldsymbol{\\Sigma }}_{ss}} &amp; {{\\boldsymbol{\\Sigma }}_{sx}} &amp; {{\\boldsymbol{\\Sigma }}_{sy}} \\\\ {{\\boldsymbol{\\Sigma }}_{xs}} &amp; {{\\boldsymbol{\\Sigma }}_{xx}} &amp; {{\\boldsymbol{\\Sigma }}_{xy}} \\\\ {{\\boldsymbol{\\Sigma }}_{ys}} &amp; {{\\boldsymbol{\\Sigma }}_{yx}} &amp; {{\\boldsymbol{\\Sigma }}_{yy}} \\\\ \\end{array}}} }\\right]} }\\right)\\tag{23}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\mu }_{s} ={\\mathbf {Fg}}_{t-1} $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\mu }_{x} ={\\mathbf {H}}^{(o_{t})}{\\mathbf {Fg}}_{t-1} $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\boldsymbol {\\mu }_{y} =\\mathbf {U}^{(b_{t})}{\\mathbf {Fg}}_{t-1} $\n</tex-math></inline-formula>,</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{ss} ={\\mathbf {FG}}_{t-1} {\\mathbf {F}}^{T}+{\\boldsymbol{\\Lambda }}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{xx} ={\\mathbf {H}}^{(o_{t})}{\\boldsymbol{\\Sigma }}_{ss} ({\\mathbf {H}}^{(o_{t})})^{T}+{\\boldsymbol{\\Sigma }}^{(o_{t})}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{yy} =\\mathbf {U}^{(b_{t})}{\\boldsymbol{\\Sigma }}_{ss} (\\mathbf {U}^{(b_{t})})^{T}+{\\boldsymbol{\\Gamma }}^{(b_{t})}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{xs} ={\\mathbf {H}}^{(o_{t})}{\\boldsymbol{\\Sigma }}_{ss} $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{ys} =\\mathbf {U}^{(b_{t})}{\\boldsymbol{\\Sigma }}_{ss} $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{xy} ={\\mathbf {H}}^{(o_{t})}{\\boldsymbol{\\Sigma }}_{ss} (\\mathbf {U}^{(b_{t})})^{T}$\n</tex-math></inline-formula>.</p><p>Based on the Bayes rule, the expectation and variance of the posterior distribution <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p({\\mathbf {s}}_{t} \\vert {\\mathbf {x}}_{t},{\\mathbf {y}}_{t},{\\mathbf {x}}_{1:t-1},{\\mathbf { y}}_{1:t-1})$\n</tex-math></inline-formula> are derived as <disp-formula id=\"deqn24-deqn25\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {\\mathbf {g}}_{t}=&amp;\\boldsymbol {\\mu }_{s} +\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\boldsymbol{\\Sigma }}_{ss}^{-1} }\\Bigg)^{-1} \\\\&amp;\\times \\, \\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}({\\mathbf {x}}_{t} - \\boldsymbol {\\mu }_{x})} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}({\\mathbf {y}}_{t} - \\boldsymbol {\\mu }_{y}) }\\Bigg) \\tag{24}\\\\ {\\mathbf {G}}_{t}=&amp;\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\boldsymbol{\\Sigma }}_{ss}^{-1} }\\Bigg)^{-1}\\tag{25}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {\\mathbf {g}}_{t}=&amp;\\boldsymbol {\\mu }_{s} +\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\boldsymbol{\\Sigma }}_{ss}^{-1} }\\Bigg)^{-1} \\\\&amp;\\times \\, \\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}({\\mathbf {x}}_{t} - \\boldsymbol {\\mu }_{x})} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}({\\mathbf {y}}_{t} - \\boldsymbol {\\mu }_{y}) }\\Bigg) \\tag{24}\\\\ {\\mathbf {G}}_{t}=&amp;\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\boldsymbol{\\Sigma }}_{ss}^{-1} }\\Bigg)^{-1}\\tag{25}\\end{align*}\n</span></span></disp-formula></p><p>The initial distribution parameters of the SF <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {g}}_{1} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {G}}_{1} $\n</tex-math></inline-formula> can be given as <disp-formula id=\"deqn26-deqn27\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {\\mathbf {g}}_{1}=&amp;\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\mathbf {I}} }\\Bigg)^{-1} \\\\&amp;\\times \\, \\left ({{({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {x}}_{1} +(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}{\\mathbf { y}}_{1}} }\\right) \\tag{26}\\\\ {\\mathbf {G}}_{1}=&amp;\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\mathbf {I}} }\\Bigg)^{-1}\\tag{27}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {\\mathbf {g}}_{1}=&amp;\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\mathbf {I}} }\\Bigg)^{-1} \\\\&amp;\\times \\, \\left ({{({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {x}}_{1} +(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})})^{-1}{\\mathbf { y}}_{1}} }\\right) \\tag{26}\\\\ {\\mathbf {G}}_{1}=&amp;\\Bigg ({({\\mathbf {H}}^{(o_{t})})^{T}({\\boldsymbol{\\Sigma }}^{(o_{t})})^{-1}{\\mathbf {H}}^{(o_{t})}} \\\\&amp;{+\\,(\\mathbf {U}^{(b_{t})})^{T}({\\boldsymbol{\\Gamma }}^{(b_{t})}\\mathbf {)}^{-1}\\mathbf {U}^{(b_{t})}+{\\mathbf {I}} }\\Bigg)^{-1}\\tag{27}\\end{align*}\n</span></span></disp-formula> Subsequently, the backward smoothing is performed according to the following formulas:<disp-formula id=\"deqn28-deqn29\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {{\\hat {\\mathbf {g}}}}_{t}=&amp;{\\mathbf {G}}_{t} {\\mathbf {F}}^{T}{\\boldsymbol{\\Sigma }}_{ss}^{-1} ({{ \\hat {\\mathbf {g}}}}_{t+1} -{\\mathbf {Fg}}_{t})+{\\mathbf {g}}_{t} \\tag{28}\\\\ {{\\hat {\\mathbf {G}}}}_{t}=&amp;{\\mathbf {G}}_{t} {\\mathbf {F}}^{T}{\\boldsymbol{\\Sigma }}_{ss}^{-1} ({{ \\hat {\\mathbf {G}}}}_{t+1} {\\boldsymbol{\\Sigma }}_{ss}^{-1} -{\\mathbf {I}}){\\mathbf {FG}}_{t} +{\\mathbf {G}}_{t}\\tag{29}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {{\\hat {\\mathbf {g}}}}_{t}=&amp;{\\mathbf {G}}_{t} {\\mathbf {F}}^{T}{\\boldsymbol{\\Sigma }}_{ss}^{-1} ({{ \\hat {\\mathbf {g}}}}_{t+1} -{\\mathbf {Fg}}_{t})+{\\mathbf {g}}_{t} \\tag{28}\\\\ {{\\hat {\\mathbf {G}}}}_{t}=&amp;{\\mathbf {G}}_{t} {\\mathbf {F}}^{T}{\\boldsymbol{\\Sigma }}_{ss}^{-1} ({{ \\hat {\\mathbf {G}}}}_{t+1} {\\boldsymbol{\\Sigma }}_{ss}^{-1} -{\\mathbf {I}}){\\mathbf {FG}}_{t} +{\\mathbf {G}}_{t}\\tag{29}\\end{align*}\n</span></span></disp-formula> Finally, the evaluation of the three expectation terms are obtained as <disp-formula id=\"deqn30-deqn32\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} E[\\mathbf {s}_{t}]=&amp;{{\\hat {\\mathbf {g}}}}_{t} \\tag{30}\\\\ E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]=&amp;{{\\hat {\\mathbf {G}}}}_{t} +{{\\hat {\\mathbf {g}}}}_{t} {{ \\hat {\\mathbf {g}}}}_{t}^{T} \\tag{31}\\\\ E[\\mathbf {s}_{t} \\mathbf {s}_{t-1}^{T}]=&amp;{\\mathbf {G}}_{t-1} {\\mathbf {F}}^{T}{\\boldsymbol{\\Sigma }}_{ss}^{-1} {{ \\hat {\\mathbf {G}}}}_{t} +{{\\hat {\\mathbf {g}}}}_{t} {{\\hat {\\mathbf {g}}}}_{t-1}^{T}\\tag{32}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} E[\\mathbf {s}_{t}]=&amp;{{\\hat {\\mathbf {g}}}}_{t} \\tag{30}\\\\ E[\\mathbf {s}_{t} \\mathbf {s}_{t}^{T}]=&amp;{{\\hat {\\mathbf {G}}}}_{t} +{{\\hat {\\mathbf {g}}}}_{t} {{ \\hat {\\mathbf {g}}}}_{t}^{T} \\tag{31}\\\\ E[\\mathbf {s}_{t} \\mathbf {s}_{t-1}^{T}]=&amp;{\\mathbf {G}}_{t-1} {\\mathbf {F}}^{T}{\\boldsymbol{\\Sigma }}_{ss}^{-1} {{ \\hat {\\mathbf {G}}}}_{t} +{{\\hat {\\mathbf {g}}}}_{t} {{\\hat {\\mathbf {g}}}}_{t-1}^{T}\\tag{32}\\end{align*}\n</span></span></disp-formula></p><p>By iteratively updating and recalculating the E-step and M-step until convergence, the optimal model parameter set can be obtained. The latent state SFs can be inferred as the mean value <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {g}}_{t} $\n</tex-math></inline-formula> of the conditional probability distribution <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$p({\\mathbf {s}}_{t} \\vert {\\mathbf {x}}_{1:t},{\\mathbf {y}}_{1:t})=N({\\mathbf { g}}_{t},{\\mathbf {G}}_{t})$\n</tex-math></inline-formula>.</p></div></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Mr-PSFR Based Quality Prediction</h3><p>Based on the MR-PSFR model, the quality prediction strategy for multi-rate processes is proposed. Suppose we have obtained an online data sample <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {x}}_{\\textrm {new}} $\n</tex-math></inline-formula> at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t_{\\textrm {new}}$\n</tex-math></inline-formula> and it contains <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$R$\n</tex-math></inline-formula> different sample rates:<disp-formula id=\"deqn33\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} {\\mathbf {x}}_{\\textrm {new}} =\\left [{ {\\left ({{{\\mathbf {x}}_{\\textrm {new}}^{(o_{\\textrm {new}}^{1})}} }\\right)^{T}\\;\\left ({{{\\mathbf {x}}_{\\textrm {new}}^{(o_{\\textrm {new}}^{2})}} }\\right)^{T}\\;\\cdots \\;\\;\\left ({{{\\mathbf {x}}_{\\textrm {new}}^{(o_{\\textrm {new}}^{R})}} }\\right)^{T}} }\\right]^{T}\\tag{33}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} {\\mathbf {x}}_{\\textrm {new}} =\\left [{ {\\left ({{{\\mathbf {x}}_{\\textrm {new}}^{(o_{\\textrm {new}}^{1})}} }\\right)^{T}\\;\\left ({{{\\mathbf {x}}_{\\textrm {new}}^{(o_{\\textrm {new}}^{2})}} }\\right)^{T}\\;\\cdots \\;\\;\\left ({{{\\mathbf {x}}_{\\textrm {new}}^{(o_{\\textrm {new}}^{R})}} }\\right)^{T}} }\\right]^{T}\\tag{33}\\end{equation*}\n</span></span></disp-formula> in which <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1\\le o_{\\textrm {new}}^{1} &lt; o_{\\textrm {new}}^{2} &lt; \\cdots &lt; o_{\\textrm {new}}^{R} \\le M$\n</tex-math></inline-formula>.</p><p>The online emission matrix and the online covariance matrix are constructed as <disp-formula id=\"deqn34-deqn35\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {\\mathbf {H}}^{(o_{\\textrm {new}})}=&amp;[\\mathbf {H}^{(o_{\\textrm {new}}^{1})};\\;\\mathbf {H}^{(o_{\\textrm {new}}^{2})};\\;\\cdots \\;;\\mathbf {H}^{(o_{\\textrm {new}}^{R})}] \\tag{34}\\\\ {\\boldsymbol{\\Sigma }}^{(o_{\\textrm {new}})}=&amp;\\textrm {diag}\\left ({{{\\boldsymbol{\\Sigma }}_{{\\mathbf { x}}}^{(o_{\\textrm {new}}^{1})};\\;{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(o_{\\textrm {new}}^{2})} \\;;\\;\\cdots \\;;\\;{\\boldsymbol{\\Sigma }}_{{\\mathbf { x}}}^{(o_{\\textrm {new}}^{R})}} }\\right) \\tag{35}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {\\mathbf {H}}^{(o_{\\textrm {new}})}=&amp;[\\mathbf {H}^{(o_{\\textrm {new}}^{1})};\\;\\mathbf {H}^{(o_{\\textrm {new}}^{2})};\\;\\cdots \\;;\\mathbf {H}^{(o_{\\textrm {new}}^{R})}] \\tag{34}\\\\ {\\boldsymbol{\\Sigma }}^{(o_{\\textrm {new}})}=&amp;\\textrm {diag}\\left ({{{\\boldsymbol{\\Sigma }}_{{\\mathbf { x}}}^{(o_{\\textrm {new}}^{1})};\\;{\\boldsymbol{\\Sigma }}_{{\\mathbf {x}}}^{(o_{\\textrm {new}}^{2})} \\;;\\;\\cdots \\;;\\;{\\boldsymbol{\\Sigma }}_{{\\mathbf { x}}}^{(o_{\\textrm {new}}^{R})}} }\\right) \\tag{35}\\end{align*}\n</span></span></disp-formula></p><p>It is assumed that the SFs in the previous sampling time follows a Gaussian distribution, which can be denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N({\\mathbf {s}}_{\\textrm {prev}} \\vert {\\mathbf {g}}_{\\textrm {prev}},{\\mathbf {G}}_{\\textrm {prev}})$\n</tex-math></inline-formula>. Then, according to the forward filtering algorithm, the prediction of the SFs at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t_{\\textrm {new}} $\n</tex-math></inline-formula> is obtained as <disp-formula id=\"deqn36\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {{\\hat {\\mathbf {s}}}}_{\\textrm {new}}=&amp;{\\mathbf {Fg}}_{\\textrm {prev}} +{\\boldsymbol{\\Sigma }}_{ss} ({\\mathbf { H}}^{(o_{\\textrm {new}})})^{T} \\\\&amp;\\times \\, \\left ({{{\\mathbf {H}}^{(o_{\\textrm {new}})}{\\boldsymbol{\\Sigma }}_{ss} ({\\mathbf {H}}^{(o_{\\textrm {new}})})^{T}+{\\boldsymbol{\\Sigma }}^{(o_{\\textrm {new}})}} }\\right)^{-1} \\\\&amp;\\times \\, ({\\mathbf {x}}_{\\textrm {new}} -{\\mathbf {H}}^{(o_{\\textrm {new}})}{\\mathbf { Fg}}_{\\textrm {prev}})\\tag{36}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {{\\hat {\\mathbf {s}}}}_{\\textrm {new}}=&amp;{\\mathbf {Fg}}_{\\textrm {prev}} +{\\boldsymbol{\\Sigma }}_{ss} ({\\mathbf { H}}^{(o_{\\textrm {new}})})^{T} \\\\&amp;\\times \\, \\left ({{{\\mathbf {H}}^{(o_{\\textrm {new}})}{\\boldsymbol{\\Sigma }}_{ss} ({\\mathbf {H}}^{(o_{\\textrm {new}})})^{T}+{\\boldsymbol{\\Sigma }}^{(o_{\\textrm {new}})}} }\\right)^{-1} \\\\&amp;\\times \\, ({\\mathbf {x}}_{\\textrm {new}} -{\\mathbf {H}}^{(o_{\\textrm {new}})}{\\mathbf { Fg}}_{\\textrm {prev}})\\tag{36}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\boldsymbol{\\Sigma }}_{ss} ={\\mathbf {FG}}_{\\textrm {prev}} {\\mathbf {F}}^{T}+{\\boldsymbol{\\Lambda }}$\n</tex-math></inline-formula>.</p><p>By substituting <a ref-type=\"disp-formula\" anchor=\"deqn36\" href=\"#deqn36\" class=\"fulltext-link\">Eq. (36)</a> into the output equation in <a ref-type=\"disp-formula\" anchor=\"deqn9\" href=\"#deqn9\" class=\"fulltext-link\">Eq. (9)</a>, the prediction of quality variables at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t_{\\textrm {new}} $\n</tex-math></inline-formula> can be estimated as <disp-formula id=\"deqn37\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathbf {y}_{\\textrm {new}}^{(n)} =\\mathbf {U}^{(n)}{{\\hat {\\mathbf {s}}}}_{\\textrm {new}},\\;\\;\\;n=1,2,\\cdots,N\\tag{37}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathbf {y}_{\\textrm {new}}^{(n)} =\\mathbf {U}^{(n)}{{\\hat {\\mathbf {s}}}}_{\\textrm {new}},\\;\\;\\;n=1,2,\\cdots,N\\tag{37}\\end{equation*}\n</span></span></disp-formula></p><p>Thus, the quality variables at time <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t_{\\textrm {new}} $\n</tex-math></inline-formula> for different sampling rates is obtained. To quantitatively evaluate the performance of the soft sensor model, the root mean square error (RMSE) is used as an assessment index, which is defined as <disp-formula id=\"deqn38\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\textrm {RMSE =}\\sqrt {\\frac {1}{K}\\sum \\limits _{i=1}^{K} {(y_{i} -\\hat {y}_{i})^{2}}}\\tag{38}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\textrm {RMSE =}\\sqrt {\\frac {1}{K}\\sum \\limits _{i=1}^{K} {(y_{i} -\\hat {y}_{i})^{2}}}\\tag{38}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K$\n</tex-math></inline-formula> is the size of the testing dataset, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{i} $\n</tex-math></inline-formula> is the real value of the output variable and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {y}_{i} $\n</tex-math></inline-formula> is the predicted value of the output variable.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Case Study</h2></div><p>In this section, the proposed MR-PSFR method is applied to a numerical example and a R2S anaerobic reactor unit in the wastewater treatment process.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Numerical Example</h3><p>A multi-rate dynamic model is simulated, in which the hidden variable <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {h}}(t)$\n</tex-math></inline-formula> is two-dimensional. The observed process variables <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {x}}(t)$\n</tex-math></inline-formula> include three sampling rates, and there are four process variables at each sampling rate. The observed quality variables <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {y}}(t)$\n</tex-math></inline-formula> include two sampling rates and two quality variables at each sampling rate. The simulated model is set as follows:<disp-formula id=\"deqn39\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\begin{cases} \\displaystyle {\\mathbf {h}(t+1)=\\mathbf {Ah}(t)+\\mathbf {e}_{{\\mathbf {h}}} (t+1)} \\\\ \\displaystyle {\\mathbf {x}(t+1)=\\mathbf {Hh}(t+1)+\\mathbf {e}_{{\\mathbf {x}}} (t+1)} \\\\ \\displaystyle {\\mathbf {y}(t+1)=\\mathbf {Uh}(t+1)+\\mathbf {e}_{{\\mathbf {y}}} (t+1)}  \\end{cases}\\tag{39}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\begin{cases} \\displaystyle {\\mathbf {h}(t+1)=\\mathbf {Ah}(t)+\\mathbf {e}_{{\\mathbf {h}}} (t+1)} \\\\ \\displaystyle {\\mathbf {x}(t+1)=\\mathbf {Hh}(t+1)+\\mathbf {e}_{{\\mathbf {x}}} (t+1)} \\\\ \\displaystyle {\\mathbf {y}(t+1)=\\mathbf {Uh}(t+1)+\\mathbf {e}_{{\\mathbf {y}}} (t+1)}  \\end{cases}\\tag{39}\\end{align*}\n</span></span></disp-formula> in which <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t=1,2,\\cdots,T-1$\n</tex-math></inline-formula> is the sampling time, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {h}}} (t)$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {x}}} (t)$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbf {e}_{{\\mathbf {y}}} (t)$\n</tex-math></inline-formula> are Gaussian white noise with zero means and variance 0.01. It is defined as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {H}}=[{\\mathbf {H}}_{1};\\;{\\mathbf {H}}_{2};\\;{\\mathbf {H}}_{3}]$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\mathbf {U}}=[{\\mathbf {U}}_{1};{\\mathbf {U}}_{2}]$\n</tex-math></inline-formula>, which are given as follows:<disp-formula id=\"deqn40\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} {\\mathbf {A}}\\;\\;=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {0.8} &amp; {0.5} \\\\ {0.4} &amp; {-0.6} \\\\ \\end{array}}} }\\right],\\;\\;\\;\\;{\\mathbf {U}}_{1} =\\left [{ {{\\begin{array}{cccccccccccccccccccc} {0.87} &amp; {0.52} \\\\ {0.49} &amp; {0.40} \\\\ \\end{array}}} }\\right], \\\\ {\\mathbf {U}}_{2}=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {-0.17} &amp; {0.58} \\\\ {-0.20} &amp; {0.75} \\\\ \\end{array}}} }\\right] \\\\ {\\mathbf {H}}_{1}=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\begin{array}{cccccccccccccccccccc} {0.49} \\\\ {-0.01} \\\\ {0.3} \\\\ {-0.12} \\\\ \\end{array}}} &amp; {{\\begin{array}{cccccccccccccccccccc} {0.71} \\\\ {-1} \\\\ {0.72} \\\\ {1.1} \\\\ \\end{array}}} \\\\ \\end{array}}} }\\right],{\\mathbf {H}}_{2} =\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\begin{array}{cccccccccccccccccccc} {0.28} \\\\ {0.62} \\\\ {1.63} \\\\ {1.36} \\\\ \\end{array}}} &amp; {{\\begin{array}{cccccccccccccccccccc} {-0.13} \\\\ {0.98} \\\\ {-0.67} \\\\ {-1.12} \\\\ \\end{array}}}\\\\ \\end{array}}} }\\right], \\\\ {\\mathbf {H}}_{3}=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\begin{array}{cccccccccccccccccccc} 1 \\\\ {-0.5} \\\\ {-0.62} \\\\ {1.56} \\\\ \\end{array}}} &amp; {{\\begin{array}{cccccccccccccccccccc} {-1.13} \\\\ {-0.64} \\\\ {0.08} \\\\ {0.62} \\\\ \\end{array}}} \\\\ \\end{array}}} }\\right]\\tag{40}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} {\\mathbf {A}}\\;\\;=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {0.8} &amp; {0.5} \\\\ {0.4} &amp; {-0.6} \\\\ \\end{array}}} }\\right],\\;\\;\\;\\;{\\mathbf {U}}_{1} =\\left [{ {{\\begin{array}{cccccccccccccccccccc} {0.87} &amp; {0.52} \\\\ {0.49} &amp; {0.40} \\\\ \\end{array}}} }\\right], \\\\ {\\mathbf {U}}_{2}=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {-0.17} &amp; {0.58} \\\\ {-0.20} &amp; {0.75} \\\\ \\end{array}}} }\\right] \\\\ {\\mathbf {H}}_{1}=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\begin{array}{cccccccccccccccccccc} {0.49} \\\\ {-0.01} \\\\ {0.3} \\\\ {-0.12} \\\\ \\end{array}}} &amp; {{\\begin{array}{cccccccccccccccccccc} {0.71} \\\\ {-1} \\\\ {0.72} \\\\ {1.1} \\\\ \\end{array}}} \\\\ \\end{array}}} }\\right],{\\mathbf {H}}_{2} =\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\begin{array}{cccccccccccccccccccc} {0.28} \\\\ {0.62} \\\\ {1.63} \\\\ {1.36} \\\\ \\end{array}}} &amp; {{\\begin{array}{cccccccccccccccccccc} {-0.13} \\\\ {0.98} \\\\ {-0.67} \\\\ {-1.12} \\\\ \\end{array}}}\\\\ \\end{array}}} }\\right], \\\\ {\\mathbf {H}}_{3}=&amp;\\left [{ {{\\begin{array}{cccccccccccccccccccc} {{\\begin{array}{cccccccccccccccccccc} 1 \\\\ {-0.5} \\\\ {-0.62} \\\\ {1.56} \\\\ \\end{array}}} &amp; {{\\begin{array}{cccccccccccccccccccc} {-1.13} \\\\ {-0.64} \\\\ {0.08} \\\\ {0.62} \\\\ \\end{array}}} \\\\ \\end{array}}} }\\right]\\tag{40}\\end{align*}\n</span></span></disp-formula></p><p>Data series with 2000 samples are simulated from the abovementioned numerical setting. To carry out soft sensor modeling, the first 1000 samples are used as the training data while the rest 1000 samples are used for testing. Assuming that the sampling interval is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$T_{s} $\n</tex-math></inline-formula>, process variables <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x_{1} \\sim x_{4} $\n</tex-math></inline-formula> are sampled every <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$T_{s} $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x_{5} \\sim x_{8} $\n</tex-math></inline-formula> are sampled every <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$2T_{s} $\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x_{9} \\sim x_{12} $\n</tex-math></inline-formula> are sampled every <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$5T_{s} $\n</tex-math></inline-formula>. For quality variables, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{1} \\sim y_{2} $\n</tex-math></inline-formula> are sampled every <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$5T_{s} $\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{3} \\sim y_{4} $\n</tex-math></inline-formula> are sampled every <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$10T_{s}$\n</tex-math></inline-formula>. For performance comparison, the proposed MR-PSFR model is compared with the probabilistic principal components regression (PPCR) <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_4a\">[40]</a>, probabilistic slow feature regression (PSFR) <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_4a\">[33]</a> and MRPCR <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_4a\">[27]</a> models. The MRPCR model is built using the same multi-rate dataset, while the PPCR and PSFR models are developed based on the down-sampling method. The latent variables\u2019 dimension of all the models is selected to be 2 with the numerical setting.</p><p>The two SFs extracted by MR-PSFR with corresponding <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{i} $\n</tex-math></inline-formula> are displayed in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{i} $\n</tex-math></inline-formula> reflects the slowness of the corresponding SF. The RMSE prediction errors for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{1} \\sim y_{4} $\n</tex-math></inline-formula> using different models are presented in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. It is obvious that the performance of MR-PSFR is superior to other approaches considered, which has the minimum RMSE for all outputs. In terms of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{1} $\n</tex-math></inline-formula>, the RMSE of the proposed model has percentage decreases of 33.73% compared to the PPCR model, percentage decreases of 24.57% compared to the PSFR model and percentage decreases of 12.54% compared to the MRPCR model. <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a> shows the detailed prediction results for output <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{1}$\n</tex-math></inline-formula> by the four models, where blue lines represent the real value and the red lines represent the predicted value. As can be seen from the figure, the prediction accuracy of MRPCR is better than that of PPCR and PSFR due to the use of multi-sampling rate datasets. Furthermore, the prediction performance has been greatly improved by the MR-PSFR model, especially for those samples 10th\u201317th, 100th\u2013112th, and 178th\u2013190th.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nRMSE Prediction Errors of Four Models in Numerical Example</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang.t1-3228048-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang.t1-3228048-small.gif\" alt=\"Table 1- &#10;RMSE Prediction Errors of Four Models in Numerical Example\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang3-3228048-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang3-3228048-small.gif\" alt=\"FIGURE 3. - SFs extracted by MR-PSFR in numerical example.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>SFs extracted by MR-PSFR in numerical example.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang4-3228048-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang4-3228048-small.gif\" alt=\"FIGURE 4. - Prediction results for output &#10;$y_{1} $&#10; in numerical example.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Prediction results for output <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$y_{1} $\n</tex-math></inline-formula> in numerical example.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div></p><p>The experiments are performed on a personal computer with the configuration shown as follows. Operating system: Windows 10 (64-bit); CPU: Intel Core i7-10700 (2.90 GHz); RAM: 16.0 GB and MATLAB 2017b software. The computation time of the PPCR, PSFR, MRPCR and MR-PSFR models are 0.49s, 0.56s, 2.98s and 3.24s, respectively.</p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. R2s Anaerobic Reactor Unit</h3><p>The MR-PSFR model is applied to a real R2S anaerobic reactor unit in the papermaking wastewater treatment process. <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a> gives the detailed flowchart of the wastewater treatment process. Papermaking wastewater treatment plant first carries out primary treatment for wastewater. The wastewater is passed successively through bar screener, blending pond and preliminary clarifier to remove large number of sediments, suspended solids and floating oil in the wastewater. Then it is fed into an anaerobic reactor for secondary treatment. The anaerobic and aerobic reaction of microorganisms is used to biodegrade wastewater. By adding suitable microbial products, the sludge yield and chemical Oxygen demand (COD) concentration can be significantly reduced. After secondary treatment, the wastewater goes through anoxic pool and secondary clarifier successively. Finally, it becomes clarified water to be recycled or directly discharged into the nearby water body.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang5-3228048-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang5-3228048-small.gif\" alt=\"FIGURE 5. - The flowchart of the wastewater treatment process.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>The flowchart of the wastewater treatment process.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div></p><p>Based on our engineering experience, 22 typical process variables are collected as the input data and five major quality indicators are chosen as the output data. <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a> list the detailed process variable description. It can be seen that U<sub>1</sub> ~ U<sub>3</sub> are sampled online per hour, U<sub>4</sub> ~ U<sub>15</sub> are sampled online per 2 h and U<sub>16</sub> ~ U<sub>22</sub> are off-line assayed every 24 h. Besides, the chosen output variables are chemical Oxygen demand (COD), volatile fatty acid (VFA), PH, suspended solids (SS) 1# and 2# in the anaerobic reactor outlet, all of which are collected every 24 h by offline analyzing in the laboratory. Firstly, the data of 74 days under normal working conditions are collected as the training set and the MR-PSFR model is constructed. It can be found that 1776 samples of U<sub>1</sub> ~ U<sub>3</sub>, 888 samples of U<sub>4</sub> ~ U<sub>15</sub>, 74 samples of U<sub>16</sub> ~ U<sub>22</sub> and 74 samples of the five quality indicators are collected. For comparison, the PPCR, PSFR and MRPCR models are also built using the same way as the numerical example. The latent variables of MR-PSFR and MRPCR are 10 and 8 respectively using the cross validation. And the latent variables of PPCR and PSFR are both 7. The ten slowest SFs extracted by MR-PSFR with corresponding <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{i} $\n</tex-math></inline-formula> are displayed in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda _{i} $\n</tex-math></inline-formula> reflects the slowness of the corresponding SF. It can be seen that the extracted SFs can separate slowly and fast changing latent features.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nExplanations of Process Variables in R2S Reactor</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang.t2-3228048-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang.t2-3228048-small.gif\" alt=\"Table 2- &#10;Explanations of Process Variables in R2S Reactor\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang6-3228048-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang6-3228048-small.gif\" alt=\"FIGURE 6. - SFs extracted by MR-PSFR in wastewater treatment process.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>SFs extracted by MR-PSFR in wastewater treatment process.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div></p><p>Subsequently, 72 days of test data from the same unit are collected to evaluate the predictive performance of the proposed model. The RMSE prediction errors for the five quality indicators using PPCR, PSFR, MRPCR and MR-PSFR are summarized in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>. Obviously, the proposed model performs better than the alternatives for all the five quality indicators. According to the prediction results of outlet COD, the RMSE of the proposed model has percentage decreases of 40.51% compared to the PPCR model, percentage decreases of 27.68% compared to the PSFR model and percentage decreases of 24.81% compared to the MRPCR model. The detailed prediction results for outlet COD using different models are illustrated in <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Figure 7</a>. Note that the concentration of outlet COD is normalized. As can be seen from the figure, the prediction performance of MR-PSFR is the best among all models, and it can track the dynamic trend of the process. Especially for those samples 14th\u201319th, 21th\u201326th, and 55th\u201362th, the MR-PSFR has a far better prediction result than the alternatives. The reason is that the proposed method has utilized the sufficient process and quality variables instead of dropping some useful information, and the extracted SFs can capture all the dynamic information contained in observations during the modeling process. It can be inferred that the MR-PSFR model is an effective soft sensor for quality prediction in multi-rate dynamic processes.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nRMSE Prediction Errors of Four Models in Wastewater Treatment Process</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang.t3-3228048-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang.t3-3228048-small.gif\" alt=\"Table 3- &#10;RMSE Prediction Errors of Four Models in Wastewater Treatment Process\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang7-3228048-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978612/zhang7-3228048-small.gif\" alt=\"FIGURE 7. - Prediction results for outlet COD in wastewater treatment process.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 7. </b><fig><p>Prediction results for outlet COD in wastewater treatment process.</p></fig></div><p class=\"links\"><a href=\"/document/9978612/all-figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion</h2></div><p>In this paper, a multi-rate probabilistic slow feature regression model is proposed for dynamic feature learning and soft sensor development in industrial processes. In the MR-PSFR, both input and output observation datasets with different sampling rates are used to extract the slow features. Then, an efficient EM-based learning algorithm is developed for training the model and the quality prediction strategy for multi-rate processes is constructed based on MR-PSFR. Finally, two case studies are carried out on a numerical example and a real R2S anaerobic reactor unit in the wastewater treatment process. The simulation results show that the extracted slow features better represent the intrinsic characteristics of the processes and the proposed model has better prediction performance for multi-rate dynamic processes than other methods. In the future, more data-driven methods will be considered and applied to adapt to various irregular data sets in practice.</p></div>\n</div></div>\n"
}