{
    "abstract": "Structured pruning methods have been used in several convolutional neural networks (CNNs). However, group-based structured pruning is a challenging task. In previous methods, the number of groups is manually determined for all layers, which is suboptimal. Moreover, which kernels should be appropriately removed? Model accuracy may be significantly reduced when a large number of kernels are deleted....",
    "articleNumber": "9975798",
    "articleTitle": "Automatic Group-Based Structured Pruning for Deep Convolutional Networks",
    "authors": [
        {
            "preferredName": "Hang Wei",
            "normalizedName": "H. Wei",
            "firstName": "Hang",
            "lastName": "Wei",
            "searchablePreferredName": "Hang Wei"
        },
        {
            "preferredName": "Zulin Wang",
            "normalizedName": "Z. Wang",
            "firstName": "Zulin",
            "lastName": "Wang",
            "searchablePreferredName": "Zulin Wang"
        },
        {
            "preferredName": "Gengxin Hua",
            "normalizedName": "G. Hua",
            "firstName": "Gengxin",
            "lastName": "Hua",
            "searchablePreferredName": "Gengxin Hua"
        },
        {
            "preferredName": "Jinjing Sun",
            "normalizedName": "J. Sun",
            "firstName": "Jinjing",
            "lastName": "Sun",
            "searchablePreferredName": "Jinjing Sun"
        },
        {
            "preferredName": "Yunfu Zhao",
            "normalizedName": "Y. Zhao",
            "firstName": "Yunfu",
            "lastName": "Zhao",
            "searchablePreferredName": "Yunfu Zhao"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227619",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9975798/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Convolutional neural networks (CNNs) <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a> have shown excellent performance on various computer vision tasks. However, deep CNNs with large sizes and large numbers of floating-point operations per second (FLOPs) are difficult to deploy directly on edge devices with limited memory and computing resources. Therefore, several compression methods have been proposed to compress the model size and reduce the computation of CNNs, including compacting networks <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, pruning <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>, quantization <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a> and knowledge distillation <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>. Among existing techniques, as one of the most effective methods, pruning has received increasing attention from researchers.</p><p>Network pruning can be divided into irregular pruning <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a>, <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a>, <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a> and structured pruning <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\">[18]</a>. However, these irregular pruning methods can easily reduce the irregular network architectures that require customized GPU kernels or specialized hardware to accelerate the process. Structured pruning is regarded as a promising solution to the problem of irregular sparse models, independent of hardware and software platforms. Group convolution (GConv) is a regular convolution with sparse kernels, compared to standard convolution and group convolution as shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1 (a) and (b)</a>. GConv was first used to accelerate the AlexNet training <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>. In theory, the computation of a GConv with G groups is 1/G of that of a standard spatial convolution. There are no connections across the channels in different partitions. Therefore, group convolution is regarded as a special channel pruning. Group convolution has been widely employed in lightweight convolutional neural networks <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\">[19]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a>.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei1ab-3227619-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei1ab-3227619-small.gif\" alt=\"FIGURE 1. - Illustration of different convolution strategies, where the color circles represent input and output channels, and the lines are the connections between them. (a) Regular convolution. Every input channel is connected to every output channel. (b) Group convolution with group number 4.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Illustration of different convolution strategies, where the color circles represent input and output channels, and the lines are the connections between them. (a) Regular convolution. Every input channel is connected to every output channel. (b) Group convolution with group number 4.</p></fig></div><p class=\"links\"><a href=\"/document/9975798/all-figures\" class=\"all\">Show All</a></p></div></p><p>Yao et al. <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\">[18]</a> proposed a repeated group convolution (RGC) kernel, SRGC-WRN-M4-N2-f9, that removes redundancy from group convolution. SRGC-WRN-M4-N2-f9 not only has better accuracy than MobileNetV2 <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a> but also has a model size approximately <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$2 \\times $\n</tex-math></inline-formula> smaller than that of MobileNetV2 <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>. Although group convolution is effective for compressing CNNs, each group number in the RGC is manually set, which easily leads to suboptimal performance. Therefore, adaptively and efficiently configuring the number of groups and filters to reconstruct the global output is challenging. To solve this issue, Huang et al. <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\">[21]</a> proposed a learned group convolution (LGC) method that determines the connections between layers during DenseNet training. Compared to DenseNets, CondenseNets <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\">[21]</a>, which is based on LGC, uses only 1/10 of the computation at comparable accuracy levels. Xijun et al. <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\">[22]</a> proposed a fully learned group convolution (FLGC) that applies two binary selection matrices to select the input channels and filters into different groups during training. Compared with the original normal convolutions, the group convolutions of CondenseNet and FLGC can reduce the model size. However, they also weaken the model representation capability owing to the sparse structures. To overcome this shortcoming, Zhuo et al. <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_1\">[23]</a> proposed a dynamic group convolution (DGC), in which each group is decided dynamically by a tiny auxiliary feature selector. Ruizhe et al. <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_1\">[24]</a> formulated group-based convolution pruning as a channel permutation optimization problem and solved channel structural constraints efficiently using a heuristic algorithm. However, it was difficult to determine the structural constraints of each convolutional layer. Meanwhile, as CNNs become deeper, the solution space of group convolution increases exponentially, which is difficult to solve using heuristic methods. Yihui et al. <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1\">[25]</a> first proposed model compression based on AutoML, named AMC, which leverages reinforcement learning (RL) to generate compression policy for each convolution layer. This learning-based compression policy outperforms the conventional rule-based compression policy by having a higher compression ratio, better preserving the accuracy and freeing human labor <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_1\">[25]</a>.</p><p>Hence, in this study, we address the problem of automatic group-based channel pruning for CNNs using RL, which is not limited to the number and types of convolutional layers. Meanwhile, for a pre-trained model, group-based channel pruning changes the model architecture. Therefore, standard convolution must be reconstructed into standard group convolution. However, determining the number of groups based on the compression rates from the RL and reconstruction of the model architecture is challenging. To address these issues, we propose an automatic group-based structured pruning (AGSPRL) method. It includes three stages: first, learning the compression rate of each convolution layer via RL; second, according to the compression rate, we propose a novel simple and efficient heuristic group configuration algorithm to adaptively determine the number of groups for each convolution layer; and finally, we apply channel pruning with the attention mechanism as a tiny auxiliary filter selector for each group to dynamically decide which part of the filters to be selected into group convolution and reconstruct the convolutional layers into a group convolution structure. To maintain accuracy, we retrain the pruned neural network. The complete architecture of AGSPRL is shown in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a>. To demonstrate the effectiveness of our proposed method, we evaluate AGSPRL on several popular CNNs. The experimental results show that the proposed AGSPRL method exhibits superior performance. For example, when compressing a network, AGSPRL adaptively implements a group-based convolution pruning process, without relying on rule-based policy or experienced engineers. It can eliminate 52.82% of parameters of ResNet-18 on CIFAR-10, and its performance is not degraded simultaneously.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei2-3227619-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei2-3227619-small.gif\" alt=\"FIGURE 2. - Figure shows the complete architecture. It includes three stages: learning the compression rate of each convolution layer via reinforcement learning; calculating the number of groups and reconstructing convolution operation; retraining the neural network.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Figure shows the complete architecture. It includes three stages: learning the compression rate of each convolution layer via reinforcement learning; calculating the number of groups and reconstructing convolution operation; retraining the neural network.</p></fig></div><p class=\"links\"><a href=\"/document/9975798/all-figures\" class=\"all\">Show All</a></p></div></p><p>The key contributions of this paper are summarized as follows:\n<ol><li><p>We formulate the group-based structured pruning problem as a reinforcement learning (RL) optimization process that can automatically generate pruned models with different considered compressed rates. Our method first finds a compressed model that meets the given compression rate for the pre-trained model, and then continuously optimizes the group-based channel pruning rate of each convolution layer until the compressed model satisfies the accuracy requirement.</p></li><li><p>To automatically obtain the number of groups for channel pruning, we propose a novel simple and efficient heuristic group configuration algorithm that can provide a feasible group number for each convolution layer within an acceptable computing time and space. Then, to avoid retaining the pruned model from scratch, we introduce the kernel attention mechanism as a tiny auxiliary filter selector for each group to dynamically determine which part of the filters should be selected in the group convolution. Thus, a convolution layer is restructured into a standard group convolution structure.</p></li><li><p>Extensive experiments show that AGSPRL can efficiently reduce the model parameters and FLOPs with negligible accuracy loss. Compared with state-of-the-art group-based pruning methods, AGSPRL can give better group configuration at a given compression rate.</p></li></ol></p><p>The rest of this paper is organized as follows. <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section II</a> reviews the related studies on network pruning. In <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section III</a>, the proposed pruning approach is discussed. The implementation results and comparisons of the proposed method are presented in <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section IV</a>. Finally, <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section V</a> concludes the paper.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Related Works</h2></div><p>A deep convolutional neural network with billions of parameters has approached human-level recognition capabilities but at the cost of enormous computational complexity. It is very time-consuming to train and difficult to deploy in embedded environments. Extensive studies have been conducted on lightweight neural networks <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2\">[14]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2\">[27]</a>, <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2\">[29]</a>, <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2\">[31]</a>, <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2\">[32]</a>, <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2\">[33]</a>, <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2\">[34]</a>, <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2\">[35]</a>, <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2\">[36]</a>, <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2\">[37]</a>. For CNNs, the computation cost originates primarily from the convolution operation. Therefore, there are many excellent compression methods to reduce the redundant parameters of the convolutional layers, such as network pruning <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2\">[26]</a>. Network pruning has been one of the most popular methods for compressing CNNs and can effectively reduce memory size and bandwidth.</p><p>The main idea of the network pruning technique is to remove unimportant weights or channels that contribute less to the model accuracy. Current research on pruning can be divided into two types: irregular pruning, which deletes less important weights <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2\">[17]</a>, and structured pruning, which removes redundant channels <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2\">[38]</a>. Compared with irregular pruning methods, structured pruning methods are friendly to hardware, that is, they accelerate network inference directly. Therefore, in recent years, structured pruning has become an attractive research topic in practical applications. For example, Li et al. <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2\">[39]</a> proposed a based-sensitivity structured pruning strategy. However, this method prunes only one layer at a time in the calculation of sensitivity without considering the correlation between different layers; therefore, the sensitivity is inaccurate.</p><p>Group convolution is an effective structured pruning strategy. In this paper, we mainly focus on this kind of structured pruning. So far, there have been many successful applications <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2\">[20]</a> based on group convolution. For example, MobileNets <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2\">[20]</a> apply the depth-wise separable convolution to replace the standard convolution, which can reduce the parameters of a convolution operation by approximately <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$9 \\times $\n</tex-math></inline-formula>. Unlike Inception, ResNeXt <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_2\">[40]</a> applies group convolution to construct the same unit, which has fewer hyper-parameters than InceptionV4. Instead of pre-defining the connection between CNNs\u2019 units, CondenseNet <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2\">[21]</a> and FLGC <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2\">[22]</a> were proposed to automatically learn the connections between groups; that is, the group convolutional structure is learned during the training process. However, these group convolution methods reduce the performance of the model compared with the original model owing to the sparsity of group convolution. To deal with this problem, dynamic group convolution (DGC) <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2\">[23]</a> applies a dynamic execution strategy to determine the number of groups and effectively solves the performance degradation problem of the group-based convolution model. Although group convolution can effectively reduce the number of parameters in standard convolution by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G \\times $\n</tex-math></inline-formula>, it still exhibits redundancy. To further refine the group convolution, Yao et.al. <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2\">[18]</a> proposed a repeated group convolution (RGC) kernel, which is more lightweight than the standard group convolution. However, the number of groups in RCG must be manually set. Zhang <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_2\">[41]</a> proposed a dynamic group convolution (DGConv) operation in an end-to-end manner, which can learn to perform various convolutions from training data and different the number of groups for different convolution layers. Although this method is flexible, it relies heavily on a learnable binary relationship matrix U, which introduces additional parameters that complicate the training of deep convolution neural networks. Ruizhe et al. <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2\">[24]</a> formulated group-based convolution pruning as a channel permutation optimization problem and efficiently solved channel structural constraints using a heuristic algorithm. However, it is difficult to be determine the structural constraints of each convolutional layer. Meanwhile, as CNNs become deeper, the solution space of group convolution increases exponentially, which is difficult to solve using the heuristic methods. Zhang et al. <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_2\">[42]</a> proposed a group-based pruning method based on kernel principal component analysis to classify filters into groups, named KPGP, which ranks the importance of the grouped kernels and prunes the \u2018unimportant\u2019 ones. However, the KPGP method introduces several types of hyperparameters, such as the number of groups <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G$\n</tex-math></inline-formula>, which is manually set.</p><p>Recently, automatic machine learning, such as AutoML <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_2\">[41]</a>, has become a popular research topic. Yihui et al. <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2\">[25]</a> first applied AutoML for model compression, named AMC, which leveraged RL to provide a model compression policy. Following AMC, Gupta et al. <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_2\">[43]</a> proposed an improved network compression framework, named Pruning using Reinforcement Learning (PuRL). Its training episodes were as much as 85% lower than those of the AMC. However, none of the above methods treat group convolution pruning as an AutoML method. In contrast, our proposed approach AGSPRL is the first automatic group-based structured pruning method that use a reinforcement learning algorithm.</p></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Method</h2></div><p>In this paper, we propose an automatic group-based structured pruning method with reinforcement learning (AGSPRL) for automatic channel pruning under different model compression rates. The proposed AGSPRL implements kernel pruning by reconstructing the convolution into a group convolution, which can reduce <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G \\times $\n</tex-math></inline-formula> parameters. In this section, we first give a brief introduction to the background in <a ref-type=\"sec\" anchor=\"sec3a\" class=\"fulltext-link\">Section III-A</a>. In <a ref-type=\"sec\" anchor=\"sec3b\" class=\"fulltext-link\">Section III-B</a>, we introduce the overall framework and the details of AGSPRL.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Preliminaries</h3><p>We assume a CNN is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathcal {M}$\n</tex-math></inline-formula>. The input of a standard convolution operation is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$F\\,\\,\\in \\,\\,\\mathbb {R} ^{N \\times {C^{in}} \\times H \\times W}$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> represents the number of samples in a mini-batch, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C^{in}$\n</tex-math></inline-formula> denotes the number of input channels, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$H$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula> respectively represent the height and width of an input channel. When a standard convolution operation with kernel size <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k \\times k$\n</tex-math></inline-formula>, stride 1 and padding 1, we input <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$F$\n</tex-math></inline-formula> to this convolution and the output feature map is denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O\\,\\,\\in \\,\\,\\mathbb {R} ^{N \\times {C^{out}} \\times H \\times W}$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C^{out}$\n</tex-math></inline-formula> is the number of filters.</p><p>Every convolutional operator output is:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} o_{i, j, c}=\\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} F_{(i+m, j+n)} * K_{(m, n)} \\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} o_{i, j, c}=\\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} F_{(i+m, j+n)} * K_{(m, n)} \\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i~\\in ~\\{1,\\ldots,H\\}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j~\\in ~\\{1,\\ldots,W\\}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c~\\in ~{C_{out}}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$m~\\in ~\\{1,\\ldots,k\\}$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n~\\in ~\\{1,\\ldots,k\\}$\n</tex-math></inline-formula>. And * denotes the convolutional operation.The number of parameters and computational cost of a standard convolution operation is calculated approximately by <a ref-type=\"disp-formula\" anchor=\"deqn2-deqn3\" href=\"#deqn2-deqn3\" class=\"fulltext-link\">(2) and (3)</a> respectively.<disp-formula id=\"deqn2-deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Params=&amp;C^{in} \\times C^{out} \\times k^{2} \\tag{2}\\\\ FLOPs=&amp;H \\times W \\times C^{in} \\times C^{out} \\times k^{2} \\tag{3}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Params=&amp;C^{in} \\times C^{out} \\times k^{2} \\tag{2}\\\\ FLOPs=&amp;H \\times W \\times C^{in} \\times C^{out} \\times k^{2} \\tag{3}\\end{align*}\n</span></span></disp-formula></p><p>Group convolution (GConv) is often implemented as a concatenation of separate convolution over grouped kernels. In every group, it only performs the relevant filters as shown in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig. 1 (b)</a>. For an input <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$F\\,\\,\\in \\,\\,\\mathbb {R} ^{N \\times {C^{in}} \\times H \\times W}$\n</tex-math></inline-formula>, we divide it into <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G$\n</tex-math></inline-formula> groups. We have <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K\\,\\,\\in \\,\\,\\mathbb {R} ^{C^{out} \\times \\frac {C^{in}}{G} \\times k \\times k}$\n</tex-math></inline-formula>. Therefore, the number of parameters and computational cost of a group convolution is calculated by <a ref-type=\"disp-formula\" anchor=\"deqn4-deqn5\" href=\"#deqn4-deqn5\" class=\"fulltext-link\">(4) and (5)</a> respectively.<disp-formula id=\"deqn4-deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} Params_{G}=&amp;C^{in} \\times C^{out} \\times k^{2} \\times \\frac {1}{G} \\tag{4}\\\\ FLOPs_{G}=&amp;H \\times W \\times C^{in} \\times C^{out} \\times k^{2} \\times \\frac {1}{G} \\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} Params_{G}=&amp;C^{in} \\times C^{out} \\times k^{2} \\times \\frac {1}{G} \\tag{4}\\\\ FLOPs_{G}=&amp;H \\times W \\times C^{in} \\times C^{out} \\times k^{2} \\times \\frac {1}{G} \\tag{5}\\end{align*}\n</span></span></disp-formula></p><p>Therefore, the group convolution from the hard assignment can easily result in considerable <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${G \\times }$\n</tex-math></inline-formula> acceleration. Group convolution significantly accelerates CNNs and compresses the model size, thereby improving the development of edge AI. However, group convolution has several limitations. For example, the number of groups is often defined by human experts, which requires numerous trial-and-error procedures. All convolutional layers of a CNN,such as KPGP <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_3a\">[42]</a>, often have the same group number. Thus, it is easy to lead to suboptimal solutions and poor performance. Therefore, to deal with these issues, AGSPRL arms to design a learnable group configuration mechanism via RL to balance acceleration and accuracy effectively.</p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Automatic Group-Based Structured Pruning Method</h3><p>This section detailedly describes our proposed method. The overall framework is shown in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig. 2</a>.</p><div class=\"section_2\" id=\"sec3b1\"><h4>1) Reinforcement Learning Framework</h4><p>Reinforcement learning (RL) is a Markov decision process (MDP), an interactive learning process between an agent and environment. Therefore, in this section, we define group-based kernel pruning as a MDP-optimization problem. For a given L-layer convolutional neural network <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${M}$\n</tex-math></inline-formula> and compressed model as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${M'}$\n</tex-math></inline-formula>, we define the pruner as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${M'}$\n</tex-math></inline-formula> = <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${AGSPRL}({M},{\\alpha })$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\alpha }$\n</tex-math></inline-formula> is the given compression rate. We aim to learn a pruning policy that maximizes the accuracy of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${M'}$\n</tex-math></inline-formula> under compression rate <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${\\alpha }$\n</tex-math></inline-formula>.</p><p>The pruning process can be formulated as a RL framework to determine the compression rate of each layer of a pretrained CNN <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${M}$\n</tex-math></inline-formula>. RL can be denoted as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$ &lt; \\text{S}$\n</tex-math></inline-formula>,A,<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\text{R}&gt;$\n</tex-math></inline-formula>, where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${S}$\n</tex-math></inline-formula> represents the state space, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${A}$\n</tex-math></inline-formula> is the action space, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${R}$\n</tex-math></inline-formula> is the reward function. The goal of RL is to find a good pruning policy to maximize the reward <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${R}$\n</tex-math></inline-formula>. In AMC <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b1\">[25]</a>, the sparsity ratio of the layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${i}$\n</tex-math></inline-formula> in a CNN as an action value <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{i}$\n</tex-math></inline-formula> is automatically determined by the RL agent.</p><p>In this paper, our goal is to find the optimal compressed rate of each layer, which is used to determine the optimal number of groups <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G$\n</tex-math></inline-formula>. Therefore, we build upon AMC <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b1\">[25]</a> and PuRL <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_3b1\">[43]</a> to formalize our reinforcement learning algorithm for group-based kernel pruning of CNNs. For each layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>, there are 11 features that characterize the state in AMC <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b1\">[25]</a>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$FLOPs[t]$\n</tex-math></inline-formula> is FLOPs of layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>. However, ShuffleNetV2 <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_3b1\">[19]</a> shows using FLOPs as the only metric for computation complexity is insufficient and could lead to sub-optimal design. And in some experiments of model compression, the results show FLOPs is not proportional to the model size. For example, the model with a smaller size has larger FLOPs, as shown in <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>. ResNet-18-4 has a smaller model size than ResNet-18-1, ResNet-18-2 and ResNet-18-3, but its FLOPs are larger than ResNet-18-1, ResNet-18-2 and ResNet-18-3. For a limited memory device, the model size is more important for model deployment than FLOPs. Therefore, in this paper, we define the state space related to the parameters and accuracy loss, as follows:<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} s_{t}=(t,n,c,h,w,k,Params[t],reduced,rest,a_{t-1}) \\tag{6}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} s_{t}=(t,n,c,h,w,k,Params[t],reduced,rest,a_{t-1}) \\tag{6}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s_{t}\\in $\n</tex-math></inline-formula> [0, 1], <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula> is the convolutional layer index, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the batch size of samples, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula> is the number of input channels, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$h$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$w$\n</tex-math></inline-formula> are height and weight of each kernel respectively, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> is the kernel size, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Params[t]$\n</tex-math></inline-formula> are the parameters of convolutional layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t$\n</tex-math></inline-formula>. Reduced is the total number of reduced parameters in previous layers. Rest is the number of remaining parameters. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t-1}$\n</tex-math></inline-formula> is the compression rate of convolutional layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$t-1$\n</tex-math></inline-formula>. Following <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b1\">[25]</a>, the reward is defined as the accuracy loss <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\Delta $\n</tex-math></inline-formula>Acc:<disp-formula id=\"deqn7\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} Reward=\\Delta Acc \\tag{7}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} Reward=\\Delta Acc \\tag{7}\\end{equation*}\n</span></span></disp-formula><div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nComparisons of Different Number of Groups for ResNet-18 on CIFAR-10 Under the Same Pruning Ratio</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t1-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t1-3227619-small.gif\" alt=\"Table 1- &#10;Comparisons of Different Number of Groups for ResNet-18 on CIFAR-10 Under the Same Pruning Ratio\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>In this paper, we adopt the deep deterministic policy gradient (DDPG) <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b1\">[25]</a> to determine the optimal pruning policy that maximizes the expected reward. We use a critic network <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q(s,a;\\theta ^{Q})$\n</tex-math></inline-formula> to approximate the true action value function <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q^{\\pi} (s,a)$\n</tex-math></inline-formula>. And we define the actor-network <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\pi (s;\\theta ^{\\pi })$\n</tex-math></inline-formula> to calculate the action value <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{t}=\\pi (s_{t};\\theta ^{\\pi })$\n</tex-math></inline-formula>. Parameters <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\theta ^{Q}$\n</tex-math></inline-formula> of critic network <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Q(s,a;\\theta ^{Q})$\n</tex-math></inline-formula> is optimized by minimizing the loss function:<disp-formula id=\"deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathcal {L}(\\theta ^{Q})=E_{s_{t} \\sim E,a_{t} \\sim \\pi }[\\delta _{t}^{2}(\\theta ^{Q})]\\tag{8}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathcal {L}(\\theta ^{Q})=E_{s_{t} \\sim E,a_{t} \\sim \\pi }[\\delta _{t}^{2}(\\theta ^{Q})]\\tag{8}\\end{equation*}\n</span></span></disp-formula> For the actor parameters <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\theta ^{\\pi }$\n</tex-math></inline-formula> can be upgraded by following function:<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathcal {L}(\\theta ^{\\pi })=E_{s_{t} \\sim E}[Q^{\\pi }(s_{t},\\pi (s_{t};\\theta ^{\\pi });\\theta ^{Q})]\\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathcal {L}(\\theta ^{\\pi })=E_{s_{t} \\sim E}[Q^{\\pi }(s_{t},\\pi (s_{t};\\theta ^{\\pi });\\theta ^{Q})]\\tag{9}\\end{equation*}\n</span></span></disp-formula></p><p>In AMC <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b1\">[25]</a>, once the pretrained network is pruned, the pruned model is trained once. This process is very time-consuming. Therefore, to improve search efficiency, in our work, we first search for a pruned model that satisfies the given compression rate without considering the accuracy of the pruned model. We then train the pruned model and set the model accuracy loss threshold, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula>. If the accuracy loss is less than <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\lambda $\n</tex-math></inline-formula>, we terminate the training of the agent and save the pruned model as the final output. Compared with prior pruning techniques via RL, our proposed method is more efficiently.</p></div><div class=\"section_2\" id=\"sec3b2\"><h4>2) Group-Based Structured Pruning</h4><p>For group-based kernel pruning, we must determine the group configuration for all convolutional layers, which can affect both model accuracy and inference budget. Therefore, finding an optimal group configuration is an optimization problem that maximizes accuracy under budget constraints. From <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a> and <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig. 3</a>, we can observe that the model accuracy loss is significantly affected by different group allocations under the condition of the same model compression rate. For example, ResNet-18-2 and ResNet-18-3 reduce the approximate parameters. The accuracy loss of ResNet-18-3 is only <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$- 0.15\\%$\n</tex-math></inline-formula>, whereas that of ResNet-18-2 reaches <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$- 1.04\\%$\n</tex-math></inline-formula>. In previous studies, the group configuration is dtermined by special rules, such as using the same number of groups for all convolutional layers.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei3-3227619-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei3-3227619-small.gif\" alt=\"FIGURE 3. - Different number of groups for ResNet-18 on CIFAR-10 under the same pruning ratio.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Different number of groups for ResNet-18 on CIFAR-10 under the same pruning ratio.</p></fig></div><p class=\"links\"><a href=\"/document/9975798/all-figures\" class=\"all\">Show All</a></p></div></p><p>To find the optimal combination of the numbers of groups, it is neccessary to find the configuration that provides the highest model accuracy by trial and error, which is very time-consuming <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b2\">[24]</a>. To address this problem, we propose a novel group configuration algorithm that depends on the sparsity rate of each convolution layer via RL algorithm in <a ref-type=\"sec\" anchor=\"sec3b1\" class=\"fulltext-link\">section III-B1</a>. We can find near-optimal configuration for model accuracy by utilizing pretrained weights. In this section, we introduce the details of the group configuration algorithm.</p><p>When the group convolution is implemented using PyTorch, the number of groups <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G$\n</tex-math></inline-formula> of the standard group convolution must be the common divisor of input and output channels. Therefore, we define the solution space to determine the number of groups of the convolution layers in a convolutional neural network as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbb {G}$\n</tex-math></inline-formula>. A convolutional neural network comprises of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$N$\n</tex-math></inline-formula> convolutional layers. The set of the common divisor of each convolutional layer is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G_{l_{i}}$\n</tex-math></inline-formula>. <a ref-type=\"disp-formula\" anchor=\"deqn10-deqn11\" href=\"#deqn10-deqn11\" class=\"fulltext-link\">Equation (11)</a> computes the common divisor of the input channels <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{in}$\n</tex-math></inline-formula> and the output channel <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{out}$\n</tex-math></inline-formula>.<disp-formula id=\"deqn10-deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\mathbb {G}=&amp;\\{G_{l_{1}},G_{l_{2}},\\ldots,G_{l_{N}}\\} \\tag{10}\\\\ G_{l_{i}}=&amp;\\mathcal {F}(c_{in},c_{out}) \\tag{11}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\mathbb {G}=&amp;\\{G_{l_{1}},G_{l_{2}},\\ldots,G_{l_{N}}\\} \\tag{10}\\\\ G_{l_{i}}=&amp;\\mathcal {F}(c_{in},c_{out}) \\tag{11}\\end{align*}\n</span></span></disp-formula> If the number of groups at each convolutional layer is determined from one or the maximum group number <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_3b2\">[24]</a>, configuring the group numbers for all convolutional layers is a computational combinatorial optimization problem. For example, VGG-16 comprises of 13 convolution layers. Each convolutional layer has <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$L_{i}$\n</tex-math></inline-formula> configurable group numbers. The search space <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathbb {G}$\n</tex-math></inline-formula> for VGG-16 is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$L_{1} \\times L_{2} \\times L_{3} \\times {\\dots } \\times L_{13}=25401600000$\n</tex-math></inline-formula>. Finding the optimal solution from such a large search space is time-consuming. To overcome the shortcoming of this combinatorial optimization method, we propose a novel simple and efficient heuristic group configuration algorithm. This method is formulated as <a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\">(12)</a>.<disp-formula id=\"deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} g_{l_{i}}=\\begin{cases} G_{l_{i}}[j+1],~{1-\\frac {1}{G_{l_{i}}[j]}&gt;a\\geq 1-\\frac {1}{G_{l_{i}}[j+1]}} \\\\ G_{l_{i}}[j],\\qquad \\qquad \\qquad \\qquad \\quad ~~{a\\geq {1-\\frac {1}{G_{l_{i}}[j]}}} \\\\ 1,\\qquad \\qquad \\qquad \\qquad \\qquad \\quad ~{else} \\end{cases} \\tag{12}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} g_{l_{i}}=\\begin{cases} G_{l_{i}}[j+1],~{1-\\frac {1}{G_{l_{i}}[j]}&gt;a\\geq 1-\\frac {1}{G_{l_{i}}[j+1]}} \\\\ G_{l_{i}}[j],\\qquad \\qquad \\qquad \\qquad \\quad ~~{a\\geq {1-\\frac {1}{G_{l_{i}}[j]}}} \\\\ 1,\\qquad \\qquad \\qquad \\qquad \\qquad \\quad ~{else} \\end{cases} \\tag{12}\\end{align*}\n</span></span></disp-formula> The <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}}$\n</tex-math></inline-formula> is the configured number of groups for convolutional layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$l_{i}$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G_{l_{i}}$\n</tex-math></inline-formula> is sorted in descending order. The process of group configuration is terminated when the agent outputs is the sparsity rate of last convolutional layer.</p><p>We find when the number of groups of a convolution operation is set to two, the sparsity rate <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula> of this layer is 0.5. When the calculated the number of groups based on the sparsity ratio <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula> is between <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G_{l_{i}}$\n</tex-math></inline-formula>[j+1] and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G_{l_{i}}$\n</tex-math></inline-formula>[j], to decrease the model precision loss as much as possible, we select <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G_{l_{i}}$\n</tex-math></inline-formula>[j+1] as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}}$\n</tex-math></inline-formula>, which is the near-optimal group configuration for pretrained networks. When the number of groups is larger than the maximum the number of groups of layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$l_{i}$\n</tex-math></inline-formula>, the maximum group number is set to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}}$\n</tex-math></inline-formula> according to <a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\">(12)</a>. In our method, we set the number of groups to one when <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a$\n</tex-math></inline-formula> is less than 0.5. In other words, we do not perform pruning for the onvolutional layer when the sparsity rate is less than 0.5.</p><p>When the number of groups of each convolution layer in a network is determined, the standard convolution operation must be refactored into standard group convolution. During the reconstructing process, the original convolution kernels are reduced by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}}\\times $\n</tex-math></inline-formula>, as shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4 (b)</a>. However, to minimize the accuracy loss, it is necessary to use pretrained weights to reconstruct group convolution. In this paper, we introduce a channel-pruning method via the attention mechanism, which is a tiny auxiliary filter selector for each group, to dynamically decide which part of kernels to be selected. As each kernel is considered as a feature selector in a convolution layer, we produce a kernel attention map by the average-pooling operation to exploit the inter-kernel relationship of kernels. In short, kernel attention is computed as:<disp-formula id=\"deqn13\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} MAP(\\mathcal {K})=\\sigma (AveragePool(\\mathcal {K})) \\tag{13}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} MAP(\\mathcal {K})=\\sigma (AveragePool(\\mathcal {K})) \\tag{13}\\end{equation*}\n</span></span></disp-formula> We denote the kernels of a convolution layer as <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathcal {K}\\,\\,\\in \\,\\,\\mathbb {R} ^{C \\times k \\times k}$\n</tex-math></inline-formula>, whose attention maps are <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$MAP\\,\\,\\in \\,\\,\\mathbb {R} ^{C \\times 1 \\times 1}$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\sigma $\n</tex-math></inline-formula> denotes the sigmoid function. We sort <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$MAP$\n</tex-math></inline-formula> by descending order and select <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${C}/{g}$\n</tex-math></inline-formula> kernels from <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\mathcal {K}$\n</tex-math></inline-formula> based on the largest first <inline-formula id=\"\"><tex-math notation=\"LaTeX\">${C}/{g}$\n</tex-math></inline-formula> values. The remainng kernels are deleted. When the number of groups is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g$\n</tex-math></inline-formula>, the process of group-based channel pruning is illustrated in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig. 4 (c)</a>.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei4-3227619-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei4-3227619-small.gif\" alt=\"FIGURE 4. - Pruning strategy: (a)Standard convolution; (b)Group convolution; (c)Kernel pruning.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Pruning strategy: (a)Standard convolution; (b)Group convolution; (c)Kernel pruning.</p></fig></div><p class=\"links\"><a href=\"/document/9975798/all-figures\" class=\"all\">Show All</a></p></div></p><p>After pruning, a relatively low learning rate is used to retrain the pruned model to improve its accuracy of the pruned model. The overall procedure of our propoed method is summarized in <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Algorithm 1</a>.<div class=\"algorithm\" id=\"alg1\"><h4>Algorithm 1 AGSPRL Algorithm Procedure</h4><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M$\n</tex-math></inline-formula>:Pre-trained model; <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{train}$\n</tex-math></inline-formula>: Training dataset; <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X_{test}$\n</tex-math></inline-formula>: Testing dataset.</p><p>The compressed network architecture <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M_{pruned}$\n</tex-math></inline-formula> and group weight <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula></p><p>Compute <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$I$\n</tex-math></inline-formula> ={<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i~\\mid $\n</tex-math></inline-formula> comvolutional layer<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> need to be pruned}</p><p>Compute parameters of each layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i~\\in ~I$\n</tex-math></inline-formula>;</p><p><b>for</b> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i=1$\n</tex-math></inline-formula>; <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i &lt; n$\n</tex-math></inline-formula>; <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i++$\n</tex-math></inline-formula> <b>do</b></p><p>Compute agent\u2019s action <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a_{i}$\n</tex-math></inline-formula> as sparsity ratio of layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula></p><p>Compute the set <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$G_{l_{i}}$\n</tex-math></inline-formula> of group numbers of layer <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula></p><p><b>if</b> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1-1/G_{l_{i}}[j] &gt;a\\geq 1-1/G_{l_{i}}[j+1]$\n</tex-math></inline-formula> <b>then</b></p><p>Compute <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}} = G_{l_{i}}[j+1]$\n</tex-math></inline-formula></p><p><b>else if</b> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$a\\geq 1-1/G_{l_{i}}[j]$\n</tex-math></inline-formula> <b>then</b></p><p>Compute <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}} = G_{l_{i}}[j]$\n</tex-math></inline-formula></p><p><b>else</b></p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$g_{l_{i}} = 1$\n</tex-math></inline-formula></p><p><b>end if</b></p><p><b>for</b> kernels <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K_{l_{i}}^{C_{in} \\times k \\times k}\\,\\,\\in \\,\\,K^{C_{out} \\times C_{in} \\times k \\times k}$\n</tex-math></inline-formula> <b>do</b></p><p>Compute <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M_{map}$\n</tex-math></inline-formula> of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K_{l_{i}}^{C_{in} \\times k \\times k}$\n</tex-math></inline-formula> by AvgPooling</p><p>Select <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C_{in}/g_{l_{i}}$\n</tex-math></inline-formula> kernels from <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$K_{l_{i}}^{C_{in}}$\n</tex-math></inline-formula></p><p><b>end for</b></p><p><b>end for</b></p><p>Return the whole pruned network <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$M_{pruned}$\n</tex-math></inline-formula> and group weight <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$W$\n</tex-math></inline-formula></p></div></p></div></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Experiments</h2></div><p>In this section, we verify the effectiveness of the proposed approach on the state-of-the-art networks ResNet-18/ 34/50/56 <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_4\">[4]</a> and VGG-16/19 <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_4\">[2]</a> with popular but very different datasets for image classification: CIFAR-10/100 <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_4\">[44]</a>, <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_4\">[45]</a>. Moreover, we prune SSD300 with backbone VGG-16 <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_4\">[2]</a> on Pascal VOC 2007/2012 <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_4\">[46]</a>, <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_4\">[47]</a> for object detection, and compare our proposed approach with several state-of-the-art pruning methods. All experiments are implemented using PyTorch. We use the pretrained networks and hyperparameter settings from OpenMMLab <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_4\">[48]</a> to obtain better benchmark accuracy and compared it with existing network pruning methods. All experiments are performed with a single NVIDIA GeForce RTX 3090 GPU and Intel Xeon(R) Silver 4215R CPU@ 3.20GHz<inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\times 16$\n</tex-math></inline-formula>.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Datasets and Setting</h3><p>CIFAR-10/100 has 60,000 RGB images of size <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$32 \\times 32$\n</tex-math></inline-formula> pixels, with 50,000 training images and 10,000 testing images in 10 and 100 classes, respectively. A common data augmentation scheme including random cropping and mirroring <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_4a\">[4]</a>, <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_4a\">[49]</a> is adopted <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_4a\">[10]</a>. After pruning, the pruned model is retrained for 200 epochs on CIFAR-10 or CIFAR-100. The learning rate is initially set to 0.01 and then decreased by a factor of 10 on 50,100,150 epochs. During training, all networks are trained using stochastic gradient descent (SGD), and the batch size is set to 64 for CIFAR-10 and CIFAR-100. We use a weight decay of 10<sup>\u22124</sup> and momentum of 0.9.\u2019 After retraining, to further improve the accuracy of the pruned models, we also apply knowledge distillation <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_4a\">[33]</a> to fine-tune the pruned models.</p><p>To further analyze the generalization of our method, we conduct object detection experiments on Pascal VOC 2007/2012 <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_4a\">[46]</a>, <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_4a\">[47]</a> datasets. The training images of Pascal VOC 2007/2012 are combined for training, and then 4963 testing images in Pascal VOC 2007 are used to estimate performance <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_4a\">[50]</a>. The performance is evaluated by mean average precision (mAP). After pruning, we retrain pruned models using SGD and 24 epochs on PASCAL VOC. The learning rate is initially set to 0.001 and then decreased by a factor of 10 on 16 and 20 epochs.</p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Results on CIFAR-10</h3><p>We first evaluate the proposed group-based pruning method on popular network architectures ResNet-18/34/50 and VGG-16/19 on CIFAR-10 and compare it with manually setting fixed groups G=2 and G=4 on ResNet-18. We use pruning rates of 30%, 50%, and 70% for our proposed AGSPRL. The results are shown in <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>, <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>,and <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>. From <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>, for a compression ratio of 30%, the accuracy losses of ResNet-18/34/50 are no more than 0.30% without any fine-tuning training. For ResNet-34, we remove 50.75% of the parameters and 40.92% of FLOPs, with a slight improvement compared with the baseline network. When the pruning rate of parameters is 70%, the performance of the three networks does not decrease by more than 1.0%. The experimental results show that the deep CNN does have certain redundancy, and the moderate compression will not affect its accuracy. When the pruning ratio is small, removing some redundant parameters in the network can improve generalization, leading to an increase in performance. When ResNet-34 is compressed to 10.48M, the classification performance on CIFAR-10 exceeded that of original ResNet-34 by 21.28M.<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nPruning Results of ResNet-18/34/50 on CIFAR-10 Without Fine-Tuning</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t2-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t2-3227619-small.gif\" alt=\"Table 2- &#10;Pruning Results of ResNet-18/34/50 on CIFAR-10 Without Fine-Tuning\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nComparisons of Different the Number of Groups for ResNet-18 on CIFAR-10 Without Fine-Tuning</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t3-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t3-3227619-small.gif\" alt=\"Table 3- &#10;Comparisons of Different the Number of Groups for ResNet-18 on CIFAR-10 Without Fine-Tuning\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nPruning Results of VGG-16/19 on CIFAR-10 With Fine-Tuning</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t4-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t4-3227619-small.gif\" alt=\"Table 4- &#10;Pruning Results of VGG-16/19 on CIFAR-10 With Fine-Tuning\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>In <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>, \u2018G=2\u2019 represents the group convolution with two groups for all convolutional layers and \u2018G=4\u2019 represents the group convolution with four groups for all convolutional layers. When the number of groups of each convolution layer is set to two or four, ResNet-18\u2019s accuracy is reduced by 1.9% or 3.18%. However, for almost the same number of reduced parameters, there is no loss of precision when ResNet-18 is pruned 50% of the parameters by the proposed method AGSPRL. When 70% of parameters for ResNet-18 are removed, its accuracy drops by only 1.0%. Moreover, we analyze the influence of the number of groups. The number of groups in each convolutional layer is visualized, as shown in <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig. 5</a>. It can be seen that the proposed AGSPRL can assign different the number of groups to different convolutional layers. Compared with the manually configured fixed the number of groups, redundancy can be pruned more appropriately without decreasing model accuracy, as shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>. The proposed method achieves better accuracy than the manual configuration under a similar compression rate. This implies that our AGSPRL can retain more useful information than a fixed the number of groups.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei5-3227619-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei5-3227619-small.gif\" alt=\"FIGURE 5. - Different the number of groups for ResNet-18 on CIFAR-10.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Different the number of groups for ResNet-18 on CIFAR-10.</p></fig></div><p class=\"links\"><a href=\"/document/9975798/all-figures\" class=\"all\">Show All</a></p></div></p><p>To further validate the effectiveness of the proposed AGSPRL, we perform experiments on VGG-16 and VGG-19 on the CIFAR-10 dataset. For VGG-16 and VGG-19, our AGSPRL prunes all convolutional layers using the RL-learned rates. <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a> presents the results for three different pruning rates: 30%, 50% and 70%. The accuracy of the baseline VGG-16 model is 93.41%. For VGG-16, we remove 73.84% of the parameters and 50.83% of the FLOPs while maintaining an accuracy of 92.83% with distilling fine-tuning training <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_4b\">[33]</a>. We remove 51.82% of the parameters and 31.16% of the FLOPs for VGG-19 with a performance reduction of only 0.30%. Generally, as the compression rate increases, the accuracy of the pruned model gradually decreases. When the parameters and FLOPs of VGG-19 are discarded by 65.73% and 53.16%, respectively, the precision is improved by 0.87% using our proposed method. This further demonstrates that the proposed method can achieve effective compression and acceleration for CNNs.</p></div><div class=\"section_2\" id=\"sec4c\"><h3>C. Results on CIFAR-100</h3><p>We continue the experiment on CIFAR-100. CIFAR-100 and CIFAR-10 have the same number of images, but the number of categories is increased from 10 to 100. As the number of training images in each class decreases, the performance of the network also decreases significantly. First, we prune the ResNet-50 on CIFAR-100 without any fine-tuning training. We arrange two pruning rates, 30% and 50%, for ResNet-50; and the results are reported in <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a>. The accuracy of the pruned network at a compression rate of 30% increased by 0.31%. When 50.61% of the parameters of ResNet-50 are removed, the accuracy of ResNet-50 decreases by 0.82%.<div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nPruning Results of ResNet-50 on CIFAR-100 Without Fine-Tuning</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t5-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t5-3227619-small.gif\" alt=\"Table 5- &#10;Pruning Results of ResNet-50 on CIFAR-100 Without Fine-Tuning\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>We then prune VGG-16 and ResNet-56 on CIFAR-100. <a ref-type=\"table\" anchor=\"table6\" class=\"fulltext-link\">Table 6</a> presents the results. The initial classification accuracy of VGG-16 on CIFAR-100 is only 72.16%. When pruning 29.85% and 52.55% of the parameters, the accuracy of the pruned model with distilling fine-tuning training <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_4c\">[33]</a> is 0.03% and 0.04% higher than that of the baseline model. For ResNet-56 on CIFAR-100, when pruning 47.47% of the parameters, the accuracy decreases by only 0.05%. The results demonstrated that the proposed method is effective and robust across a wide range of networks.<div class=\"figure figure-full table\" id=\"table6\"><div class=\"figcaption\"><b class=\"title\">TABLE 6 </b>\nPruning Results of VGG-16/ResNet-56 on CIFAR-100 With Fine-Tuning</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t6-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t6-3227619-small.gif\" alt=\"Table 6- &#10;Pruning Results of VGG-16/ResNet-56 on CIFAR-100 With Fine-Tuning\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4d\"><h3>D. Comparison With Other Methods on CIFAR-10/100</h3><p>In this subsection, we compare the methods on CIFAR-10 and CIFAR-100. Among the compared methods, KPGP <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_4d\">[42]</a>, SFP <a ref-type=\"bibr\" anchor=\"ref51\" id=\"context_ref_51_4d\">[51]</a>, FPGM <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_4d\">[49]</a> and ACP <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_4d\">[50]</a> are the state-of-the-art kernel pruning methods. The results of these competing methods have been reported in the original articles. For the remainder of this paper, the pruning rate is set to different values in comparative experiments to prove the effectiveness of our method. First, we compare the methods of pruning VGG-16 on CIFAR-10. The results are listed in <a ref-type=\"table\" anchor=\"table7\" class=\"fulltext-link\">Table 7</a>. As we can see, our proposed AGSPRL is superior to the KPGP <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_4d\">[42]</a> in terms of parameter compression rate and performance after pruning. For example, when AGSPRL reduces approximately 30.43% of the parameters of VGG-16, the reduction in accuracy is only 0.03, which is lower than the 0.1% of KPGP (25%). Moreover, the results in <a ref-type=\"table\" anchor=\"table7\" class=\"fulltext-link\">Table 7</a> show that under 50% and 70% pruning rates, compared with KPGP, our proposed method AGSPRL always achieves the best results. When the parameters are reduced by 70%, the accuracy of AGSPRL is only 0.58% lower than that of the baseline model, whereas the accuracy of KPGP drops by 1.91%. The superior performance indicates that AGSPRL can effectively prune VGG-16 on CIFAR-10.<div class=\"figure figure-full table\" id=\"table7\"><div class=\"figcaption\"><b class=\"title\">TABLE 7 </b>\nComparisons of Different Pruning Methods for VGG-16 on CIFAR-10</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t7-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t7-3227619-small.gif\" alt=\"Table 7- &#10;Comparisons of Different Pruning Methods for VGG-16 on CIFAR-10\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>For CIFAR-100 dataset, we compare the experimental results of pruning ResNet-56, as presented in <a ref-type=\"table\" anchor=\"table8\" class=\"fulltext-link\">Table 8</a>. The pruning accuracy of our AGSPRL decreases by only 0.05%. Compared with other methods, the proposed method obtains better performance. All the results show that the proposed AGSPRL performs well on CIFAR-10 and CIFAR-100 image classification tasks.<div class=\"figure figure-full table\" id=\"table8\"><div class=\"figcaption\"><b class=\"title\">TABLE 8 </b>\nComparisons of Different Methods for ResNet-56 on CIFAR-100</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t8-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t8-3227619-small.gif\" alt=\"Table 8- &#10;Comparisons of Different Methods for ResNet-56 on CIFAR-100\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4e\"><h3>E. Results on Pascal VOC</h3><p>As described above, our proposed method is effective for image classification. To further analyze our method, we conduct experiments on object detection. Pruning object detection is more challenging than image classification due to its larger input size and more complicated networks.</p><p>We select SSD300 with VGG-16 backbone on PASCAL VOC dataset. The training images of PASCAL VOC 2007/2012 are combined for training, and 4963 testing images of PASCAL VOC 2007 are used to estimate performance. The mAP with an IoU threshold of 0.50 is customary for evaluating this dataset. We train SSD300 from scratch using SGD and 24 epochs on PASCAL VOC. The learning rate is initially set to 0.001 and then decreased on 16 and 20 epochs. The mAP of SSD300 is 74.03% as the accuracy of the baseline model. From <a ref-type=\"table\" anchor=\"table9\" class=\"fulltext-link\">Table 9</a>, it can be seen that SSD300 has a significant redundancy on PASCAL VOC. When 35.41% of the parameters are removed, the mAP has a negligible loss of accuracy of 0.03%. The experiment proves that our AGSPRL also has good generalization ability and performance for object detection.<div class=\"figure figure-full table\" id=\"table9\"><div class=\"figcaption\"><b class=\"title\">TABLE 9 </b>\nPruning Results of SSD300 on PASCAL VOC</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t9-3227619-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei.t9-3227619-small.gif\" alt=\"Table 9- &#10;Pruning Results of SSD300 on PASCAL VOC\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>We further examine AGSPRL pruned models by visualizing their detection results and comparing them with those of the original baseline model. As an example, <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6</a> shows the detection results from the original SSD300 and pruned SSD300 on PASCAL VOC.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei6-3227619-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9975798/wei6-3227619-small.gif\" alt=\"FIGURE 6. - Detection results of SSD300.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Detection results of SSD300.</p></fig></div><p class=\"links\"><a href=\"/document/9975798/all-figures\" class=\"all\">Show All</a></p></div></p><p>The original images are obtained from the validation set of PASCAL VOC 2017, and the detection results of the original SSD300 are shown in <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6 (a) and (c)</a>. The <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6 (b) and (d)</a> show the detection results of pruned SSD300 using our AGSPRL method with a pruning rate of 33.47%. From <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig. 6</a>, even if the mAP of the pruned SSD300 model is reduced by 0.03%, we can see that the anchors from pruned SSD300 are not reduced, but are more confident in their prediction.</p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion</h2></div><p>In this paper, we propose a novel automatic group-based structured pruning method via reinforcement learning algorithm, which not only improves group configuration optimization solutions, but can also effectively reduce the model size with negligible loss of accuracy. Different kernels are chosen for different output channel groups using an attention-based group pruning strategy. Finally, a compressed neural network is constructed by the AGSPRL method, which can be lightweight and efficiently implemented. The empirical results show that AGSPRL achieves the best overall best performance compared with other state-of-the-art group-based pruning methods. In the future, we plan to research the combinations of grouped-based pruning and other pruning methods to balance the model size and accuracy preferably.</p></div>\n</div></div></response>\n"
}