{
    "abstract": "Food image classification is useful in diet management apps for personal health management. Various methods for classifying food images in a particular country have been proposed in multiple studies. However, knowledge of Korean food image classification is limited. The objective of this study was to train a classification model for Korean food images. To train the classification model, we collect...",
    "articleNumber": "9976036",
    "articleTitle": "Development of Korean Food Image Classification Model Using Public Food Image Dataset and Deep Learning Methods",
    "authors": [
        {
            "preferredName": "Minki Chun",
            "normalizedName": "M. Chun",
            "firstName": "Minki",
            "lastName": "Chun",
            "searchablePreferredName": "Minki Chun"
        },
        {
            "preferredName": "Hyeonhak Jeong",
            "normalizedName": "H. Jeong",
            "firstName": "Hyeonhak",
            "lastName": "Jeong",
            "searchablePreferredName": "Hyeonhak Jeong"
        },
        {
            "preferredName": "Hyunmin Lee",
            "normalizedName": "H. Lee",
            "firstName": "Hyunmin",
            "lastName": "Lee",
            "searchablePreferredName": "Hyunmin Lee"
        },
        {
            "preferredName": "Taewon Yoo",
            "normalizedName": "T. Yoo",
            "firstName": "Taewon",
            "lastName": "Yoo",
            "searchablePreferredName": "Taewon Yoo"
        },
        {
            "preferredName": "Hyunggu Jung",
            "normalizedName": "H. Jung",
            "firstName": "Hyunggu",
            "lastName": "Jung",
            "searchablePreferredName": "Hyunggu Jung"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227796",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9976036/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Food image classification is useful in diet management apps for personal health management. For example, the mobile food image classification used in diet management apps enables users to determine their daily dietary requirements and calorie intake <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>. However, food classification based on image processing and computer vision is challenging because the visual differences between food classes are not significant <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>. Furthermore, Asian foods, including Korean foods, generally have more diverse recipes than Western foods <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>. It is also difficult to define the structure of food because food can be deformed, such as by placing or hiding some ingredients in entirely different positions, depending on the cooking method <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a>. Therefore, it is not easy to specify a class of food through image classification because foods may be visually similar to foods of other classes and visually different from foods in the same class.</p><p>Prior studies attempted to solve the difficult task of food image classification using traditional machine learning (ML) or deep learning (DL) methods. The traditional ML methods that have been used include (1) using color histogram and Gabor texture method for feature extraction, (2) combining an individual\u2019s diet pattern with the results of image analysis, (3) using neural networks, (4) using support vector machine (SVM)-based image segmentation <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, and (5) using food images of specific domains <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>. DL methods are widely used to train food image classification models. The DL-based methods differ according to whether fine-tuning was performed <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a> or not. Several researchers trained a classification model using fine-tuning. Among them, some proposed a system using the model and others did not <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>.</p><p>Nevertheless, none of these researchers proposed a method for Korean food image classification using the public Korean food image dataset. Thus, a food image classification model that classifies more than 100 types of Korean food was developed in this study using a public food image dataset. Besides, although Korean foods that belong to the same food category have similar characteristics, the differences between food categories have been simplified or ignored in previous studies. Hence, we considered the differences between the 27 food categories defined by the Korea Culture and Information Service.<a ref-type=\"fn\" anchor=\"fn1\" class=\"footnote-link\">1</a> Analysis of the Korean food image dataset is still in its infancy. In this study, we examined the characteristics of Korean food image dataset that affect the classification accuracy.</p><p>We trained a model based on the convolutional neural network (CNN) that classifies 150 classes of Korean food images using a Korean food image dataset. First, we obtained 150 classes of Korean food image datasets from the AI-Hub platform <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a> operated by the National Information Society Agency.<a ref-type=\"fn\" anchor=\"fn2\" class=\"footnote-link\">2</a> AI-Hub platform is a data repository for sharing datasets with model developers training their artificial intelligence models. AI-Hub platform enables them to download publicly available data, such as text data (e.g., corpus data, machine reading comprehension data) and image data (e.g., Korean food image data, eye gaze data).</p><p>Second, we reduced model complexity and enhanced accuracy through image pre-processing. Subsequently, we augmented the pre-processed image set to prevent overfitting. Third, we extracted features using augmented image datasets and pre-trained deep neural networks (DNNs) (i.e., ResNet-50, ResNet-101, ResNet-152, MobileNetV2, InceptionResNetV2, and NasNetLarge) and fine-tuned our trained model. We also used the max-pooling, density, and dropout layers to train the classification model. We then compared the performance (e.g., accuracy and training time) of the DNNs. Hence, our study makes the following contributions to the target users of the model, which automatically classifies visually similar Korean food images, and the community of researchers who use the Korean food image dataset:\n<ul style=\"list-style-type:disc\"><li><p>We present a classification model that automatically classifies images of the visually similar classes of Korean food into 150 food classes.</p></li><li><p>We present a pre-processed dataset for training Korean food image classification models.</p></li><li><p>We present the level of accuracy and learning time of several pre-trained DNNs in training a model to classify Korean food images into 150 food classes.</p></li></ul></p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Related Work</h2></div><p>Previous studies attempted to classify food images using ML algorithms using traditional ML or DL. We also found studies that created and evaluated food image classification models in specific domains.</p><div class=\"section_2\" id=\"sec2a\"><h3>A. Traditional ML-Based Food Image Classification</h3><p>Prior studies created and evaluated ML-based food image classification models <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_2a\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_2a\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2a\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2a\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2a\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2a\">[16]</a>, <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2a\">[17]</a>, <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2a\">[18]</a>, <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2a\">[19]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2a\">[20]</a>, <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2a\">[21]</a>, <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2a\">[22]</a>, <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2a\">[23]</a>. To train food image classification models, ten studies used the images of food items whose origins are unknown (e.g., self-made dataset, PFID dataset) <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_2a\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_2a\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2a\">[8]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2a\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\">[10]</a>, <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2a\">[16]</a>, <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2a\">[17]</a>, <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2a\">[18]</a>, <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2a\">[19]</a>, <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2a\">[20]</a>. Out of the ten studies, five proposed food image classification models using a dataset created with images from the web <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_2a\">[7]</a>, <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2a\">[9]</a>, <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_2a\">[18]</a> and personal devices <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_2a\">[16]</a>, <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2a\">[17]</a>. For example, food images from the web were classified by the model that uses the histogram of oriented gradients, scale-invariant feature transforms, local binary patterns, and filter responses <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_2a\">[17]</a>, while an SVM with the scatter search approach was used to classify food images from personal devices into multiple classes <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_2a\">[9]</a>. Meanwhile, the other studies created a food image classification model using a dataset that have already been created (i.e., Food-101 dataset <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_2a\">[10]</a>, PFID dataset <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_2a\">[19]</a>, Purdue dataset <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_2a\">[20]</a>, UNICT-FD889 dataset <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_2a\">[1]</a>, <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2a\">[8]</a>). For instance, the UNICT-FD889 dataset contains 3,583 food images related to 889 different plates <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2a\">[8]</a>. Farinella et al. <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_2a\">[8]</a> used the dataset for creating an image classification model that enables classifying images into food and non-food images.</p><p>On the contrary, four studies used the images of food items that belong to a specific country (e.g., China <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2a\">[11]</a>, Indonesia <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2a\">[22]</a>, <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2a\">[23]</a>, Japan <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2a\">[21]</a>) when training their proposed food image classification models. They created food image classification models using images from the web <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2a\">[21]</a> and personal devices <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_2a\">[11]</a>, <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_2a\">[22]</a>, <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2a\">[23]</a>, respectively. In particular, food images from the web were classified by the model that uses the bag-of-features <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2a\">[24]</a>, a color histogram, a gradient histogram, and the Gabor texture <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2a\">[23]</a>, while a random forest method <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_2a\">[25]</a> was used to classify food images from personal devices into multiple classes <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_2a\">[21]</a>.</p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. Deep Learning-Based Food Images Classification</h3><p>DL-based food image classification methods are widely used because they enable various systems to extract and classify image features. Prior studies created and evaluated DL-based food image classification models <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2b\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2b\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2b\">[14]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2b\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>, <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2b\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2b\">[29]</a>, <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2b\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2b\">[31]</a>, <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2b\">[32]</a>, <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2b\">[33]</a>, <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\">[34]</a>, <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2b\">[35]</a>, <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2b\">[36]</a>, <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2b\">[37]</a>, <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2b\">[38]</a>. To train food image classification models, researchers used the images of food items whose origins are unknown (e.g., Food-475 dataset, Food-524 dataset) <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2b\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2b\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2b\">[14]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2b\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>, <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2b\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2b\">[29]</a>, <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2b\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2b\">[31]</a>, <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2b\">[32]</a>, <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2b\">[33]</a>. Out of eleven studies, eight proposed food image classification models using a fine-tuning method in order to reduce training time and enhance model accuracy <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2b\">[13]</a>, <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_2b\">[14]</a>, <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_2b\">[28]</a>, <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2b\">[29]</a>, <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_2b\">[30]</a>, <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_2b\">[31]</a>, <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_2b\">[32]</a>, <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_2b\">[33]</a>. For example, Islam et al. <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2b\">[13]</a> used Inception V3 and a fine-tuning method to enhance the accuracy of the proposed model by about 16 percent over the model accuracy of the previously proposed convolutional neural network or AlexNet <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2b\">[39]</a>. Similarly, Pan et al. <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_2b\">[29]</a> used ResNet-50 <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_2b\">[40]</a> and a fine-tuning method to enhance the accuracy of the proposed model by about 10 percent over the model accuracy of AlexNet <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2b\">[39]</a>.</p><p>While a fine-tuning method is being widely used for training image classification models, some researchers proposed food image classification models without using the fine-tuning method <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2b\">[12]</a>, <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2b\">[26]</a>, <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a>. For example, Samraj et al. <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_2b\">[12]</a> developed a visual food classification model to prevent errors in handling large amounts of food. They created an image classification model that classifies 170 food image classes using a dataset of food items such as Asian vegetarian meals and Bland meals. Similarly, Christodoulidis et al. <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_2b\">[26]</a> proposed a method of extracting patches from food images and then feeding them to a CNN. They used a dataset of 246 images of food items served in the restaurants of Bern University hospital. In addition, Memi\u015f et al. <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_2b\">[27]</a> compared the performance of several DL image classification models that classify 100 categories of food images using the UEC FOOD 100 dataset, and reported that ResNet-50 showed the best classification performance <a ref-type=\"bibr\" anchor=\"ref41\" id=\"context_ref_41_2b\">[41]</a>.</p><p>In contrast with the aforementioned studies, five studies used the images of food items that belong to a specific country (e.g., Australia <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\">[34]</a>, Brazil <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2b\">[35]</a>, Japan <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2b\">[36]</a>, Korea <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2b\">[37]</a>, <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2b\">[38]</a>) when training their proposed food image classification models. While three studies proposed classification models without using a fine-tuning method <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_2b\">[35]</a>, <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_2b\">[36]</a>, <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2b\">[37]</a>, for training a food image classification model, two studies used a fine-tuning method <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\">[34]</a>, <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2b\">[38]</a>. For example, Islam et al. <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_2b\">[34]</a> used a fine-tuning method to enhance the accuracy of the proposed model by about four percent over the model accuracy of the previously proposed AlexNet <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_2b\">[39]</a>.</p></div><div class=\"section_2\" id=\"sec2c\"><h3>C. Limitations of Prior Studies</h3><p>Based on the aforementioned studies, we found a few limitations as follows. First, although previous studies proposed image classification models trained using food from various countries, only few used the Korean food image dataset. Moreover, the researchers who developed the Korean food image classification models pointed out that the dataset was insufficient <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_2c\">[37]</a>, <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_2c\">[38]</a>. We trained our model on a larger dataset consisting of 27 food categories, including 150 food classes, which may lead to the discovery of additional challenges and opportunities in classifying Korean food images. Second, understanding of the classification performance for each Korean food category is limited. Although Korean foods in the same food category have similar characteristics, food categories have been simplified or ignored in previous studies. The dataset used in this study contained 27 food categories defined by the Korean Culture and Information Service <a ref-type=\"bibr\" anchor=\"ref42\" id=\"context_ref_42_2c\">[42]</a>. Few models for classifying Korean food images classification have been proposed in a previous study. However, the analysis of the Korean food image dataset is insufficient. Therefore, we examined the characteristics of the Korean food image dataset that affect classification accuracy. Investigating food images is expected to provide the researchers with opportunities to improve Korean food classification models.</p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>Methods</h2></div><p>The objective of this study was to develop a classification model for Korean food image. We collected images from a Korean food image dataset to train a model for classifying Korean food images. We obtained image data for model training through image pre-processing and augmentation. The classification model was then trained using the obtained data. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Figure 1</a> shows the training process of the classification model. Each light blue box matches one to one with subsections in the Methods section.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung1abcd-3227796-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung1abcd-3227796-small.gif\" alt=\"FIGURE 1. - Training process of classification models. (A) Collect Korean food images from the AI-Hub platform. (B) Pre-process image to reduce the complexity of the model network, increase the classification accuracy, and prevent overfitting. (C) Load pre-trained feature extraction network and add last classifier layers. (D) Initial training and fine-tuning the trained model.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 1. </b><fig><p>Training process of classification models. (A) Collect Korean food images from the AI-Hub platform. (B) Pre-process image to reduce the complexity of the model network, increase the classification accuracy, and prevent overfitting. (C) Load pre-trained feature extraction network and add last classifier layers. (D) Initial training and fine-tuning the trained model.</p></fig></div><p class=\"links\"><a href=\"/document/9976036/all-figures\" class=\"all\">Show All</a></p></div></p><div class=\"section_2\" id=\"sec3a\"><h3>A. Dataset</h3><p>We used a Korean food image dataset provided by the AI-Hub platform. Each image of the dataset was collected by researchers using a web crawler from Korea Institute of Science and Technology. As reported by the Ministry of Agriculture, Food and Rural Affairs and the Korean Food Foundation, the AI-Hub platform enabled researchers to navigate 27 categories of 150 food classes (see <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Table 1</a>). As a result, we obtained an average of 1,004 images (SD=17.02) per class and a total of 150,610 images from the platform.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nList of Food Categories in Korean Food Data</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t1-3227796-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t1-3227796-small.gif\" alt=\"Table 1- &#10;List of Food Categories in Korean Food Data\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Image Pre-Processing</h3><p>We reduced the complexity of the model network and increase classification accuracy by pre-processing the food images before training the CNNs. We split the entire image dataset into 105,427 images for training, 30,122 images for testing, and 15,061 images for validation. All the images were resized to 331 pixels <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\times331$\n</tex-math></inline-formula> pixels to fit the input of the model.</p><p>We applied data augmentation using random seeds to our training dataset to obtain additional data. The images obtained by data augmentation were as follows: (1) original training images; (2) images with brightness changed between 0.5 and 1.5 times using random seeds; (3) images left in which we changed the saturation from 0.5 to 0.8 using random seeds; (4) images in which we changed the contrast from 1.2x to 1.7x using a random seed; (5) images flipped horizontally. The training data used for model training were a total of 527,135 images, a five-fold increase from 105,427 images selected for use in model training.</p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Convolutional Neural Network Model</h3><p>We employed a CNN, which is widely used to classify objects in an image, to extract image features. In this study, ResNet-50V2, ResNet-101V2, ResNet-152V2 <a ref-type=\"bibr\" anchor=\"ref43\" id=\"context_ref_43_3c\">[43]</a>, InceptionResNetV2 <a ref-type=\"bibr\" anchor=\"ref44\" id=\"context_ref_44_3c\">[44]</a>, NasNetLarge <a ref-type=\"bibr\" anchor=\"ref45\" id=\"context_ref_45_3c\">[45]</a>, and MobileNetV2 <a ref-type=\"bibr\" anchor=\"ref46\" id=\"context_ref_46_3c\">[46]</a> models pre-trained with ImageNet data <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_3c\">[39]</a> were used to classify Korean food. We connected the pre-trained feature extraction network to the classification model network, which consists of global average pooling layers, fully connected layers, and dropout layers. <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Figure 2</a> describes the classification model structure.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung2-3227796-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung2-3227796-small.gif\" alt=\"FIGURE 2. - Architecture of the classification model layer.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 2. </b><fig><p>Architecture of the classification model layer.</p></fig></div><p class=\"links\"><a href=\"/document/9976036/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3d\"><h3>D. Classification</h3><p>We trained each Korean food image classification model. Each classification model was trained through two stages: initial training and fine-tuning stages. The aim of the initial training stage was to obtain pre-trained classification models using ImageNet data <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_3d\">[39]</a>. During the initial training stage, we trained only the parameters of each classification model. To fix the parameters of the feature extraction network, we set multiple values: the batch size to 128, the training epoch value to 1,000, and the early-stopping value to 50, respectively. We used the Adam optimizer with a 0.001 learning rate and adopted categorical cross-entropy as a loss function. To fit each trained classification model to the Korean food image data, we trained each model again by setting the learning rate to 0.0001 instead of 0.001.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Results</h2></div><div class=\"section_2\" id=\"sec4a\"><h3>A. Model Evaluation</h3><p>The trained model was evaluated using images from the evaluation set. We used ten percent of the Korean food images collected from the AI-Hub platform for model performance evaluation. The accuracy was measured using the following formula:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\mathit {Accuracy}= \\frac {\\mathit {TP}+\\mathit {TN}}{\\mathit {TP}+\\mathit {FP} + \\mathit {TN}+\\mathit {FN}}\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\mathit {Accuracy}= \\frac {\\mathit {TP}+\\mathit {TN}}{\\mathit {TP}+\\mathit {FP} + \\mathit {TN}+\\mathit {FN}}\\tag{1}\\end{equation*}\n</span></span></disp-formula> where TP is the true positive, TN is the true negative, FP is the false positive, and FN is the false negative.</p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Obtained Results</h3><p>We evaluated the classification performance of each model using an evaluation image set and found out commonalities and differences between the models. We report accuracy of each model and accuracy for each food category. The average top-1 recall of all the transfer-learning-based models was 0.6. The model based on InceptionResNetV2 outperformed the other pre-trained models (see <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">Table 2</a>).<div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nEvaluation Accuracy of Each Model Used in This Study. InceptionResNetV2 and ResNet-50V2 Showed the Highest and Lowest Evaluation Accuracy, Respectively</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t2-3227796-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t2-3227796-small.gif\" alt=\"Table 2- &#10;Evaluation Accuracy of Each Model Used in This Study. InceptionResNetV2 and ResNet-50V2 Showed the Highest and Lowest Evaluation Accuracy, Respectively\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>We found that each model, using different feature extraction networks, shared a common result. According to the evaluation results, the recall and precision for each food category showed large differences (see <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>). The recall of the 27 main categories, including the rest of the food types, is shown in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Figure 3</a>, and the precision and recall of each food type are shown in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Figure 4</a> with representative images of each food. The classification results for the images of foods belonging to Jeongol/Jjigae/Tang had a low recall and precision. On the other hand, food images in major classes such as Juk/Namul/Eumcheongryu showed the highest recall and precision of all the food classes, relatively.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nClassification Recall, Precision and f1-Score of 27 Korean Food Categories in Descending Order Based on Recall</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t3-3227796-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t3-3227796-small.gif\" alt=\"Table 3- &#10;Classification Recall, Precision and f1-Score of 27 Korean Food Categories in Descending Order Based on Recall\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung3-3227796-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung3-3227796-small.gif\" alt=\"FIGURE 3. - Box-plot describing evaluation results of InceptionResNetV2 using 27 categories of Korean food data. Circles represent the TP rate of each food class. X-marks represent the average value of the TP rate of each food category. Boxes represent first and third quartile value of the TP rate of each food category. A horizontal line in each box represents the middle value of the TP rate of each food category. A horizontal line at the end of each whisker represents the lowest or highest TP rate of each category, excluding any outliers. The whiskers of the box-plot are drawn within the 1.5 interquartile range.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 3. </b><fig><p>Box-plot describing evaluation results of InceptionResNetV2 using 27 categories of Korean food data. Circles represent the TP rate of each food class. X-marks represent the average value of the TP rate of each food category. Boxes represent first and third quartile value of the TP rate of each food category. A horizontal line in each box represents the middle value of the TP rate of each food category. A horizontal line at the end of each whisker represents the lowest or highest TP rate of each category, excluding any outliers. The whiskers of the box-plot are drawn within the 1.5 interquartile range.</p></fig></div><p class=\"links\"><a href=\"/document/9976036/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung4-3227796-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung4-3227796-small.gif\" alt=\"FIGURE 4. - Recall and precision of each 27 categories of Korean food data on the coordinate plane. The X- and Y-Axes represents the recall and precision, respectively, of each category. The food images placed at each coordinate are representative of the corresponding food category.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 4. </b><fig><p>Recall and precision of each 27 categories of Korean food data on the coordinate plane. The X- and Y-Axes represents the recall and precision, respectively, of each category. The food images placed at each coordinate are representative of the corresponding food category.</p></fig></div><p class=\"links\"><a href=\"/document/9976036/all-figures\" class=\"all\">Show All</a></p></div></p><p>In the classification of 150 classes of food images, the classification model exhibited high accuracy in classifying images of spinach salad, beef tartare, and braised lotus roots. However, the accuracy of the classification model was significantly lower when classifying images of some foods such as Dongtae Jjigae, Maeuntang, and Gopchang Jeongol (see <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>). Similarly, images of other food categories, such as Jeon/Jjim/Jjigae were incorrectly predicted as other food images in the same category. According to the evaluation results of the model based on InceptionResNetV2, only nine out of 46 incorrect predictions of potato pancakes belonged to different categories except Jeon, whereas the remaining 37 incorrect predictions belongEd to the same category, Jeon (see <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Figure 5</a>). Furthermore, we found that the classification accuracy decreased for some foods when the augmented dataset was used. For example, the accuracy of Jjamppong, UeongJorim, Dakbokkeumtang, and Haemuljjim was less accurate after data augmentation (see <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a>).<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nNumber of Accurate Predictions From the Evaluation Result of the Model Based on InceptionResNetV2. The Number of Test Samples for Each Food Class is 200</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t4-3227796-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t4-3227796-small.gif\" alt=\"Table 4- &#10;Number of Accurate Predictions From the Evaluation Result of the Model Based on InceptionResNetV2. The Number of Test Samples for Each Food Class is 200\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nRecall of Specific Food Class From the Evaluation Result of InceptionResNetV2 Model Using Original and Augmented Dataset</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t5-3227796-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung.t5-3227796-small.gif\" alt=\"Table 5- &#10;Recall of Specific Food Class From the Evaluation Result of InceptionResNetV2 Model Using Original and Augmented Dataset\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div>\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung5-3227796-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung5-3227796-small.gif\" alt=\"FIGURE 5. - Classification result of Jeon type food images using model based on InceptionResNetV2.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 5. </b><fig><p>Classification result of Jeon type food images using model based on InceptionResNetV2.</p></fig></div><p class=\"links\"><a href=\"/document/9976036/all-figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Discussion</h2></div><p>Some interesting observations made during the training of our classification model, the questions that followed, and other studies that yielded similar results are discussed in this section.</p><div class=\"section_2\" id=\"sec5a\"><h3>A. Dataset Characteristics Affecting Classification Accuracy</h3><p>We discovered that the classification accuracy of the models was consistent for all the food classes regardless of the feature extraction networks used. One of the main reasons for the low classification accuracy in certain food image classes was the complex mixing of foods in an image and high similarities between some image classes. The classification model exhibited low performance when classifying the images of food that was a combination of different food classes or food with similar shape and color <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_5a\">[3]</a>.</p><p>Our findings regarding incorrect predictions within the same category reaffirm the fact that image recognition of Korean food is difficult because Korean food images are complex, and images of foods within the same food class may appear different <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_5a\">[37]</a>. Because images in the same food categories may have similar color or pattern characteristics, the integration of a hierarchical DL model could improve classification accuracy in the same category of foods <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_5a\">[38]</a>. However, foods in the unsalted vegetable category showed a lower classification accuracy than other food categories. Moreover, 32 out of the 200 test images of Dongtae Jjigae were incorrectly predicted as Maeuntang which belonged to a different food category (see <a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Figure 6</a>). Because the hierarchical classification model required high accuracy of the first-layer classification model <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_5a\">[38]</a>, a hierarchical approach was necessary to distinguish food classes sharing similar features.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung6abc-3227796-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9976036/jung6abc-3227796-small.gif\" alt=\"FIGURE 6. - Images of Kimchi Jjigae(left), Dongtae Jjigae(middle), and Maeuntang(right).\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">FIGURE 6. </b><fig><p>Images of Kimchi Jjigae(left), Dongtae Jjigae(middle), and Maeuntang(right).</p></fig></div><p class=\"links\"><a href=\"/document/9976036/all-figures\" class=\"all\">Show All</a></p></div></p><p>We compared the classification accuracy between the augmented data and the original data and found that the classification accuracy for some food images decreased after augmentation for all the models. Some augmentation processes, such as saturation modification and risk discarding important color information <a ref-type=\"bibr\" anchor=\"ref47\" id=\"context_ref_47_5a\">[47]</a> for food recognition. We assumed that color information was more important than shape or patterns into recognizing food classes, such as Jjamppong, UeongJorim, and Dakbokkeumtang. In addition, instead of using unified and randomized data augmentation, data augmentation with stochastic gradient descent has the potential to improve precision and robustness <a ref-type=\"bibr\" anchor=\"ref48\" id=\"context_ref_48_5a\">[48]</a>.</p></div><div class=\"section_2\" id=\"sec5b\"><h3>B. Backbone Characteristics Affecting Model Performance</h3><p>We found interesting elements that affect the classification performance in feature extraction networks and data augmentation. Among the feature extraction networks we used, InceptionResNetV2 with data augmented showed the highest TP rate. Our results align with prior studies which established that InceptionResNetV2 was superior to ResNet-50 and DenseNet-201 in extracting features from food images <a ref-type=\"bibr\" anchor=\"ref49\" id=\"context_ref_49_5b\">[49]</a>. The food classification accuracy of the models based on six particular feature extraction networks (e.g., ResNet50V2) decreased when using our data augmentation process. We assumed that augmenting food image data by modifying color information affected classification accuracy.</p></div><div class=\"section_2\" id=\"sec5c\"><h3>C. Limitations and Future Work</h3><p>We trained a model that classified Korean food images; however our study has several limitations. First, although we built a model that classifies Korean images, we did not resolve the poor classification of certain food classes. Resolving the poor performance of the classification model for some food classes is essential for increasing the average performance. Second, we attempted to improve the performance of the model by augmenting the image datasets, but we did not explain why the classification performance for some classes of foods decreased after data augmentation. Determining the cause of the classification performance degradation for some classes of food images after data augmentation is essential for improving model performance. Third, six backbone classes were used for model training. Nevertheless, we need to use the backbones of more than six classes for model training because an architecture such as AlexNet <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_5c\">[39]</a> and CaffeNet <a ref-type=\"bibr\" anchor=\"ref50\" id=\"context_ref_50_5c\">[50]</a> used for food image classification in previous studies may be more suitable for use as a feature extraction network for our model <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_5c\">[38]</a>, <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_5c\">[39]</a>. Furthermore, although our model classified 150 food class images, we need to train other food classes by collecting additional food image data. Collecting more food images and additional training enable the classification model to classify various food classes. Finally, although we built a Korean image classification model, we did not propose and evaluate a system that uses the classification model. The development of a system that uses the classification model enables users to use the model and evaluate their satisfaction with the performance of the classification model.</p><p>Future studies should focus on improving the classification model performance and evaluating the feasibility of the classification models. First, to improve the classification model performance, we may increase the classification accuracy of some food classes (e.g., Jjigae, Jeongol) for which the model showed poor performance. For example, we may consider using hierarchical ensemble structures <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_5c\">[38]</a> for model training for such classes with poor performance. Also, it would be worthwhile to convert the color space from RGB to HSV and use histogram normalization during the pre-processing process. Additionally, we may train the classification models using additional backbones (e.g., CoCa, efficient adaptive ensembling, InceptionResNetV3 <a ref-type=\"bibr\" anchor=\"ref51\" id=\"context_ref_51_5c\">[51]</a>) not used in this study. Second, to confirm the feasibility of the classification models, we would need to conduct usability studies with target users using our proposed classification models. For instance, during the usability study, we may calculate the inference time from the difference between the measured times by measuring the time when the models receive input data and the time when the models generate output results, respectively. In addition, we may determine the level of users\u2019 satisfaction with our proposed classification models.</p></div></div>\n<div class=\"section\" id=\"sec6\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION VI.</div><h2>Conclusion</h2></div><p>We aimed to train a Korean food image classification model. We believe that our model can be used in a system that automatically classifies the type of food that the user has consumed. Our work will benefit the community of users and researchers using trained models. It also benefits users of systems that automatically classify and record the types of food they take. Users can to classify images of similar Korean food into 150 classes using the model and record what they eat. The key contribution of this study to the research community is that we created a pre-processed dataset for training a Korean food image classification model. Also, we used several pre-trained deep neural networks to train models that classify Korean food images into 150 classes and evaluated the classification accuracy and time required for model training.</p></div>\n<h3>ACKNOWLEDGMENT</h3><p>The authors acknowledge the Urban Big Data and AI Institute of the University of Seoul supercomputing resources (<a target=\"_blank\" href=\"https://ubai.uos.ac.kr\">https://ubai.uos.ac.kr</a>) made available for conducting the research reported in this paper. The authors appreciate HCAIL members for their constructive feedback on our initial manuscript.</p></div></div>\n"
}