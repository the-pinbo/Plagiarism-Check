{
    "abstract": "Density peaks clustering (DPC) is a simple and efficient density-based clustering algorithm without complex iterative procedures. However, DPC needs to manually choose clustering centers via a decision graph, which often can\u2019t identify real centers and breaks the continuous flow of the algorithm. In addition, DPC is highly sensitive to the cut-off distance and suffers from the domino chain reactio...",
    "articleNumber": "9978604",
    "articleTitle": "Density Peaks Clustering Based on Potential Model and Diffusion Strength",
    "authors": [
        {
            "preferredName": "Jing Che",
            "normalizedName": "J. Che",
            "firstName": "Jing",
            "lastName": "Che",
            "searchablePreferredName": "Jing Che"
        },
        {
            "preferredName": "Wenke Zang",
            "normalizedName": "W. Zang",
            "firstName": "Wenke",
            "lastName": "Zang",
            "searchablePreferredName": "Wenke Zang"
        },
        {
            "preferredName": "Jingwen Xiong",
            "normalizedName": "J. Xiong",
            "firstName": "Jingwen",
            "lastName": "Xiong",
            "searchablePreferredName": "Jingwen Xiong"
        },
        {
            "preferredName": "Xiyu Liu",
            "normalizedName": "X. Liu",
            "firstName": "Xiyu",
            "lastName": "Liu",
            "searchablePreferredName": "Xiyu Liu"
        }
    ],
    "doi": "10.1109/ACCESS.2022.3227936",
    "publicationTitle": "IEEE Access",
    "publicationYear": "2022",
    "publicationVolume": null,
    "publicationIssue": null,
    "volume": "10",
    "issue": null,
    "documentLink": "/document/9978604/",
    "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<response><accessType>CCBY - IEEE is not the copyright holder of this material. Please follow the instructions via https://creativecommons.org/licenses/by/4.0/ to obtain full-text articles and stipulations in the API documentation.</accessType><div id=\"BodyWrapper\" class=\"ArticlePage\" xmlns:ieee=\"http://www.ieeexplore.ieee.org\"><div id=\"article\">\n<div class=\"section\" id=\"sec1\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION I.</div><h2>Introduction</h2></div><p>Clustering is an unsupervised method that divides a collection of data points into some non-empty groups based on the distance or similarity between data points. There are partition-based <a ref-type=\"bibr\" anchor=\"ref1\" id=\"context_ref_1_1\">[1]</a>, hierarchy-based <a ref-type=\"bibr\" anchor=\"ref2\" id=\"context_ref_2_1\">[2]</a>, grid-based <a ref-type=\"bibr\" anchor=\"ref3\" id=\"context_ref_3_1\">[3]</a>, model-based <a ref-type=\"bibr\" anchor=\"ref4\" id=\"context_ref_4_1\">[4]</a>, and density-based <a ref-type=\"bibr\" anchor=\"ref5\" id=\"context_ref_5_1\">[5]</a> Clustering algorithms. Clustering has many applications, such as image segmentation <a ref-type=\"bibr\" anchor=\"ref6\" id=\"context_ref_6_1\">[6]</a>, <a ref-type=\"bibr\" anchor=\"ref7\" id=\"context_ref_7_1\">[7]</a>, pattern recognition <a ref-type=\"bibr\" anchor=\"ref8\" id=\"context_ref_8_1\">[8]</a>, recommender system <a ref-type=\"bibr\" anchor=\"ref9\" id=\"context_ref_9_1\">[9]</a>, gene expression <a ref-type=\"bibr\" anchor=\"ref10\" id=\"context_ref_10_1\">[10]</a>, and intrusion detection <a ref-type=\"bibr\" anchor=\"ref11\" id=\"context_ref_11_1\">[11]</a>.</p><p>The density-based clustering method considers that the cluster center is surrounded by high-density points, and the points at the edge area of the cluster are low-density points. DBSCAN is representative of density-based clustering methods. It can find clusters with various shapes, which makes up for the shortcoming of K-means that can only find spherical clusters <a ref-type=\"bibr\" anchor=\"ref12\" id=\"context_ref_12_1\">[12]</a>. However, DBSCAN takes two parameters: the neighborhood radius <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Eps$\n</tex-math></inline-formula> and the minimum number <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$MinPts$\n</tex-math></inline-formula> of points, the values of which exert tremendous influence on the algorithm results.</p><p>The density peaks clustering (DPC) algorithm <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_1\">[13]</a> is a novel density-based clustering algorithm. DPC computes each data point\u2019s density and relative distance <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta $\n</tex-math></inline-formula> to construct a decision graph and selects the data points with high <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta $\n</tex-math></inline-formula> and relatively high density as the cluster centers. Each remaining point is assigned to the same cluster as its nearest neighbor with higher density. DPC has great advantages in dealing with non-spherical data distribution datasets. DPC algorithm does not require iteration, relies on few parameters, and operates efficiently. Nevertheless, DPC has the following limitations: <a ref-type=\"disp-formula\" anchor=\"deqn1\" href=\"#deqn1\" class=\"fulltext-link\">(1)</a> highly sensitive to the choice of cut-off distance parameter <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>, <a ref-type=\"disp-formula\" anchor=\"deqn2\" href=\"#deqn2\" class=\"fulltext-link\">(2)</a> using decision graph to select cluster centers manually <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a>, and <a ref-type=\"disp-formula\" anchor=\"deqn3\" href=\"#deqn3\" class=\"fulltext-link\">(3)</a> affected by the problem of chain reaction <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a>.</p><p>To alleviate these problems, numerous improved density peaks clustering algorithms have been proposed. In order to select the cut-off distance effectively, Jiang et al. <a ref-type=\"bibr\" anchor=\"ref16\" id=\"context_ref_16_1\">[16]</a> put forward a method to calculate the cut-off distance based on the Gini coefficient and k-nearest neighbors. Gao et al. <a ref-type=\"bibr\" anchor=\"ref17\" id=\"context_ref_17_1\">[17]</a> constructed an optimization function using the uncertainty of the target dataset to optimize the cut-off distance for various clustering tasks. There are also some researchers who design new density to avoid setting the cut-off distance. Lotfi et al. <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_1\">[18]</a> proposed a method called DPC-DBFN that uses fuzzy kernel and k-nearest neighbors to compute the local density for improving the separability of clusters. Sun et al. <a ref-type=\"bibr\" anchor=\"ref19\" id=\"context_ref_19_1\">[19]</a> developed a new local density that utilizes KNN-based neighborhood and mutual neighbor degree. To enhance the precision of selecting cluster centers, Guo et al. <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_1\">[20]</a> propounded DPC-CE that estimates the connectivity information between local centers with a graph-based strategy and re-evaluates the similarity between local centers by a distance punishment, which can ensure that the true cluster centers stand out in the decision graph. Li et al. <a ref-type=\"bibr\" anchor=\"ref21\" id=\"context_ref_21_1\">[21]</a> set two new thresholds to select candidate centers and proposed a new cluster fusion strategy to achieve the correct clustering of clusters with multiple density peaks. Flores et al. <a ref-type=\"bibr\" anchor=\"ref15\" id=\"context_ref_15_1\">[15]</a> came up with a method that can automatically select cluster centers by detecting gaps between data points in a one-dimensional decision graph. To eliminate the chain reaction, the FHC-LDP algorithm proposed by Guan et al. <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_1\">[22]</a> uses the idea of hierarchical clustering to establish a hierarchical structure of sub-clusters by considering the association between data points. Xie et al. <a ref-type=\"bibr\" anchor=\"ref14\" id=\"context_ref_14_1\">[14]</a> presented two sample assignment strategies based on K-nearest neighbors, one is to assign non-outliers using a breadth-first search. The second is to assign outliers and points not assigned in the first assignment process using fuzzy weighted K-nearest neighbors.</p><p>The algorithms mentioned above have modified DPC in different aspects, but there is still a lot of work to be done when dealing with datasets with complex shapes and uneven density distribution. In this paper, we propose a novel density peaks clustering algorithm based on the potential model and diffusion strength called DPC-PMDS. Firstly, the potential of data points is computed, and the centrality of data points is obtained based on the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$nneigh$\n</tex-math></inline-formula> information. Then a new density calculation method is proposed to better find the density peaks. Secondly, inspired by the information diffusion in social networks, the influence of data points and the diffusion strength between points are presented to select cluster centers accurately. And the initial clusters are generated by merging core points via label diffusion rule, which can well reflect the core distribution structure of the clusters and contributes to the correct assignment of the boundary points. Finally, the labels of the boundary points are obtained based on their distances from each cluster to avoid chain reaction. The main contributions of DPC-PMDS are the following three points: \n<ol><li><p>Using the potential model and centrality without cut-off distance to calculate density. The density calculated by the new method can better reflect the structure of the dataset and make the density peaks stand out compared with the original potential.</p></li><li><p>The label diffusion rule is used for label propagation of core points, which can enhance the effectiveness of selecting cluster centers and avoid a cluster with multiple peaks from being split into multiple clusters.</p></li><li><p>To avoid the chain reaction, a new distance-based assignment method that efficiently assigns boundary points is presented.</p></li></ol></p><p>The rest of this paper is as follows. <a ref-type=\"sec\" anchor=\"sec2\" class=\"fulltext-link\">Section 2</a> shows the DPC algorithm and the potential model. <a ref-type=\"sec\" anchor=\"sec3\" class=\"fulltext-link\">Section 3</a> presents the DPC-PMDS algorithm proposed in this paper. <a ref-type=\"sec\" anchor=\"sec4\" class=\"fulltext-link\">Section 4</a> shows the experiments and analysis. Finally, the conclusion in <a ref-type=\"sec\" anchor=\"sec5\" class=\"fulltext-link\">Section 5</a> summarizes our work.</p></div>\n<div class=\"section\" id=\"sec2\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION II.</div><h2>Related Works</h2></div><p>In this section, we briefly introduce DPC and the potential model and illustrate their weaknesses with examples.</p><div class=\"section_2\" id=\"sec2a\"><h3>A. Density Peaks Clustering Algorithm</h3><p>DPC first calculates the density and relative distance of points to find the density peaks <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_2a\">[13]</a>. For large-scale datasets, the local density <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho _{i} $\n</tex-math></inline-formula> of data point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is estimated by the cut-off kernel:<disp-formula id=\"deqn1\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\rho _{i} =\\sum \\limits _{j} \\chi \\left ({{d_{ij} -d_{c}} }\\right),\\tag{1}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\rho _{i} =\\sum \\limits _{j} \\chi \\left ({{d_{ij} -d_{c}} }\\right),\\tag{1}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} $\n</tex-math></inline-formula> is the predefined cut-off distance and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{ij} $\n</tex-math></inline-formula> is the Euclidean distance between point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. For <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x=d_{ij} -d_{c}$\n</tex-math></inline-formula>, if <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$x&lt; 0$\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\chi \\left ({x }\\right)=1$\n</tex-math></inline-formula>, otherwise <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\chi \\left ({x }\\right)=0$\n</tex-math></inline-formula>. For small datasets, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho _{i} $\n</tex-math></inline-formula> is computed by Gaussian kernel:<disp-formula id=\"deqn2\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\rho _{i} =\\sum \\limits _{j} {e^{\\left ({{-\\frac {d_{ij}}{d_{c}}} }\\right)^{2}}}.\\tag{2}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\rho _{i} =\\sum \\limits _{j} {e^{\\left ({{-\\frac {d_{ij}}{d_{c}}} }\\right)^{2}}}.\\tag{2}\\end{equation*}\n</span></span></disp-formula></p><p>The relative distance <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta _{i} $\n</tex-math></inline-formula> is calculated as follows:<disp-formula id=\"deqn3\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\delta _{i} =\\begin{cases} \\min \\limits _{\\exists j:\\rho _{j} &gt;\\rho _{i}} &amp;\\left ({{d_{ij}} }\\right) \\\\ \\max \\limits _{\\forall j:\\rho _{j} \\le \\rho _{i}} &amp;\\left ({{d_{ij}} }\\right) \\\\ \\end{cases}.\\tag{3}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\delta _{i} =\\begin{cases} \\min \\limits _{\\exists j:\\rho _{j} &gt;\\rho _{i}} &amp;\\left ({{d_{ij}} }\\right) \\\\ \\max \\limits _{\\forall j:\\rho _{j} \\le \\rho _{i}} &amp;\\left ({{d_{ij}} }\\right) \\\\ \\end{cases}.\\tag{3}\\end{align*}\n</span></span></disp-formula> After computing <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho _{i} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta _{i} $\n</tex-math></inline-formula>, DPC takes the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left ({{\\rho _{i},\\delta _{i}} }\\right)$\n</tex-math></inline-formula> values of all data points to build a decision graph, and manually finds the points with high <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta $\n</tex-math></inline-formula> and relatively high <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho $\n</tex-math></inline-formula> as cluster centers. A second method is to select the data points with larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma _{i} =\\rho _{i} \\delta _{i} $\n</tex-math></inline-formula> value as cluster centers. Finally, DPC assigns each remaining point to the same cluster as its nearest point with higher density.</p><p>Although DPC has great performance on a wide range of datasets, it still has several limitations. First of all, the accuracy of DPC algorithm is heavily dependent on the cut-off distance. Second, DPC manually selects the points with high density and high relative distance as cluster centers without considering the relationship between density peaks. On the dataset with uneven density distribution and complex shape, DPC may ignore the centers of low-density clusters and select redundant centers of high-density clusters. This subjective approach also breaks the continuity of the algorithm. Third, DPC is affected by the chain reaction problem, i.e., if a data point is incorrectly assigned, it may lead to the misallocation of its nearby data points, resulting in erroneous propagation of clustering labels. These limitations can be exemplified by the Jain dataset. Jain contains two clusters with significantly different density distributions. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1</a> shows the decision graph and clustering results obtained with the cut-off kernel for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} =0.23\\% $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} =0.24\\% $\n</tex-math></inline-formula>. The colored points in the decision graph correspond to the centers of the corresponding colored clusters in the result graph. From <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1(a)</a> and <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1(b)</a>, it can be seen that the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} $\n</tex-math></inline-formula> have a great influence on the results. <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1(b)</a> shows that the cluster centers selected by DPC via decision graph are all in the bottom cluster, because the density peaks in the bottom cluster have much higher density and relative distance than the upper cluster. Furthermore, in <a ref-type=\"fig\" anchor=\"fig1\" class=\"fulltext-link\">Fig.1(a)</a>, it is obvious that the assignment of data points is affected by the chain reaction problem.\n<div class=\"figure figure-full\" id=\"fig1\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang1-3227936-large.gif\" data-fig-id=\"fig1\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang1-3227936-small.gif\" alt=\"Fig. 1. - Decision graph and clustering results obtained with the cut-off kernel for &#10;$d_{c} =0.23$&#10;% and &#10;$d_{c} =0.24$&#10;%.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 1. </b><fig><p>Decision graph and clustering results obtained with the cut-off kernel for <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} =0.23$\n</tex-math></inline-formula>% and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} =0.24$\n</tex-math></inline-formula>%.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec2b\"><h3>B. The Potential Model</h3><p>Lu et al. <a ref-type=\"bibr\" anchor=\"ref23\" id=\"context_ref_23_2b\">[23]</a> put forward a clustering method called Clustering by Sorting Potential Values (CSPV) based on a potential model. The potential model considers the data points following Newton\u2019s law of universal gravitation and sets the mass of all points to 1. The gravitational force between point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> is obtained as:<disp-formula id=\"deqn4\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\vec {F}_{ij} \\left ({{\\vec {r}_{ij}} }\\right)=\\begin{cases} \\displaystyle G\\frac {\\hat {r}_{ij}}{r_{ij}^{2}}&amp; if~r_{ij} \\ge \\eta \\\\ \\displaystyle 0 &amp;if~r_{ij} &lt; \\eta\\end{cases},\\tag{4}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\vec {F}_{ij} \\left ({{\\vec {r}_{ij}} }\\right)=\\begin{cases} \\displaystyle G\\frac {\\hat {r}_{ij}}{r_{ij}^{2}}&amp; if~r_{ij} \\ge \\eta \\\\ \\displaystyle 0 &amp;if~r_{ij} &lt; \\eta\\end{cases},\\tag{4}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\vec {r}_{ij} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\hat {r}_{ij} $\n</tex-math></inline-formula> are the vector and unit vector from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>, respectively, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r_{ij} $\n</tex-math></inline-formula> is the Euclidean distance between <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. G is the gravitational constant. The parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\eta $\n</tex-math></inline-formula> is used to avoid the problem of singularity when <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$r_{ij} $\n</tex-math></inline-formula> is zero.</p><p>In the potential model, only the relative value of the potential is considered, so G is set to 1 for the convenience of calculation. The simplified potential at point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> is calculated as:<disp-formula id=\"deqn5\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\Phi _{ij} \\left ({{r_{ij}} }\\right)=\\int _{r_{ij}}^{\\infty} {\\vec {F}_{ij} \\left ({{\\vec {r}} }\\right)} \\cdot \\hat {r}dr=\\begin{cases} -\\frac {1}{r_{ij}} &amp;if~r_{ij} \\ge \\eta \\\\ -\\frac {1}{\\eta } &amp;if~r_{ij} &lt; \\eta \\end{cases}.\\tag{5}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\Phi _{ij} \\left ({{r_{ij}} }\\right)=\\int _{r_{ij}}^{\\infty} {\\vec {F}_{ij} \\left ({{\\vec {r}} }\\right)} \\cdot \\hat {r}dr=\\begin{cases} -\\frac {1}{r_{ij}} &amp;if~r_{ij} \\ge \\eta \\\\ -\\frac {1}{\\eta } &amp;if~r_{ij} &lt; \\eta \\end{cases}.\\tag{5}\\end{align*}\n</span></span></disp-formula></p><p>The potential of point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is:<disp-formula id=\"deqn6\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\Phi _{i} =\\sum \\limits _{j\\ne i} {\\Phi _{ij} \\left ({{r_{ij}} }\\right)}.\\tag{6}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\Phi _{i} =\\sum \\limits _{j\\ne i} {\\Phi _{ij} \\left ({{r_{ij}} }\\right)}.\\tag{6}\\end{equation*}\n</span></span></disp-formula> Lu et al. <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2b\">[24]</a> used the distance matrix of the dataset to select the parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\eta $\n</tex-math></inline-formula> to satisfy the condition of Scale-Invariance:<disp-formula id=\"deqn7-deqn8\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*}&amp;MinD_{i}=\\min \\limits _{r_{ij} \\ne 0,j=1,\\cdots n} \\left ({{r_{ij}} }\\right), \\tag{7}\\\\&amp;\\eta =mean\\left ({{MinD_{i}} }\\right)/S,\\tag{8}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*}&amp;MinD_{i}=\\min \\limits _{r_{ij} \\ne 0,j=1,\\cdots n} \\left ({{r_{ij}} }\\right), \\tag{7}\\\\&amp;\\eta =mean\\left ({{MinD_{i}} }\\right)/S,\\tag{8}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$MinD_{i} $\n</tex-math></inline-formula> is the minimum distance from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to all the other points, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n$\n</tex-math></inline-formula> is the number of points. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula> is a scale factor, generally set to 10.</p><p>The Parzen window function, a nonparametric estimation method, is used to demonstrate that the total potential value is negatively proportional to the estimated probability density <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_2b\">[24]</a>. Thereby, the smaller the potential of a data point, the higher its density.</p><p><a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a> shows the contour map of potential of the Jain dataset. The darker the color in <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a>, the lower the potential, i.e., the higher the density. The potential is calculated from a global perspective, so the potential does not display the data distribution of the dataset with uneven density well. As can be seen from <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a>, the potential of high-density clusters is extremely high, while the potential of low-density clusters is extremely low. The boundaries of clusters are not clear, and density peaks of low-density clusters also do not stand out well in the figure.\n<div class=\"figure figure-full\" id=\"fig2\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang2-3227936-large.gif\" data-fig-id=\"fig2\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang2-3227936-small.gif\" alt=\"Fig. 2. - Potential contour map of the Jain dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 2. </b><fig><p>Potential contour map of the Jain dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p></div></div>\n<div class=\"section\" id=\"sec3\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION III.</div><h2>The Proposed Method</h2></div><p>In this section, the detailed procedure of the DPC-PMDS algorithm is presented.</p><div class=\"section_2\" id=\"sec3a\"><h3>A. Density Calculation</h3><p>In this subsection, the density is calculated by considering potential and centrality. The values of potential are negative, and the smaller the potential, the higher the corresponding density. Therefore, we first calculate the density by changing the sign of the potential and normalizing it:<disp-formula id=\"deqn9\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\rho _{i} =\\frac {-\\Phi _{i}}{\\max \\limits _{j=1,\\cdots n} \\left ({{\\Phi _{j}} }\\right)}.\\tag{9}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\rho _{i} =\\frac {-\\Phi _{i}}{\\max \\limits _{j=1,\\cdots n} \\left ({{\\Phi _{j}} }\\right)}.\\tag{9}\\end{equation*}\n</span></span></disp-formula> Because low-density clusters can\u2019t be identified well with the density calculated by the original potential, we improve it by considering the centrality of data points. For any point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> except the point with the highest density, we use <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$nneigh_{j} $\n</tex-math></inline-formula> to denote the nearest neighbor with higher density of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>:<disp-formula id=\"deqn10\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} nneigh_{j} =\\left \\{{{i\\vert \\min \\limits _{\\exists i:\\rho _{i} &gt;\\rho _{j}} \\left ({{d_{ji}} }\\right)} }\\right \\}.\\tag{10}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} nneigh_{j} =\\left \\{{{i\\vert \\min \\limits _{\\exists i:\\rho _{i} &gt;\\rho _{j}} \\left ({{d_{ji}} }\\right)} }\\right \\}.\\tag{10}\\end{equation*}\n</span></span></disp-formula></p><p>The centrality of point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is defined as follows:<disp-formula id=\"deqn11\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} c_{i} =\\left |{ {A_{i}} }\\right |+1,\\;\\;\\;\\;A_{i} =\\left \\{{{j\\vert nneigh_{j} =i} }\\right \\},\\tag{11}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} c_{i} =\\left |{ {A_{i}} }\\right |+1,\\;\\;\\;\\;A_{i} =\\left \\{{{j\\vert nneigh_{j} =i} }\\right \\},\\tag{11}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\left |{ {\\;\\cdot \\;} }\\right |$\n</tex-math></inline-formula> is the cardinality of the set. The centrality <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula> is greater than or equal to 1. A large value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula> means that <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is the nearest neighbor with higher density of many points around it, i.e., point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> has a relatively high density in its neighborhood. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} =1$\n</tex-math></inline-formula> means that no point has the nearest neighbor with higher density equal to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, which indicates that the density of point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is relatively small in its neighborhood. Therefore, the larger the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula>, the more likely that point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is a density peak. We take <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula> as the weight and multiply it with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho _{i} $\n</tex-math></inline-formula>, then the calculation formula of new density is:<disp-formula id=\"deqn12\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\rho _{i}^{\\prime } =c_{i} \\times \\rho _{i}.\\tag{12}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\rho _{i}^{\\prime } =c_{i} \\times \\rho _{i}.\\tag{12}\\end{equation*}\n</span></span></disp-formula></p><p><a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig.3</a> is the contour map of the new density of the Jain dataset. The darker the color in <a ref-type=\"fig\" anchor=\"fig3\" class=\"fulltext-link\">Fig.3</a>, the higher the density. Compared with <a ref-type=\"fig\" anchor=\"fig2\" class=\"fulltext-link\">Fig.2</a>, it can be found that the new density can better reflect the data distribution and facilitate the selection of density peaks.\n<div class=\"figure figure-full\" id=\"fig3\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang3-3227936-large.gif\" data-fig-id=\"fig3\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang3-3227936-small.gif\" alt=\"Fig. 3. - Density contour map of the Jain dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 3. </b><fig><p>Density contour map of the Jain dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3b\"><h3>B. Diffusion Strength</h3><p>In this subsection, the influence of data points and the diffusion strength of two data points will be introduced.</p><p>In social networks, information diffusion is carried out via interactions between nodes (links in the network), i.e., information of nodes is diffused through paths composed of edges. Each information diffusion network can be considered as a tree-like structure. The root node, the publisher of information, diffuses information to the leaf nodes. Generally, the influence of nodes and the relationship between nodes affect information diffusion <a ref-type=\"bibr\" anchor=\"ref25\" id=\"context_ref_25_3b\">[25]</a>. Inspired by the information diffusion in social networks, we regard the assignment of clustering labels as a label diffusion process. The core points of each cluster are linked via label diffusion rule to avoid selecting excessive centers of high-density clusters and neglecting centers of low-density clusters. Before introducing the label diffusion rules, the definitions of influence and diffusion strength are given.</p><p>The influence of a point is related to its distance from its surrounding points. The definition of the influence of point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is:<disp-formula id=\"deqn13\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} I_{i} =\\bar {X}_{i} +2S_{i},\\tag{13}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} I_{i} =\\bar {X}_{i} +2S_{i},\\tag{13}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{i} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$S_{i} $\n</tex-math></inline-formula> are the mean and standard deviation of the distance from the point to its surrounding points, respectively. They are calculated as:<disp-formula id=\"deqn14-deqn15\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} \\bar {X}_{i}=&amp;\\frac {1}{s}\\sum \\limits _{j\\in sNN_{i}} {d_{ij}}, \\tag{14}\\\\ S_{i}=&amp;\\sqrt {\\frac {1}{s-1}\\sum \\limits _{j\\in sNN_{i}} {\\left ({{d_{ij} -\\bar {X}_{i}} }\\right)}^{2}},\\tag{15}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} \\bar {X}_{i}=&amp;\\frac {1}{s}\\sum \\limits _{j\\in sNN_{i}} {d_{ij}}, \\tag{14}\\\\ S_{i}=&amp;\\sqrt {\\frac {1}{s-1}\\sum \\limits _{j\\in sNN_{i}} {\\left ({{d_{ij} -\\bar {X}_{i}} }\\right)}^{2}},\\tag{15}\\end{align*}\n</span></span></disp-formula> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$sNN(x_{i})$\n</tex-math></inline-formula> is a set of nearest neighbors of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>:<disp-formula id=\"deqn16\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\textrm {s}NN_{i} =\\left \\{{{j\\vert d_{ij} \\le d_{is}} }\\right \\},\\tag{16}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\textrm {s}NN_{i} =\\left \\{{{j\\vert d_{ij} \\le d_{is}} }\\right \\},\\tag{16}\\end{equation*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{is} $\n</tex-math></inline-formula> is the Euclidean distance between data points <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$sth$\n</tex-math></inline-formula> nearest neighbor of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s$\n</tex-math></inline-formula> is equal to the maximum value of the centrality <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c$\n</tex-math></inline-formula> of all data points plus a parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula>, i.e., <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s=\\max (c)+\\alpha $\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> is an integer greater than <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$-\\max (c)$\n</tex-math></inline-formula>. The larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula>, the larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{i} $\n</tex-math></inline-formula>. Thus the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> is positively correlated with the influence of the data points. The greater the influence of data points, the greater their ability to diffuse labels to surrounding points.</p><p>Next, the computation formula for measuring diffusion strength from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> will be described. The influence of a data point represents its label diffusion ability. The larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$I_{i} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$I_{j} $\n</tex-math></inline-formula>, the larger diffusion strength from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{i} $\n</tex-math></inline-formula> represents the mean distance between data point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and its neighbors. If <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{i} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{j} $\n</tex-math></inline-formula> are very different, it means that the distribution characteristics of points around them are very different and point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> are very likely not close to each other. Hence, the diffusion strength between points <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> should be small. In addition, the diffusion strength from points in the boundary region to other points should be small to avoid propagating cluster labels to other clusters. Let <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$ci$\n</tex-math></inline-formula> denote the cluster center of the cluster where point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is located. If the difference between <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{i} $\n</tex-math></inline-formula> and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\bar {X}_{ci} $\n</tex-math></inline-formula> is large, point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is likely to be a boundary point far from the cluster center, so the diffusion strength of point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to other points should be small. Thereby, the diffusion strength from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> is:<disp-formula id=\"deqn17\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} ds_{ij} =\\frac {\\min \\left ({{\\bar {X}_{i},\\bar {X}_{j}} }\\right)}{\\max \\left ({{\\bar {X}_{i},\\bar {X}_{j}} }\\right)}\\cdot \\frac {\\min \\left ({{\\bar {X}_{i},\\bar {X}_{ci}} }\\right)}{\\max \\left ({{\\bar {X}_{i},\\bar {X}_{ci}} }\\right)}\\cdot \\left ({{\\frac {I_{i} +I_{j}}{2}} }\\right),\\tag{17}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} ds_{ij} =\\frac {\\min \\left ({{\\bar {X}_{i},\\bar {X}_{j}} }\\right)}{\\max \\left ({{\\bar {X}_{i},\\bar {X}_{j}} }\\right)}\\cdot \\frac {\\min \\left ({{\\bar {X}_{i},\\bar {X}_{ci}} }\\right)}{\\max \\left ({{\\bar {X}_{i},\\bar {X}_{ci}} }\\right)}\\cdot \\left ({{\\frac {I_{i} +I_{j}}{2}} }\\right),\\tag{17}\\end{equation*}\n</span></span></disp-formula> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$ds_{ij} $\n</tex-math></inline-formula> indicates the ability of point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to diffuse labels to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. The larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$ds_{ij} $\n</tex-math></inline-formula> is, the more likely point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is to diffuse its own label to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>.</p></div><div class=\"section_2\" id=\"sec3c\"><h3>C. Center Identification and Core Point LABLE Diffusion</h3><p>In this subsection, the label diffusion rule is defined, the process of automatically determining cluster centers and assigning core points is described.</p><p>The decision value <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> is the probability of each data point becoming a cluster center and the decision value of data point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is calculated by:<disp-formula id=\"deqn18\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} \\gamma _{i} =\\frac {\\rho _{i}^{\\prime }}{\\max (\\rho ^{\\prime })}\\times \\frac {\\delta _{i} }{\\max (\\delta)}.\\tag{18}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} \\gamma _{i} =\\frac {\\rho _{i}^{\\prime }}{\\max (\\rho ^{\\prime })}\\times \\frac {\\delta _{i} }{\\max (\\delta)}.\\tag{18}\\end{equation*}\n</span></span></disp-formula></p><p>The data points are sorted in descending order according to the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value.</p><p>We assume that point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> already has a cluster label. Based on the diffusion strength, the label diffusion rule is defined as follows:<disp-formula id=\"deqn19\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{align*} CR_{ij} =\\begin{cases} 1 &amp;if~d_{ij} \\le ds_{ij} \\wedge j\\in sNN_{i} \\\\ 0 &amp;if~d_{ij} &gt;ds_{ij} \\vee j\\notin sNN_{i} \\end{cases},\\tag{19}\\end{align*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{align*} CR_{ij} =\\begin{cases} 1 &amp;if~d_{ij} \\le ds_{ij} \\wedge j\\in sNN_{i} \\\\ 0 &amp;if~d_{ij} &gt;ds_{ij} \\vee j\\notin sNN_{i} \\end{cases},\\tag{19}\\end{align*}\n</span></span></disp-formula> where <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$ds_{ij} $\n</tex-math></inline-formula> is the diffusion strength from point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> to point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{ij} $\n</tex-math></inline-formula> is the Euclidean distance between point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> and point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>. <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$CR_{ij} =1$\n</tex-math></inline-formula> means that point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> can propagate its label to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>, point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> is within the diffusion range of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>. Otherwise, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$CR_{ij} =0$\n</tex-math></inline-formula> means that <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> cannot propagate its label to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula>, i.e., <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$j$\n</tex-math></inline-formula> is not within the diffusion range of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>.</p><p>Then the diffusion range of data point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> can be obtained through the label diffusion rule:<disp-formula id=\"deqn20\" class=\"display-formula\"><tex-math notation=\"LaTeX\">\\begin{equation*} DR_{i} =\\left \\{{{j\\vert CR_{ij} =1} }\\right \\}.\\tag{20}\\end{equation*}\n</tex-math><span class=\"formula\"><span class=\"link\">View Source</span><img aria-describedby=\"qtip-0\" style=\"display:inline;\" title=\"Right-click on figure or equation for MathML and additional features.\" data-hasqtip=\"0\" class=\"qtooltip moreInfo\" alt=\"Right-click on figure for MathML and additional features.\" src=\"/assets/img/icon.support.gif\" border=\"0\" height=\"20\" width=\"24\"/><span class=\"tex tex2jax_ignore\" style=\"display:none;\">\\begin{equation*} DR_{i} =\\left \\{{{j\\vert CR_{ij} =1} }\\right \\}.\\tag{20}\\end{equation*}\n</span></span></disp-formula></p><p>A point can propagate its label to points that are within its diffusion range.</p><p>As mentioned above, if there are two or more data points with high <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta $\n</tex-math></inline-formula> and high <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho $\n</tex-math></inline-formula> in the same cluster, selecting the center by decision graph or <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value may split a cluster into multiple clusters. In addition, the cluster centers with lower density in the dataset are easily ignored. To address this problem, we propose a strategy to select centers by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> values and the label diffusion rule, which can select the centers of low-density clusters and connect core points of each cluster.</p><p>Firstly, we select the point with the largest <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value as the first cluster center, and then add the points within the diffusion range of the cluster center to the cluster. Next, we iterate through the newly added points and assigned the points within their diffusion range to the cluster. This traversal process continues until the diffusion ranges of all points assigned to the cluster are traversed and no new points can be assigned to the cluster. Then we select the unassigned point with the largest <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value as the new cluster center, and loop the above steps of assigning points according to the label diffusion rule until the number of cluster centers reaches the number of clusters we want. In this process, the points assigned by the label diffusion rule are called core points and the remaining points are boundary points. Finally, the cluster center and core points of each cluster are obtained, i.e., the initial clusters are generated. The specific steps are shown in <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Algorithm 1</a>.<div class=\"algorithm\" id=\"alg1\"><h3>Algorithm 1 Center Identification and Core Point Label Diffusion</h3><p><b>Input:</b> Dataset <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula>, the number of clusters <i>npeak</i>.</p><p><b>Output:</b> Cluster centers and the clustering results of core points <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$C_{core} $\n</tex-math></inline-formula>.</p><p>Let <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$NCLUST=0$\n</tex-math></inline-formula>.</p><p><b>while</b> <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$NCLUST\\ne npeak$\n</tex-math></inline-formula> and there are unassigned points in the dataset</p><p>Select the unassigned point with the largest <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value as the cluster center to create a new cluster.</p><p><inline-formula id=\"\"><tex-math notation=\"LaTeX\">$NCLUST=NCLUST+1$\n</tex-math></inline-formula>.</p><p>Create a queue Q and put the clustering center</p><p>into the queue Q.</p><p><b>while</b> Q is not empty</p><p>Take the head node q1 of Q.</p><p>Find all the unassigned points within the</p><p>diffusion range of q1, assign these points to</p><p>the cluster where q1 is located and put them</p><p>in the queue Q.</p><p>Delete q1 in Q.</p><p><b>end</b></p><p>Delete Q</p><p><b>end</b></p><p>Output the cluster centers and the clustering results of core points.</p></div><div class=\"algorithm\" id=\"alg2\"><h3>Algorithm 2 DPC-PMDS</h3><p><b>Input:</b> Dataset <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X=\\left \\{{{x_{1},x_{2},\\cdots x_{n}} }\\right \\}$\n</tex-math></inline-formula>, cluster number <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$npeak$\n</tex-math></inline-formula>.</p><p><b>Output:</b> Cluster centers and cluster label vector of data points.</p><p>Normalize the dataset <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$X$\n</tex-math></inline-formula></p><p>Calculate the distance matrix using Euclidean distance</p><p>Calculate the density <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho _{i} $\n</tex-math></inline-formula> from <a ref-type=\"disp-formula\" anchor=\"deqn9\" href=\"#deqn9\" class=\"fulltext-link\">Eq. (9)</a></p><p>Calculate <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula> from <a ref-type=\"disp-formula\" anchor=\"deqn11\" href=\"#deqn11\" class=\"fulltext-link\">Eq. (11)</a></p><p>Calculate new density <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\rho _{i}^{\\prime } $\n</tex-math></inline-formula> from <a ref-type=\"disp-formula\" anchor=\"deqn12\" href=\"#deqn12\" class=\"fulltext-link\">Eq. (12)</a></p><p>Calculate the relative density <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\delta _{i} $\n</tex-math></inline-formula> from <a ref-type=\"disp-formula\" anchor=\"deqn3\" href=\"#deqn3\" class=\"fulltext-link\">Eq. (3)</a></p><p>Calculate diffusion strength <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$ds_{ij} $\n</tex-math></inline-formula>from <a ref-type=\"disp-formula\" anchor=\"deqn17\" href=\"#deqn17\" class=\"fulltext-link\">Eq. (17)</a></p><p>Calculate <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma _{i} $\n</tex-math></inline-formula> from <a ref-type=\"disp-formula\" anchor=\"deqn18\" href=\"#deqn18\" class=\"fulltext-link\">Eq. (18)</a> and sort the data points in descending order by <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value.</p><p>Select cluster centers and assign core points according to Algorithm 1.</p><p>Handle boundary points. Each boundary point is assigned to the cluster with the minimum distance.</p><p>Output the cluster centers and cluster label vector.</p></div></p><p>We take the Flame and Jain datasets as an example, the results of selecting the center and assigning core points of Flame and Jain according to <a ref-type=\"algorithm\" anchor=\"alg1\" class=\"fulltext-link\">Algorithm 1</a> are shown in Fig.4. Points colored black in the graph are boundary points that are not assigned through the label diffusion rule, and the points marked with black triangles are the cluster centers. The directed line segment shows the process of label diffusion. In each cluster, the labels start from the cluster center and spread to the surrounding points. As can be found in <a ref-type=\"fig\" anchor=\"fig4\" class=\"fulltext-link\">Fig.4</a>, the algorithm correctly selects the cluster centers and connects the core points of each cluster. On the Flame dataset, the two clusters have intersection and the points at the junction of the two clusters are the boundary points. On the Jain dataset, the two clusters have no intersection and most of the points of both clusters are considered as core points.\n<div class=\"figure figure-full\" id=\"fig4\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang4-3227936-large.gif\" data-fig-id=\"fig4\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang4-3227936-small.gif\" alt=\"Fig. 4. - Results of selecting cluster centers and assigning core point of dataset Flame and Jain.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 4. </b><fig><p>Results of selecting cluster centers and assigning core point of dataset Flame and Jain.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3d\"><h3>D. Boundary Points Assignment</h3><p>The core points of each cluster are connected by the label diffusion rule, there are still some boundary points that are not assigned because they do not conform to the label diffusion rule. In this subsection, a distance-based assignment method is used to obtain the labels of the boundary points to avoid the chain reaction.</p><p>For any boundary point <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula>, we calculate its distance to each cluster through the sum of its distance to the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula> nearest points of each cluster separately. Then <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$i$\n</tex-math></inline-formula> is assigned to the cluster with the minimum distance. <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig.5</a> shows the final clustering results of the Flame dataset and the Jain dataset. From <a ref-type=\"fig\" anchor=\"fig5\" class=\"fulltext-link\">Fig.5</a>, it can be seen that the algorithm proposed in this paper obtains the correct clustering results on the Flame and Jain datasets.\n<div class=\"figure figure-full\" id=\"fig5\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang5-3227936-large.gif\" data-fig-id=\"fig5\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang5-3227936-small.gif\" alt=\"Fig. 5. - Clustering results of Flame and Jain datasets.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 5. </b><fig><p>Clustering results of Flame and Jain datasets.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec3e\"><h3>E. The Time Complexity</h3><p>This section gives the computational complexity of the DPC-PMDS algorithm. The time complexity of DPC-PMDS depends on five main steps: 1) calculating the distance between data points and the potential of each data point, with a time complexity of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula>, 2) calculating the centrality and density of data points with a time complexity of no more than <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula>, 3) searching for the nearest <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$s$\n</tex-math></inline-formula> points to calculate the diffusion strength of data points requires <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula>, 4) obtaining the cluster centers and connecting the core points within the same cluster according to the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\gamma $\n</tex-math></inline-formula> value and label diffusion rule, the time complexity of this process will not exceed <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula>, 5) assigning the boundary points according to the distance to the nearest <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$c_{i} $\n</tex-math></inline-formula> points of each cluster, the time complexity is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula>. Therefore, the time complexity of the DPC-PMDS algorithm proposed in this paper is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula> as that of DPC algorithm.</p></div></div>\n<div class=\"section\" id=\"sec4\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION IV.</div><h2>Experiments</h2></div><p>In this section, for the sake of evaluating the performance of DPC-PMDS, we compare DPC-PMDS with DPC <a ref-type=\"bibr\" anchor=\"ref13\" id=\"context_ref_13_4\">[13]</a>, PHA <a ref-type=\"bibr\" anchor=\"ref24\" id=\"context_ref_24_4\">[24]</a>,<a ref-type=\"fn\" anchor=\"fn1\" class=\"footnote-link\">1</a> and state-of-the-art clustering methods including DPC-DBFN <a ref-type=\"bibr\" anchor=\"ref18\" id=\"context_ref_18_4\">[18]</a>,<a ref-type=\"fn\" anchor=\"fn2\" class=\"footnote-link\">2</a> DPC-CE <a ref-type=\"bibr\" anchor=\"ref20\" id=\"context_ref_20_4\">[20]</a><a ref-type=\"fn\" anchor=\"fn3\" class=\"footnote-link\">3</a> and FHC-LDP <a ref-type=\"bibr\" anchor=\"ref22\" id=\"context_ref_22_4\">[22]</a><a ref-type=\"fn\" anchor=\"fn4\" class=\"footnote-link\">4</a> on a variety of datasets. The time complexity of the PHA, DPC-DBFN and DPC-CE algorithms is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n^{2})$\n</tex-math></inline-formula>, and the time complexity of the FHC-LDP is <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$O(n\\log (n))$\n</tex-math></inline-formula>. The Accuracy(ACC) <a ref-type=\"bibr\" anchor=\"ref26\" id=\"context_ref_26_4\">[26]</a>, Normalized Mutual Information (NMI) <a ref-type=\"bibr\" anchor=\"ref27\" id=\"context_ref_27_4\">[27]</a>, Rand Index(RI) <a ref-type=\"bibr\" anchor=\"ref28\" id=\"context_ref_28_4\">[28]</a> and Adjusted Rand Index (ARI) <a ref-type=\"bibr\" anchor=\"ref29\" id=\"context_ref_29_4\">[29]</a> are used to evaluate the performance of clustering algorithms. We implement the proposed PDC-PMDS and other five comparison algorithms in a desktop computer with Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20 GHz 2.20 GHz, Windows 10 Professional Edition 64-bit OS. All the clustering methods\u2019 codes are written, run, and tested by MATLAB R2017b.</p><div class=\"section_2\" id=\"sec4a\"><h3>A. Datasets</h3><p>The synthetic datasets including Aggregation <a ref-type=\"bibr\" anchor=\"ref30\" id=\"context_ref_30_4a\">[30]</a>, Flame <a ref-type=\"bibr\" anchor=\"ref31\" id=\"context_ref_31_4a\">[31]</a>, Jain <a ref-type=\"bibr\" anchor=\"ref32\" id=\"context_ref_32_4a\">[32]</a>, Spiral <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_4a\">[33]</a>, Pathbased <a ref-type=\"bibr\" anchor=\"ref33\" id=\"context_ref_33_4a\">[33]</a>, Compound <a ref-type=\"bibr\" anchor=\"ref34\" id=\"context_ref_34_4a\">[34]</a>, R15 <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_4a\">[35]</a>, D31 <a ref-type=\"bibr\" anchor=\"ref35\" id=\"context_ref_35_4a\">[35]</a>, threecircles <a ref-type=\"bibr\" anchor=\"ref36\" id=\"context_ref_36_4a\">[36]</a>, CMC <a ref-type=\"bibr\" anchor=\"ref37\" id=\"context_ref_37_4a\">[37]</a>, S1 <a ref-type=\"bibr\" anchor=\"ref38\" id=\"context_ref_38_4a\">[38]</a>, and Unbalance <a ref-type=\"bibr\" anchor=\"ref39\" id=\"context_ref_39_4a\">[39]</a> are used in this paper. These twelve synthetic datasets can evaluate the ability of DPC-PMDS to identify clusters of datasets with diverse shapes and uneven density distributions. Additionally, eight UCI datasets (available at <a target=\"_blank\" href=\"https://archive.ics.uci.edu/ml/datasets.php\">https://archive.ics.uci.edu/ml/datasets.php</a>) and the Olivetti Faces <a ref-type=\"bibr\" anchor=\"ref40\" id=\"context_ref_40_4a\">[40]</a> dataset are used to further evaluate the performance of DPC-PMDS on real-world datasets with different data volumes and dimensions. The UCI datasets contain Iris, Seeds, DNA, Diabetes, Thyroid, Abalone, Cloud, and Robot navigation. <a ref-type=\"table\" anchor=\"table1\" class=\"fulltext-link\">Tables 1</a> and <a ref-type=\"table\" anchor=\"table2\" class=\"fulltext-link\">2</a> show the number of instances, the number of attributes, and the number of clusters for each dataset.<div class=\"figure figure-full table\" id=\"table1\"><div class=\"figcaption\"><b class=\"title\">TABLE 1 </b>\nSynthetic datasets used in this paper.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t1-3227936-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t1-3227936-small.gif\" alt=\"Table 1- &#10;Synthetic datasets used in this paper.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div><div class=\"figure figure-full table\" id=\"table2\"><div class=\"figcaption\"><b class=\"title\">TABLE 2 </b>\nUCI datasets used in this paper.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t2-3227936-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t2-3227936-small.gif\" alt=\"Table 2- &#10;UCI datasets used in this paper.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4b\"><h3>B. Parameters</h3><p>In order to obtain a fair comparison, the parameters are set according to the description of the parameters in the original paper of the comparison algorithms. For DPC algorithm, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} $\n</tex-math></inline-formula> is usually chosen so that the average number of neighbors is around 1% to 2% of the total number of points in the dataset. We expand this scope to 0.5% to 3% and run the algorithm multiple times in steps of 0.5 to take the optimal value. The PHA algorithm has only one parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$S$\n</tex-math></inline-formula>, which defaults to 10. For DPC-DBFN algorithm with parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula>, the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> is selected from 1 to 40 to get the optimal value. DPC-CE contains three parameters, which are set to <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$dc = 2\\% $\n</tex-math></inline-formula>, <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Tr = 0.25$\n</tex-math></inline-formula>, and <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$Pr = 0.3$\n</tex-math></inline-formula> in the original paper. The parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$k$\n</tex-math></inline-formula> of FHC-LDP is also set according to the original paper. When the number of data points <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n&lt; 500$\n</tex-math></inline-formula>, we set <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$5\\le k\\le 20$\n</tex-math></inline-formula>, when <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$500\\le n&lt; 10000$\n</tex-math></inline-formula>, we set <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$1\\% n\\le k\\le 3\\% n$\n</tex-math></inline-formula>, when <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$n\\ge 10000$\n</tex-math></inline-formula>, we set <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$20\\le k\\le 2\\% n$\n</tex-math></inline-formula>. The specific parameter settings of each algorithm are shown in <a ref-type=\"table\" anchor=\"table3\" class=\"fulltext-link\">Table 3</a>.<div class=\"figure figure-full table\" id=\"table3\"><div class=\"figcaption\"><b class=\"title\">TABLE 3 </b>\nParameters setting of algorithms used in this paper.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t3-3227936-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t3-3227936-small.gif\" alt=\"Table 3- &#10;Parameters setting of algorithms used in this paper.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p></div><div class=\"section_2\" id=\"sec4c\"><h3>C. Results on Synthetic Datasets</h3><p>In this subsection, experiments are conducted on 12 synthetic datasets. The visual clustering results of DPC, PHA, DPC-DBFN, DPC-CE, FHC-LDP and the method proposed in this paper (DPC-PMDS) are shown in Fig.6-17. The ACC, NMI, RI, and ARI metrics of all algorithms are given in <a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a>. The optimal values of the evaluation metrics on each dataset are bolded.<div class=\"figure figure-full table\" id=\"table4\"><div class=\"figcaption\"><b class=\"title\">TABLE 4 </b>\nClustering results of algorithms on twelve synthetic datasets.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t4-3227936-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t4-3227936-small.gif\" alt=\"Table 4- &#10;Clustering results of algorithms on twelve synthetic datasets.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p><a ref-type=\"fig\" anchor=\"fig6\" class=\"fulltext-link\">Fig.6</a> and <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig.7</a> show the clustering results on the Aggregation and Flame datasets. The points marked with black triangles are the cluster centers. On the Aggregation dataset, the clustering results of PHA, FHC-LDP, and DPC-PMDS are completely correct with the value of each evaluation metric is 1. DPC, DPC-DBFN, and DPC-CE have errors for the assignment of a few points. Flame is a dataset with overlapping area between clusters. From <a ref-type=\"fig\" anchor=\"fig7\" class=\"fulltext-link\">Fig.7</a>, it can be seen that DPC and PHA obviously cannot separate the two clusters, DPC-DBFN has inaccurate assignment for the points in the junction part of two clusters. The clustering results of DPC-CE, FHC-LDP, and DPC-PMDS are completely correct.\n<div class=\"figure figure-full\" id=\"fig6\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang6abcdef-3227936-large.gif\" data-fig-id=\"fig6\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang6abcdef-3227936-small.gif\" alt=\"Fig. 6. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Aggregation dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 6. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Aggregation dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig7\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang7abcdef-3227936-large.gif\" data-fig-id=\"fig7\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang7abcdef-3227936-small.gif\" alt=\"Fig. 7. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Flame dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 7. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Flame dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig8\" class=\"fulltext-link\">Fig.8</a> and <a ref-type=\"fig\" anchor=\"fig9\" class=\"fulltext-link\">Fig.9</a> show the clustering results on the Jain and Spiral datasets. These two datasets have clusters with irregular shape and data points with uneven density distribution. And there is no intersection between clusters. The clustering results of DPC-CE, FHC-LDP, and DPC-PMDS on these two datasets are correct because DPC-CE is based on the local central connectivity information estimation strategy of the graph, FHC-LDP considers the association between adjacent points, and the DPC-PMDS algorithm proposed in this paper considers the diffusion strength when finding the clustering centers. On the Jain dataset, DPC and DPC-DBFN cannot identify the correct cluster centers, and PHA also cannot separate the two clusters. On the Spiral dataset, although DPC and DPC-DBFN can identify the correct cluster centers, there is a problem with the assignment strategy. The potential-based hierarchical clustering method PHA also obtains the correct clustering results.\n<div class=\"figure figure-full\" id=\"fig8\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang8abcdef-3227936-large.gif\" data-fig-id=\"fig8\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang8abcdef-3227936-small.gif\" alt=\"Fig. 8. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Jain dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 8. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Jain dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig9\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang9abcdef-3227936-large.gif\" data-fig-id=\"fig9\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang9abcdef-3227936-small.gif\" alt=\"Fig. 9. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Spiral dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 9. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Spiral dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig10\" class=\"fulltext-link\">Fig.10</a> and <a ref-type=\"fig\" anchor=\"fig11\" class=\"fulltext-link\">Fig.11</a> show the clustering results on the Pathbased and Compound datasets. These two datasets have clusters with large density differences and intersections. The clustering result of DPC-PMDS on these two datasets is significantly better than other algorithms. On Pathbased dataset, the evaluation metrics of DPC-PMDS is 1, while that of all other algorithms is far less than 1. On the Compound dataset, the DPC-PMDS algorithm identifies cluster centers and correctly assigns most of the points. DPC, PHA, DPC-DBFN and FHC-LDP cannot identify the correct clusters. DPC-CE has erroneous assignment for low-density clusters.\n<div class=\"figure figure-full\" id=\"fig10\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang10abcdef-3227936-large.gif\" data-fig-id=\"fig10\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang10abcdef-3227936-small.gif\" alt=\"Fig. 10. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Pathbased dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 10. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Pathbased dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig11\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang11abcdef-3227936-large.gif\" data-fig-id=\"fig11\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang11abcdef-3227936-small.gif\" alt=\"Fig. 11. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Compound dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 11. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Compound dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig12\" class=\"fulltext-link\">Fig.12</a> and <a ref-type=\"fig\" anchor=\"fig13\" class=\"fulltext-link\">Fig.13</a> show the clustering results on the D31 and R15 datasets. These two datasets have more instances and clusters than the previous datasets, and the clusters in these two datasets are mainly spherical in shape. The clustering results of the algorithms on these two datasets are similar. On the D31 dataset, DPC-PMDS does not perform as well as DPC-DBFN, DPC-CE and FHC-LDP, but it is not much different from them, and it outperforms the other two algorithms. On the R15 dataset, the clustering results of DPC-PMDS and DPC-DBFN are the best.\n<div class=\"figure figure-full\" id=\"fig12\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang12abcdef-3227936-large.gif\" data-fig-id=\"fig12\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang12abcdef-3227936-small.gif\" alt=\"Fig. 12. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the D31 dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 12. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the D31 dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig13\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang13abcdef-3227936-large.gif\" data-fig-id=\"fig13\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang13abcdef-3227936-small.gif\" alt=\"Fig. 13. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the R15 dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 13. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the R15 dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig.14</a> and <a ref-type=\"fig\" anchor=\"fig15\" class=\"fulltext-link\">Fig.15</a> show the clustering results on the Threecircles and CMC datasets. The points in the central region of these two datasets have a higher density than the surrounding points. From <a ref-type=\"fig\" anchor=\"fig14\" class=\"fulltext-link\">Fig.14</a>, it can be seen that DPC and DPC-DBFN cannot separate clusters correctly, which is because just choosing points with high density and relative distance as centers on this dataset will choose the wrong cluster centers. PHA incorrectly combines two clusters in the central region into one cluster. The clustering results of DPC-CE, FHC-LDP, and DPC-PMDS are completely correct. On the CMC dataset, the clustering results of DPC-PMDS and FHC-LDP are correct. PHA cannot distinguish different clusters and DPC cannot identify the correct cluster centers. DPC-DBFN and DPC-CE can identify the correct cluster centers, but the points are inaccurately assigned.\n<div class=\"figure figure-full\" id=\"fig14\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang14abcdef-3227936-large.gif\" data-fig-id=\"fig14\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang14abcdef-3227936-small.gif\" alt=\"Fig. 14. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Threecircles dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 14. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Threecircles dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig15\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang15abcdef-3227936-large.gif\" data-fig-id=\"fig15\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang15abcdef-3227936-small.gif\" alt=\"Fig. 15. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the CMC dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 15. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the CMC dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"fig\" anchor=\"fig16\" class=\"fulltext-link\">Fig.16</a> and <a ref-type=\"fig\" anchor=\"fig17\" class=\"fulltext-link\">Fig.17</a> show the clustering results on S1 and Unbalance datasets. The number of instances for these two datasets is 5000 and 6500, respectively, which can verify the performance of the algorithm on large-scale datasets. On the S1 dataset, the result of DPC-PMDS is optimal among all algorithms. The shape of clusters on the Unbalanced dataset is simple, and there is no intersection between clusters. All the algorithms except the PHA algorithm have correct clustering results on this dataset.\n<div class=\"figure figure-full\" id=\"fig16\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang16abcdef-3227936-large.gif\" data-fig-id=\"fig16\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang16abcdef-3227936-small.gif\" alt=\"Fig. 16. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the S1 dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 16. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the S1 dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div>\n<div class=\"figure figure-full\" id=\"fig17\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang17abcdef-3227936-large.gif\" data-fig-id=\"fig17\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang17abcdef-3227936-small.gif\" alt=\"Fig. 17. - Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Unbalance dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 17. </b><fig><p>Clustering results of (a) DPC, (b)PHA, (c)DPC-DBFN, (d)DPC-CE, (e)FHC-LDP and (f) the proposed method (DPC-PMDS) clustering methods on the Unbalance dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p><a ref-type=\"table\" anchor=\"table4\" class=\"fulltext-link\">Table 4</a> shows the optimal clustering results of all algorithms on the 12 synthetic datasets. DPC-PMDS is optimal among all algorithms on all datasets except the D31 dataset. The values of ACC, NMI, RI, and ARI for DPC-PMDS on Aggregation, Flame, Jain, Spiral, Pathbased, Threecircles, CMC, and Unbalance datasets are all 1. On the Compound dataset, ACC, NMI, RI, and ARI of DPC-PMDS are significantly greater than other algorithms. On the R15 dataset, the evaluation metrics of DPC-PMDS and DPC-DBFN are marginally higher than the other algorithms. On the S1 datasets, DPC-PMDS also has slightly better performance than the other algorithms. On the D31 dataset, the clustering results of DPC-PMDS are also acceptable.</p><p>In general, the DPC-PMDS algorithm proposed in this paper obtains the best clustering result on 12 synthetic datasets, and DPC-PMDS performs better on datasets with complex shapes and uneven density distribution compared to the other algorithms.</p></div><div class=\"section_2\" id=\"sec4d\"><h3>D. Results on UCI Datasets</h3><p>In this subsection, experiments are conducted on eight UCI datasets. <a ref-type=\"table\" anchor=\"table5\" class=\"fulltext-link\">Table 5</a> shows the best clustering results of all algorithms on the eight UCI datasets, and the optimal values of the evaluation metrics are in bold.<div class=\"figure figure-full table\" id=\"table5\"><div class=\"figcaption\"><b class=\"title\">TABLE 5 </b>\nClustering results of algorithms on eight UCI datasets.</div><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t5-3227936-large.gif\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang.t5-3227936-small.gif\" alt=\"Table 5- &#10;Clustering results of algorithms on eight UCI datasets.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div></div></p><p>These UCI datasets contain different dimensions and numbers of instances, among which Seeds and Thyroid have 210 and 215 instances with 7 and 5 features, respectively. Diabetes has 768 instances with 8 features, and Cloud has 1024 instances and 10 features. On Seeds, Diabetes, Thyroid, and Cloud datasets, the values of ACC, NMI, RI, and ARI of DPC-PMDS are significantly higher than those of other algorithms, which indicates that DPC-PMDS has better clustering result compared to other algorithms. The Iris dataset contains 4 features and 150 instances. On the Iris dataset, the clustering result of DPC-PMDS is slightly lower than that of FHC-LDP but better than that of other algorithms. DNA is a high-dimensional dataset with 2000 instances and 180 features. On the DNA dataset, the NMI value of DPC-PMDS is much higher than that of other algorithms, and the maximum values of ARI, RI, and ACC are achieved by DPC-DBFN, DPC-CE, and FHC-LDP, respectively. Abalone is a large-scale dataset with 4177 instances and 7 features. On the Abalone dataset, DPC-PMDS has the highest NMI and ARI values. Although the values of ACC and RI of DPC-PMDS are slightly lower than those of FHC-LDP, they are higher than those of other algorithms. Robot navigation is a large-scale high-dimensional dataset containing 5456 instances and 24 features, and the NMI value of DPC-PMDS is the highest on this dataset. The experimental results show that the clustering result of the DPC-PMDS algorithm proposed in this paper is overall optimal on the UCI dataset, and the DPC-PMDS algorithm can handle large-scale and high-dimensional real-world datasets relatively well.</p></div><div class=\"section_2\" id=\"sec4e\"><h3>E. Results on Olivetti Faces Dataset</h3><p>The Olivetti Faces dataset has a total of 400 different face images, with <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$92\\times 112$\n</tex-math></inline-formula> features per instance. It is a commonly used dataset for clustering. We selected its top 100 images for experiments to verify the performance of the proposed algorithm on the image dataset. The values of ACC, NMI, RI, and ARI of DPC-PMDS on the top 100 face images of the Olivetti Faces dataset are 0.9900, 0.9857, 0.9962, and 0.9768, respectively. <a ref-type=\"fig\" anchor=\"fig18\" class=\"fulltext-link\">Fig.18</a> shows the visualization results. Different colors represent different clusters. From <a ref-type=\"fig\" anchor=\"fig18\" class=\"fulltext-link\">Fig.18</a>, it can be seen that only one face is not successfully assigned. Thereby DPC-PMDS algorithm can obtain valid clustering results on the Olivetti Faces dataset.\n<div class=\"figure figure-full\" id=\"fig18\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang18-3227936-large.gif\" data-fig-id=\"fig18\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang18-3227936-small.gif\" alt=\"Fig. 18. - Clustering results of DPC-PMDS on the Olivetti Faces dataset.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 18. </b><fig><p>Clustering results of DPC-PMDS on the Olivetti Faces dataset.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p></div><div class=\"section_2\" id=\"sec4f\"><h3>F. Sensitivity Analysis of the Proposed Method</h3><p>In this subsection, the effect of the parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> on the clustering result of the DPC-PMDS algorithm will be analyzed. <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig.19</a> shows the NMI and RI values of DPC-PMDS with different values of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> on the synthetic datasets.\n<div class=\"figure figure-full\" id=\"fig19\"><!--\n          Workaround for combined images.Eg.- 1000116 Fig. 5\n        --><div class=\"img-wrap\"><a href=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang19-3227936-large.gif\" data-fig-id=\"fig19\"><img src=\"/mediastore_new/IEEE/content/media/6287639/9668973/9978604/zang19-3227936-small.gif\" alt=\"Fig. 19. - NMI and RI values of DPC-PMDS with different &#10;$\\alpha $&#10; values on synthetic datasets.\"/><div class=\"zoom\" title=\"View Larger Image\"/></a></div><div class=\"figcaption\"><b class=\"title\">Fig. 19. </b><fig><p>NMI and RI values of DPC-PMDS with different <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> values on synthetic datasets.</p></fig></div><p class=\"links\"><a href=\"/document/9978604/all-figures\" class=\"all\">Show All</a></p></div></p><p>According to the previous section, it can be known that the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> determines the diffusion strength value, and the diffusion strength determines the size and structure of the initial clusters. Therefore, if the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> is too small, i.e., the diffusion strength is too small, the initial clusters generated according to the label diffusion rule cannot contain enough core points. Then some points will be assigned incorrectly on non-spherical datasets with complex shapes, which is because the boundary points are assigned by a distance-based strategy. In <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig. 19</a>, we can see that when the <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> takes a small value, the clustering results on the Pathbased, Compound, Jain, Spiral, Threecircles, and CMC datasets are not optimal.</p><p>Through experiments, we find that the parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> has different effects on datasets with different characteristics. From <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig.19(a)</a> and <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig.19(c)</a>, it can be found that for the dataset with intersections between clusters, a slightly smaller value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> in general leads to better clustering results. This is because if the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> is large, the large diffusion strength will cause excessive label diffusion and easily connect the points of multiple clusters. This situation is especially obvious on the dataset with intersections between clusters. From <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig. 19(b)</a> and <a ref-type=\"fig\" anchor=\"fig19\" class=\"fulltext-link\">Fig. 19(d)</a>, it can be found that for the dataset with no intersection between clusters, better results are generally obtained with a slightly larger value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula>. This is because on datasets without clusters intersection, a slightly larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> will facilitate label diffusion so that the initial cluster can better reflect the cluster structure.</p><p>In summary, the parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> is less sensitive on spherical clusters. On a dataset that the clusters do not intersect, a slightly larger <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> can get better clustering results. On a dataset with clusters intersection, the value of <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$\\alpha $\n</tex-math></inline-formula> should be slightly smaller.</p></div></div>\n<div class=\"section\" id=\"sec5\"><div class=\"header article-hdr\"><div class=\"kicker\">\n\t\t                        SECTION V.</div><h2>Conclusion</h2></div><p>In this paper, an improved density peaks clustering algorithm based on the potential model and diffusion strength is proposed. The main purpose of this paper is to avoid the dependence of DPC on the parameter <inline-formula id=\"\"><tex-math notation=\"LaTeX\">$d_{c} $\n</tex-math></inline-formula>, to improve the accuracy of the selection of cluster centers on datasets with complex shapes and uneven density distribution, and to reduce the chain reaction. The potential and centrality of data points are used to calculate the density. We present the concept of diffusion strength and the label diffusion rule. By considering the diffusion strength, DPC-PMDS can accurately select the cluster centers of datasets with uneven density distribution. The initial clusters consisting of centers and core points can reflect the core structure of clusters well. The motivation for this study is that obtaining the core structure of clusters usually leads to great clustering results.</p><p>The performance of DPC-PMDS is compared with DPC, PHA, DPC-DBFN, DPC-CE, and FHC-LDP algorithms on synthetic and UCI datasets, and the performance of DPC-PMDS on image dataset is examined with the first 100 face images from the Olivetti Faces dataset. The experimental results indicate that the proposed DPC-PMDS algorithm exhibits good clustering effectiveness on all datasets.</p><p>This work can be further improved in the following two directions. The first is the adaptive selection of parameter since the optimal value of parameter is different for datasets with different characteristics. DPC-PMDS uses Euclidean distance for similarity calculation of data points, which is not suitable for high-dimensional datasets. Therefore, we will look for new similarity measure suitable for high-dimensional datasets to further improve the effectiveness of the algorithm.</p></div>\n</div></div></response>\n"
}